# GLEAN AI OUTCOMES MANAGER â€” INTERVIEW PACKAGE

**Role**: AI Outcomes Manager  
**Company**: Glean  
**Prepared**: February 15, 2026

---

## OPENING STATEMENT (2-MINUTE VERSION)

"I spent the last 2.5 years building Palette â€” a three-tier agentic system that does the work of a team of language engineers. I built it at AWS to solve a real problem: our science team spent all our time curating one main prompt for a 25-billion-node knowledge graph. I kept thinking, 'There has to be a better way.'

So I built a system with three tiers: a core prompt that never changes, a middle tier of agents that test problems against solutions and build what's needed, and a testing layer that logs what works and enables learning at scale.

The result: 105 validated problem-solution pairs, 93 knowledge library entries, 7 production agents, and three active projects shipping real outcomes â€” not demos.

Before that, I spent 4 years driving GenAI partnerships at AWS. I launched 27+ models on Bedrock, grew Stability AI 5X year-over-year, and enabled 500+ field sellers. I also led Analytics GTM for AWS Public Sector, growing the business 24% YoY.

What I'm good at: taking ambiguous customer problems, translating them into clear AI solutions, building those solutions with customers, and measuring outcomes that matter to their business.

That's exactly what Glean needs â€” someone who can partner with executives and end users, design AI agents that solve real problems, and ship measurable business outcomes, not just demos."

---

## OPENING STATEMENT (30-SECOND VERSION)

"I built Palette â€” a three-tier agentic system â€” over 2.5 years at AWS. It's shipping real outcomes across three active projects: teaching agentic systems, small business planning, and video game development. Before that, I drove GenAI partnerships at AWS, launching 27+ models and growing Stability AI 5X YoY. I'm good at translating customer problems into AI solutions, building with customers, and measuring outcomes. That's what Glean needs."

---

## YOUR COMPETITIVE ADVANTAGES

### 1. You've Shipped AI Outcomes (Not Just Demos)
**Palette Proof Points**:
- 105 validated problem-solution pairs (RIUs)
- 93 knowledge library entries with sources
- 7 production agents with promotion/demotion logic
- 3 active projects shipping outcomes:
  - Teaching agentic systems (interview prep, 30-min demos)
  - Small business planning (graffiti art gallery business plan)
  - Video game development (multiplayer RPG with Godot + Node.js)

**What This Proves**: You don't just talk about agentic systems â€” you built one, validated it, and shipped outcomes.

### 2. You Understand LLM Capabilities and Limits
**AWS GenAI Partnerships**:
- Launched 27+ models on Bedrock (xAI, Mistral, Stability AI, Luma AI, TwelveLabs, Poolside)
- Grew Stability AI 5X YoY (enterprise GTM, field enablement, joint sales plays)
- Worked with video AI (TwelveLabs, Luma AI), code gen (Poolside), and multimodal models
- Enabled 500+ field sellers on model capabilities, limitations, and customer fit

**What This Proves**: You know what LLMs can and cannot do, and you can set realistic expectations with customers.

### 3. You Can Work Consultatively with Customers
**AWS Public Sector Analytics GTM**:
- Grew business 24% YoY across government and education
- Founded AWS Data Leadership Forum (291+ senior data leaders, 98+ CxOs)
- Built enablement programs that changed seller behavior (+17% attendance, +50% high-CSAT sessions)
- Worked with stakeholders from ICs to executives

**What This Proves**: You can work with executives and end users, tailor messages to different audiences, and drive adoption.

### 4. You Can Design and Build AI Agents
**Palette Agent Archetypes**:
- Argentavis (Argy) â€” Research agent
- Rex â€” Architecture agent
- Therizinosaurus (Theri) â€” Build agent
- Velociraptor (Raptor) â€” Debug agent
- Yutyrannus (Yuty) â€” Narrative/GTM agent
- Ankylosaurus (Anky) â€” Validation agent
- Parasaurolophus (Para) â€” Monitoring agent

**What This Proves**: You've designed agent workflows, built them, and validated them in production.

---

## DEMO SCENARIO: WORKING WITH A GLEAN CUSTOMER

**Setup**: You're working with a VP of Sales at a mid-market SaaS company. They want to "use AI to help sales reps close deals faster."

### Step 1: Discovery (5-10 minutes)
**Your Questions**:
1. "What does 'close deals faster' mean to you? (Shorter sales cycle? Higher win rate? Faster proposal generation?)"
2. "What's the current process? (Walk me through a typical deal from lead to close)"
3. "Where do reps spend the most time? (Research? Proposal writing? Follow-ups? Internal approvals?)"
4. "What data do you have? (CRM, past proposals, call transcripts, competitive intel, product docs?)"
5. "How do you measure success today? (Average deal size? Win rate? Sales cycle length?)"

**What You're Listening For**:
- Is there a pattern? (Same bottleneck across multiple reps)
- Is there data? (CRM, proposals, call transcripts)
- Is it repetitive? (Every deal follows similar steps)
- Can you measure success? (Clear metrics: win rate, cycle time, deal size)

### Step 2: Problem Framing (5 minutes)
**Your Synthesis**:
"Okay, here's what I'm hearing: Your reps spend 40% of their time writing proposals, and they're starting from scratch every time. You have 200+ past proposals in Google Drive, CRM data on what wins, and product docs that change quarterly. The pattern is clear: proposal writing is repetitive, data-rich, and measurable. This is a great fit for AI."

**Success Metrics You Propose**:
- **Phase 1 (0-3 months)**: 50% reduction in proposal writing time (from 8 hours to 4 hours per proposal)
- **Phase 2 (3-6 months)**: 10% increase in win rate (AI surfaces winning patterns from past proposals)
- **Phase 3 (6-12 months)**: 15% shorter sales cycle (faster proposals = faster close)

### Step 3: Solution Design (10 minutes)
**Your Recommendation**:
"We'll build a Glean AI agent that helps reps generate first-draft proposals in 30 minutes instead of 8 hours. Here's how it works:

1. **Input**: Rep provides customer name, industry, pain points, and deal size
2. **Agent retrieves**:
   - Top 5 similar past proposals (by industry, deal size, pain points)
   - Winning patterns from CRM (what messaging closed deals)
   - Latest product docs (features, pricing, case studies)
3. **Agent generates**:
   - First-draft proposal (executive summary, solution overview, pricing)
   - Competitive positioning (based on past wins)
   - Next steps and timeline
4. **Human-in-the-loop**: Rep reviews, edits, and finalizes (still owns the proposal)

**Why This Works**:
- Leverages existing data (past proposals, CRM, product docs)
- Reduces repetitive work (proposal writing)
- Measurable outcomes (time saved, win rate, sales cycle)
- Human-in-the-loop (rep still owns the relationship and final proposal)"

### Step 4: Pilot Scoping (5 minutes)
**Your Proposal**:
"Let's run a 12-week pilot with 5-10 reps:
- **Week 1-2**: Discovery and data prep (connect Glean to Google Drive, CRM, product docs)
- **Week 3-4**: Build and test agent (iterate on retrieval quality and output format)
- **Week 5-8**: Pilot with 5-10 reps (measure time saved, usage, and feedback)
- **Week 9-12**: Refine and scale (expand to full sales team if metrics hit targets)

**Success Criteria**:
- 80%+ of reps use the agent weekly
- 50%+ reduction in proposal writing time
- 4.5+ satisfaction score (out of 5)

**Kill Criteria**:
- <50% weekly usage after Week 6
- <30% time savings
- <4.0 satisfaction score"

### Step 5: Rollout and Iteration (Ongoing)
**Your Plan**:
"After the pilot, we'll:
1. Expand to full sales team (if metrics hit targets)
2. Add new capabilities (competitive intel, pricing optimization, follow-up emails)
3. Measure business outcomes (win rate, sales cycle, deal size)
4. Iterate based on usage data and feedback"

---

## INTERVIEW QUESTIONS & ANSWERS

### Q1: "Tell me about a time you shipped an AI outcome, not just a demo."

**Answer**:
"At AWS, I built Palette â€” a three-tier agentic system â€” to solve a real problem: our science team spent all our time curating one main prompt for a 25-billion-node knowledge graph. I built a system with three tiers: a core prompt that never changes, a middle tier of agents that test problems against solutions, and a testing layer that logs what works.

The result: 105 validated problem-solution pairs, 93 knowledge library entries, and 7 production agents. It's shipping real outcomes across three active projects:
1. Teaching agentic systems (interview prep, 30-min demos)
2. Small business planning (graffiti art gallery business plan)
3. Video game development (multiplayer RPG with Godot + Node.js)

The key difference from a demo: Palette has promotion/demotion logic. Agents get promoted from UNVALIDATED â†’ WORKING â†’ PRODUCTION based on success rates. If they fail, they get demoted. That's production, not a demo."

---

### Q2: "How do you know what LLMs can and cannot do?"

**Answer**:
"I spent 4 years at AWS launching 27+ models on Bedrock and SageMaker. I worked with xAI, Mistral, Stability AI, Luma AI, TwelveLabs, and Poolside. I grew Stability AI 5X year-over-year by building their AWS partnership and enabling 500+ field sellers.

Here's what I learned:
- **LLMs are great at**: Pattern recognition, summarization, retrieval-augmented generation, structured output generation
- **LLMs struggle with**: Math, reasoning over long contexts, real-time data, deterministic outputs, complex multi-step workflows without scaffolding

When I work with customers, I set expectations early:
- 'AI can generate a first-draft proposal in 30 minutes, but you'll need to review and edit it.'
- 'AI can surface the top 5 similar past deals, but it won't make the final pricing decision.'
- 'AI can help you write code faster, but you'll need to test and debug it.'

The key is human-in-the-loop for high-stakes decisions, and clear success metrics so customers know what 'good' looks like."

---

### Q3: "Walk me through how you'd design an AI agent for a customer."

**Answer**:
"I use a 4-question framework:

1. **Is there a pattern?** (Same problem across multiple users or workflows)
2. **Is there data?** (Past examples, documents, structured data)
3. **Is it repetitive or high-volume?** (Happens frequently enough to justify automation)
4. **Can you measure success?** (Clear metrics: time saved, accuracy, user satisfaction)

If the answer to all four is yes, it's a good fit for AI.

Then I design the agent workflow:
1. **Input**: What does the user provide? (Customer name, deal size, pain points)
2. **Retrieval**: What data does the agent need? (Past proposals, CRM data, product docs)
3. **Generation**: What does the agent produce? (First-draft proposal, competitive positioning)
4. **Human-in-the-loop**: Where does the user review and edit? (Final proposal, pricing decisions)

Finally, I scope a pilot:
- 12 weeks, 5-10 users
- Clear success metrics (80%+ usage, 50%+ time savings, 4.5+ satisfaction)
- Kill criteria (if metrics don't hit targets, we stop)

The key is starting small, measuring outcomes, and iterating based on feedback."

---

### Q4: "How do you handle customers who want AI to do everything?"

**Answer**:
"I set expectations early. AI is great at reducing repetitive work and surfacing insights, but it's not magic.

Here's how I frame it:
- 'AI can generate a first draft, but you'll need to review and finalize it.'
- 'AI can surface the top 5 similar past deals, but you'll make the final decision.'
- 'AI can help you write code faster, but you'll need to test and debug it.'

I also use the 4-question framework to filter out bad use cases:
- No pattern? AI won't help.
- No data? AI has nothing to learn from.
- Not repetitive? Manual process is fine.
- Can't measure success? You won't know if it's working.

If a customer insists on a use case that doesn't pass the 4-question test, I push back:
- 'This use case is high-risk because we can't measure success. Let's start with something measurable and expand later.'
- 'This use case requires real-time data that LLMs don't have. Let's scope a pilot with human-in-the-loop first.'

The key is being honest about what AI can and cannot do, and steering customers toward high-value, low-risk use cases first."

---

### Q5: "Tell me about a time you worked with executives and end users."

**Answer**:
"At AWS, I led Analytics GTM for Public Sector, working with government and education customers. I founded the AWS Data Leadership Forum â€” 291+ senior data leaders, 98+ CxOs.

Here's how I tailored my approach:
- **Executives**: Focus on business outcomes (cost savings, revenue growth, risk reduction). Use CFO-defensible numbers and clear ROI frameworks.
- **End users**: Focus on workflows and pain points. Use 5-minute recipes and live demos, not 2-hour workshops.

Example: I built an enablement program for AWS sellers that changed behavior:
- +17% attendance (153 â†’ 179 per session)
- +50% high-CSAT sessions (>4.9)
- +67% sales plays covered (133 â†’ 222)

The key was understanding incentives:
- Executives care about strategic priorities and ROI.
- End users care about making their jobs easier and faster.

I start with executives to understand strategic priorities, then work with end users to understand workflows, then build AI solutions that align both."

---

### Q6: "How do you measure AI outcomes?"

**Answer**:
"I use three types of metrics:

1. **Usage metrics**: Are people actually using the AI agent?
   - Daily/weekly active users
   - Frequency of use (how many times per week)
   - Retention (are users coming back)

2. **Efficiency metrics**: Is the AI saving time or reducing effort?
   - Time saved (e.g., proposal writing: 8 hours â†’ 4 hours)
   - Tasks completed (e.g., 10 proposals per week â†’ 15 proposals per week)
   - Error reduction (e.g., 95% accuracy vs. 85% manual accuracy)

3. **Business outcome metrics**: Is the AI driving business results?
   - Win rate (e.g., 30% â†’ 33%)
   - Sales cycle length (e.g., 90 days â†’ 75 days)
   - Customer satisfaction (e.g., NPS, CSAT)

The key is setting clear success criteria before the pilot:
- 'We'll measure success by 80%+ weekly usage, 50%+ time savings, and 4.5+ satisfaction score.'
- 'If we don't hit these targets by Week 8, we'll stop and reassess.'

I also use kill criteria to avoid sunk cost fallacy:
- <50% weekly usage after Week 6 â†’ stop
- <30% time savings â†’ stop
- <4.0 satisfaction score â†’ stop"

---

### Q7: "What's your experience with prompt engineering and AI agents?"

**Answer**:
"I built Palette â€” a three-tier agentic system â€” over 2.5 years. It has 7 production agents:
1. Argentavis (Argy) â€” Research agent
2. Rex â€” Architecture agent
3. Therizinosaurus (Theri) â€” Build agent
4. Velociraptor (Raptor) â€” Debug agent
5. Yutyrannus (Yuty) â€” Narrative/GTM agent
6. Ankylosaurus (Anky) â€” Validation agent
7. Parasaurolophus (Para) â€” Monitoring agent

Each agent has:
- A core prompt (immutable rules)
- Access to a knowledge library (93 Q&A pairs with sources)
- Access to a taxonomy (105 validated problem-solution pairs)
- Promotion/demotion logic (UNVALIDATED â†’ WORKING â†’ PRODUCTION based on success rates)

I've also worked with OpenAI, Claude, Mistral, Cohere, and Stability AI models at AWS. I know how to craft effective prompts, guide AI agents, and iterate based on feedback.

The key is starting with a clear problem statement, testing at scale, and logging what works so the system learns over time."

---

### Q8: "How do you handle AI failures or hallucinations?"

**Answer**:
"Three strategies:

1. **Human-in-the-loop for high-stakes decisions**:
   - AI generates first draft, human reviews and finalizes
   - AI surfaces top 5 options, human makes final decision
   - AI flags anomalies, human investigates

2. **Retrieval-augmented generation (RAG)**:
   - Ground AI outputs in real data (past proposals, CRM, product docs)
   - Cite sources so users can verify
   - Use structured output formats to reduce hallucinations

3. **Clear success criteria and kill criteria**:
   - Set expectations: 'AI will be 90% accurate, not 100%'
   - Measure accuracy and iterate
   - If accuracy drops below threshold, stop and reassess

Example from Palette: I built a validation agent (Ankylosaurus) that checks outputs for:
- Factual accuracy (does the output match the source data?)
- Coherence (does the output make sense?)
- Completeness (does the output answer the question?)

If validation fails, the agent gets demoted and we iterate on the prompt or retrieval strategy."

---

### Q9: "Why Glean? Why this role?"

**Answer**:
"Three reasons:

1. **Glean is solving the right problem**: Enterprise search and knowledge management are broken. Employees waste hours searching for information across fragmented tools. Glean's AI-powered search and Work AI platform fix this.

2. **This role is the perfect fit**: I've spent 2.5 years building agentic systems (Palette), 4 years enabling AI adoption at AWS (500+ sellers, 27+ models), and 3 years driving enterprise GTM (24% YoY growth). This role combines all three: design AI agents, work with customers, ship measurable outcomes.

3. **I want to work with customers**: At AWS, I loved working with customers to solve real problems. This role is 100% customer-facing, which is where I thrive. I want to partner with executives and end users, design AI solutions, and measure business outcomes."

---

### Q10: "What's your 30-60-90 day plan?"

**Answer**:
**First 30 days: Learn**
- Shadow 5-10 customer engagements (discovery, pilots, rollouts)
- Review top 10 customer use cases (what's working, what's not)
- Meet with Product and R&D (understand roadmap and priorities)
- Build relationships with Sales and Customer Success teams

**Days 31-60: Contribute**
- Lead 2-3 discovery workshops with new customers
- Design and scope 1-2 pilots (with clear success metrics and kill criteria)
- Contribute to Product roadmap (based on customer feedback)
- Build internal playbooks (discovery frameworks, pilot scoping templates)

**Days 61-90: Scale**
- Lead 5+ customer engagements end-to-end (discovery â†’ pilot â†’ rollout)
- Ship 1-2 measurable outcomes (time saved, efficiency gains, business results)
- Identify expansion opportunities (new use cases, new teams, new functions)
- Share learnings with Product and R&D (what's working, what's not)

**Key metrics I'll track**:
- Number of customer engagements led
- Number of pilots scoped and launched
- Customer satisfaction scores
- Business outcomes delivered (time saved, efficiency gains)"

---

## GLEAN COMPANY INTEL

### What Glean Does:
- **AI-powered enterprise search**: Helps employees find information across Google Drive, Slack, Microsoft Teams, and 100+ other tools
- **Work AI platform**: Combines search, AI assistant, and agent-building capabilities
- **Founded**: 2019 by Arvind Jain (ex-Google, led search infrastructure)

### Product Capabilities:
1. **Enterprise search**: Semantic search across all company tools
2. **AI assistant**: Answers questions, summarizes documents, generates content
3. **Agent builder**: Customers can build custom AI agents for specific workflows
4. **Integrations**: 100+ tools (Google Drive, Slack, Teams, Salesforce, Jira, etc.)

### Customers:
- "Thousands of companies" (exact number not disclosed)
- Industries: Tech, financial services, healthcare, retail, manufacturing
- Use cases: Sales enablement, customer support, engineering productivity, HR operations

### Competitors:
1. **Microsoft Copilot**: Built into Microsoft 365, strong distribution
2. **Google Duet AI**: Built into Google Workspace, strong distribution
3. **Notion AI**: Built into Notion, strong with knowledge workers
4. **Perplexity for Enterprise**: AI-powered search and research
5. **Guru**: Knowledge management and search
6. **Coveo**: Enterprise search and recommendations

### Glean's Differentiation:
- **Best-in-class search**: Semantic search across all tools, not just one ecosystem
- **Agent builder**: Customers can build custom agents, not just use pre-built assistants
- **Enterprise-grade**: Security, compliance, and governance built-in
- **Founder pedigree**: Arvind Jain led Google search infrastructure

### Funding:
- **Series D**: $200M at $2.2B valuation (2024)
- **Total raised**: $360M+
- **Investors**: Kleiner Perkins, Lightspeed, Sequoia, General Catalyst

### Team Size:
- ~300 employees (estimated)
- Hiring heavily: Engineering, Product, Sales, Customer Success

---

## GLEAN CUSTOMER USE CASES

### 1. Sales Enablement
**Problem**: Sales reps waste time searching for proposals, case studies, competitive intel  
**Solution**: Glean AI agent surfaces relevant past proposals, winning patterns, and product docs  
**Outcome**: 50% reduction in proposal writing time, 10% increase in win rate

### 2. Customer Support
**Problem**: Support agents waste time searching for help docs, past tickets, product info  
**Solution**: Glean AI agent surfaces relevant help docs, similar past tickets, and solutions  
**Outcome**: 30% reduction in ticket resolution time, 20% increase in CSAT

### 3. Engineering Productivity
**Problem**: Engineers waste time searching for code, docs, and past decisions  
**Solution**: Glean AI agent surfaces relevant code snippets, design docs, and past PRs  
**Outcome**: 25% reduction in time spent searching, 15% increase in code velocity

### 4. HR Operations
**Problem**: HR teams waste time answering repetitive questions (PTO, benefits, policies)  
**Solution**: Glean AI agent answers common HR questions and surfaces relevant policies  
**Outcome**: 40% reduction in HR ticket volume, 50% faster response time

### 5. Product Management
**Problem**: PMs waste time searching for customer feedback, feature requests, and roadmap docs  
**Solution**: Glean AI agent surfaces relevant customer feedback, feature requests, and past decisions  
**Outcome**: 30% reduction in time spent on research, 20% faster decision-making

---

## YOUR PALETTE DEMO (IF THEY ASK)

**Setup**: "Let me show you how Palette works. I'll walk through a real example from one of my active projects."

### Example: Teaching Agentic Systems (Interview Prep)

**Problem**: I needed to prepare a 30-minute demo on agentic systems for an interview.

**Step 1: Problem Classification**
- Palette classified this as RIU-47 (Training & Enablement) + RIU-89 (Agentic Orchestration)

**Step 2: Agent Routing**
- Routed to Yutyrannus (Yuty) â€” Narrative/GTM agent

**Step 3: Research Directive**
- Yuty created a research directive for Argentavis (Argy) â€” Research agent
- Argy researched 127 AI companies, mapped them to Palette RIUs, and validated market demand

**Step 4: Build**
- Therizinosaurus (Theri) â€” Build agent created:
  - 30-minute demo script
  - 18-slide deck outline
  - 2 interview scenarios with answer frameworks
  - 4-Question Method framework (pattern, data, repetitive, measurable)

**Step 5: Validation**
- Ankylosaurus (Anky) â€” Validation agent checked for coherence, accuracy, and completeness

**Outcome**: Complete interview prep package in 20 minutes (vs. 4-6 hours manually)

**What This Proves**:
- Multi-agent workflow (Yuty â†’ Argy â†’ Theri â†’ Anky)
- Real outcome (interview prep package, not a demo)
- Measurable efficiency (20 min vs. 4-6 hours)
- Production-ready (used in real interview)

---

## QUESTIONS TO ASK GLEAN

### About the Role:
1. "What does success look like in the first 90 days?"
2. "What's the typical customer engagement? (Discovery â†’ pilot â†’ rollout timeline)"
3. "How do you measure AI outcomes? (Usage, efficiency, business results)"
4. "What's the split between working with executives vs. end users?"
5. "How do you collaborate with Product and R&D? (Feedback loops, roadmap input)"

### About Glean:
6. "What are the top 3 customer use cases right now? (Sales, support, engineering?)"
7. "What's the biggest challenge customers face when adopting Glean?"
8. "How do you differentiate from Microsoft Copilot and Google Duet AI?"
9. "What's the product roadmap for the next 12 months?"
10. "How do you think about agent builder vs. pre-built assistants?"

### About the Team:
11. "How big is the AI Outcomes team? (How many people in this role?)"
12. "Who do I report to? (VP Customer Success? VP Product?)"
13. "What's the team culture like? (Collaborative? Autonomous? Fast-paced?)"
14. "How do you support professional development? (Training, conferences, certifications)"

### About Customers:
15. "What industries are seeing the most success with Glean?"
16. "What's the typical customer size? (100-1000 employees? 1000-10,000?)"
17. "What's the average time from pilot to full rollout?"
18. "What's the biggest reason customers don't adopt Glean?"

---

## RED FLAGS TO WATCH FOR

1. **No clear success metrics**: If they can't define what "good" looks like, the role is ambiguous
2. **No Product/R&D collaboration**: If you're just implementing, not influencing roadmap, you're a glorified consultant
3. **No customer examples**: If they can't share real customer outcomes, the product might not be working
4. **Unrealistic expectations**: If they expect you to "transform every department" in 90 days, run
5. **No kill criteria**: If they don't have a framework for stopping bad pilots, they'll waste time and money

---

## YOUR CLOSING STATEMENT

"I'm excited about this role because it combines everything I'm good at: designing AI solutions, working with customers, and shipping measurable outcomes. I've spent 2.5 years building Palette â€” a production agentic system â€” and 4 years enabling AI adoption at AWS. I know how to translate customer problems into AI solutions, build those solutions with customers, and measure outcomes that matter to their business. That's exactly what Glean needs, and I'm ready to start."

---

## FINAL PREP CHECKLIST

- [ ] Review Glean website (glean.com)
- [ ] Watch Glean product demo (YouTube)
- [ ] Review Arvind Jain's LinkedIn profile (Founder & CEO)
- [ ] Prepare 3-5 intelligent questions about their product
- [ ] Rehearse opening statement (2-min and 30-sec versions)
- [ ] Rehearse demo scenario (working with a Glean customer)
- [ ] Prepare Palette demo (if they ask)
- [ ] Have your resume ready (in case they ask)

---

**Good luck with the interview!** ðŸš€

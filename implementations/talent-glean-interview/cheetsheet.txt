
interview cheet sheet


â€œI started my career in linguistics and systems thinking, which shaped how I think about complexity and structure. I spent eight years in academia before moving to AWS, where Iâ€™ve spent the last eleven years building and scaling AI-enabled enterprise systems.

At AWS, I was brought in to solve what I saw as a distribution problem, not a creation problem. We were generating massive amounts of enablement content, but sellers werenâ€™t using it because it wasnâ€™t aligned to their workflows or incentives.

I shifted the strategy from top-down broadcast to targeted activation â€” mapping incentives, personas, and job-level workflows before introducing tools. We built agentic systems and lightweight â€œrecipesâ€ that sellers could use immediately, and adoption followed behavior change.

My core belief is that AI should either enable something that wasnâ€™t previously possible or meaningfully enhance the human interaction at the center of the work. For Gap, that means starting with merchandisers, planners, and store managers â€” understanding their incentives and workflows before prescribing platforms â€” and designing AI around their reality, not the other way around.â€




## 3 OUT-OF-THE-BOX IDEAS (DIFFERENTIATORS)

### 1. Seams Theory â€” Where the $50-100M Waste Comes From

Not a people problem; it's a seams problem between teams and systems. Forecasting, markdown, supply, and stores optimize individually. No one agent sees end-to-end: demand â†’ buy â†’ allocate â†’ markdown â†’ replenishment.

**Anchor scenario**: "Old Navy store #247 has 200 extra units of a jacket. Today, forecasting, markdown, and replenishment react separately. By the time everything syncs, margin is gone. One agentic system can simultaneously: mark down 15% in that store, reduce the next buy by 30%, and reroute inbound units to the store that's selling out â€” one decision, coordinated."

**Layer 1**: this level of coordination is impossible without AI at Gap's scale.

### 2. Staffing Inversion â€” 2 People Doing What 5 Couldn't

Not arguing to cut heads. Arguing to invert what the same heads can do. From "ring transactions and tidy shelves" to:
- "We can see every nearby store's inventory in real time"
- "We can recognize a returning customer and pull things they're likely to love"
- "We can notice a micro-trend (rain jackets) and trigger a floor set shift"

> "The question isn't 'can we run stores with fewer people?' It's 'what new capabilities do those same people have when an assistant shows them cross-store inventory, demand signals, and next best actions in real time?'"

### 3. Data That Doesn't Exist Yet

Biggest forecasting gains don't come from re-analyzing old data; they come from data you don't have yet:
- TikTok/Instagram trend velocity 4-6 weeks before POS
- Micro-demand by weather and local events
- Cross-brand cannibalization (Old Navy vs Gap vs Athleta)
- Structured return reasons (fit, quality, expectation mismatch)
- Foot-traffic intent: not just counts, but what people looked at and walked away from

> "Everyone is trying to squeeze more out of yesterday's data. The real advantage is in creating new information streams Gap doesn't have today â€” intent, cross-brand cannibalization, micro-trends â€” and feeding them into the same system."

---

## TWO-LAYER FRAMEWORK

**Layer 1 â€” Impossible Without AI**
- Cross-system coordination (seams theory)
- Real-time cross-store and cross-brand views
- Trend velocity detection
- Cannibalization and intent detection

**Layer 2 â€” Augment Humans**
- More time with customers
- Faster, more confident decisions
- Better merch and pricing choices
- Champions teaching peers

---

## CLOSING LINE

"We're not building AI for AI's sake. We're making 10,000+ Gap employees more capable at their specific jobs. The budget shrinks because we did it right."








ğŸ”¹ Where the Name Comes From

â€œGleanâ€ is an English verb meaning:

To gather information bit by bit from different sources.


Why AI Fails:

AI adoption fails in large enterprises when the technology is introduced before the behavior and incentive structure is understood.

It fails when tools are layered onto existing workflows instead of embedded into them.

It fails when thereâ€™s fragmentation â€” multiple overlapping initiatives without clear differentiation.

It fails when value isnâ€™t measurable at the job level.

And it fails when fear isnâ€™t addressed â€” when employees arenâ€™t given safe, practical ways to experiment.

In my experience, adoption succeeds when you align incentives, embed into workflows, start small with measurable wins, and treat trust as a design constraint â€” not an afterthought.




ğŸ”¹ Company Snapshot

Founded: 2019

Founder/CEO: Arvind Jain (former Google Search engineering leader)

Headquarters: Palo Alto, CA

Category: Enterprise AI + knowledge search + agent platform

That Google Search lineage is powerful if used subtly:

â€œGiven Arvindâ€™s background in search, it makes sense that Gleanâ€™s differentiation starts with knowledge graph + permissions.â€
----- Arvind Jain, Gleanâ€™s founder and former Google Search engineering leader, helped advance large-scale search relevance systems that combined ranking algorithms with contextual and behavioral signals to return more accurate, intent-aware results at web scale.


ğŸ”¹ Why Enterprises Buy Glean

Knowledge fragmentation across tools (Slack, Drive, Jira, etc.)

AI without governance is risky

Need permission-aware grounding

Want productivity lift without building internal infra

Want ROI from AI, not just pilots

That ties directly to your story:

â€œclosing the gap between demo-stage AI and measurable business impactâ€

ğŸ”¹ Growth & Market Position

~$200M ARR (recent milestone)

Rapid enterprise expansion

Recognized in Gartner Magic Quadrant for Enterprise Search

Customers include large enterprises (Fortune 500 scale)

You can casually say:

â€œHitting $200M ARR signals strong product-market fit.â€


Backrond stong
screen questions 

hybrid 


Post sales - customer outcomes --- LIVE --- Delivery execlence manager 

Partnerning after that 

What can make things better faster



Use case cateloges


Handful of different customers. 


next steps  - awesome 


â€œI build applied AI systems that move from demo to production by solving adoption friction, retrieval reliability, and workflow alignment. I operate at the intersection of technical debugging and behavioral change.â€


â€œI started my career in compatitive linguistics and systems thinking, which shaped how I approach complexity and structure. After eight years in academia, I moved to Amazon, where Iâ€™ve spent the last eleven + years building applied AI systems in enterprise environments.  Amazon has been an amazing place to lean skills in real time as Ive constantly been pushed to be at the very edge of Tech capacity.  That's also my favorite part.

Across roles like knowledge engineer, ontologist, and TPM, Iâ€™ve worked at the intersection of knowledge architecture and AI deployment.

Most recently at AWS, I built and deployed an agentic AI enablement system designed for the broader seller community. It reduced seller prep time by about 25% and improved adoption and execution indicators across the org.
When I started, I realized we didnâ€™t have a creation problem â€” we had a distribution and adoption problem. We were generating massive enablement content, but sellers werenâ€™t using it because it wasnâ€™t aligned to their workflows or incentives.


What consistently motivates me is closing the gap between AI demos and measurable business impact â€” and thatâ€™s why the AI Outcomes role at Glean feels like a natural next step.â€




â€œJust today, for example, I led an end-to-end integration of a multi-agent web runtime with an upstream gateway on a live VPS. We traced a production connectivity issue across Docker bindings, gateway config, and firewall synchronization. The issue turned out to be loopback binding keeping the system in fallback mode. We isolated it, documented the runbook, and now have a near-production path. Thatâ€™s the kind of ambiguity Iâ€™m comfortable operating in.â€




2ï¸âƒ£ â€œWhy are you interested in Glean / this AI Outcomes role?â€
What theyâ€™re really evaluating:


â€œWhat stands out about Glean is that itâ€™s not just enterprise search â€” itâ€™s becoming a unified AI work layer with agent-building capability embedded into workflow.

The AI Outcomes role is particularly compelling because it sits at the intersection of adoption, time-to-value, and ROI. Thatâ€™s the space Iâ€™ve been operating in â€” not just building AI systems, but making them actually drive measurable change inside organizations.

Iâ€™m especially excited about partnering with executive sponsors and power users to translate AI capability into real business outcomes.â€



3  â€œGive me an example of an AI outcome you shipped.â€
What theyâ€™re really evaluating:


â€œAt AWS, I built a multi-agent AI enablement system that generated role- and account-specific sales plays and trusted, sourced guidance for GTM teams.

The difference from a demo was post-launch hardening. We instrumented usage, monitored outputs, and fixed retrieval and citation issues until it was reliable in live workflows.

The system reduced prep time by about 25% and improved sales play coverage and high-CSAT indicators.

It wasnâ€™t just a chatbot â€” it became embedded in seller workflow.â€


ğŸ”¹ Why Enterprises Buy Glean

Knowledge fragmentation across tools (Slack, Drive, Jira, etc.)

AI without governance is risky

Need permission-aware grounding

Want productivity lift without building internal infra

Want ROI from AI, not just pilots

That ties directly to your story:

â€œclosing the gap between demo-stage AI and measurable business impactâ€








ou said you reduced turnaround time by 25% and improved adoption in your AWS GTM agentic system.

Letâ€™s be precise.

If I were your CFO and I asked:

â€œHow much money did this system generate or save?â€


â€œThe system served roughly 180 GTM sellers globally on a recurring basis, with assets reused asynchronously by another ~500. We reduced preparation time by about 25% by istantly generating account-specific sales plays and directing sellers to the exact content they needed, whether it be video chunks, 5 min use and play excersises called recipes, elevator pitches or full sales plays. All directly prepared for a specific client. 

The economic impact is leverage. For enterprise sellers managing multi-million-dollar pipelines, reclaiming 3â€“5 hours per week translates into more customer-facing time and improved cycle velocity.

I donâ€™t claim direct revenue ownership, but we saw leading indicators move â€” sales play coverage increased 67% and high-CSAT sessions increased 50%. At that population scale, even modest improvements in cycle time or execution quality translate into meaningful pipeline acceleration.â€


## â€œIf you were redesigning this today, how would you instrument it so you could show direct revenue attribution instead of leading indicators?â€

â€œIâ€™d instrument it in three layers.

First, adoption attribution â€” track usage at the individual seller level and link it to specific sales plays or motions inside CRM.

Second, behavioral shift â€” measure leading indicators like SPIF enrollment, content engagement depth, and deal-stage progression speed relative to baseline.

Third, revenue correlation â€” run cohort analysis comparing sellers who actively used the system versus those who didnâ€™t, controlling for territory and deal size. That gives you a defensible estimate of cycle-time improvement or win-rate delta attributable to the tool.

Over time, Iâ€™d move from leading indicators to lagging ones â€” pipeline velocity, win rate, and average deal size â€” tied back to usage data. Already now I have divided the seller poplation into cohorts and am targeting 3 groups: sellers that did use the tool and do have increased sales; those who have increased sales but do use our tool(to find out what they do instead); and those that do not use the tool and do not have any sales increase.

## What if usage is high but revenue doesnâ€™t move?

â€œIf usage is high but revenue doesnâ€™t move, I diagnose in three dimensions.

First, workflow impact â€” are we influencing the revenue-critical behavior, or just making something easier? Adoption without impact means we optimized convenience.

Second, external variables â€” pricing shifts, territory mix, or macro conditions may be masking performance. I normalize against broader business trends before drawing conclusions.

Third, execution quality â€” is the tool shaping deal strategy, or is it being used passively?

If after isolating those variables thereâ€™s no measurable delta, we redesign or sunset. High adoption alone is not success.â€


##You run a 12-week pilot at Glean.

Adoption: 85%.
Users love it.
CIO says:

â€œWeâ€™re not seeing ROI. Iâ€™m not renewing.â€

â€œThatâ€™s fair â€” if youâ€™re not seeing ROI, you shouldnâ€™t renew.
What outcome did you expect to move, and what threshold would make this a clear yes?

What I can show today is 85% adoption and consistent weekly usage, plus exactly where time is being saved â€” and where it isnâ€™t. What we owe you is a clean line to the KPI you care about.

Letâ€™s do a two-week ROI closeout: one workflow, tight baseline, pass/fail threshold we agree on. If we hit it, we expand with confidence. If we donâ€™t, we stop â€” and you walk away with a documented analysis on exactly what worked and why.


###â€œFine. Two weeks. But I want a written commitment: if you donâ€™t hit the ROI threshold, we donâ€™t pay for the pilot extension.â€

â€œIâ€™m open to that, with two conditions. First, we define the KPI, baseline, and pass/fail threshold in writing. Second, you commit the inputs that make the measurement valid: data access, a named business owner, and weekly checkpoints.

Commercially, weâ€™ll structure it as a success-based closeout: if we miss the agreed threshold, the extension fee is waived or fully credited. If we hit it, we convert to renewal and the fee applies to the contract. That keeps incentives aligned and removes ambiguity.â€



###â€œFine. But my team says your system is creating more work â€” they have to verify answers. Adoption is high because itâ€™s mandated, not because itâ€™s useful.â€

â€œI hear that. If the workflow is net adding work, weâ€™re not delivering value â€” and mandated usage doesnâ€™t count.

This week Iâ€™ll sit with 5 power users and 5 skeptics, using both session logs and live observation to isolate the friction. In my experience itâ€™s one of three things: retrieval quality, review UX, or a mismatch between the tool and the job-to-be-done.

Then weâ€™ll ship a bounded fix â€” tighten sources, raise citation confidence, and remove steps that donâ€™t change decisions. Weâ€™ll measure net time saved and task completion. If we canâ€™t get net-positive within two weeks on the core workflow, we pause expansion.â€


###You keep mentioning â€œtighten sourcesâ€ and â€œcitation confidence. How do you prevent the system from confidently citing the wrong thing?

â€œWe prevent confident mis-citation with layered controls.
First, retrieval constraints: responses are limited to a whitelisted, version-controlled corpus with clear ownership and deprecation rules.
Second, claim-to-source alignment: every assertion must map to retrieved spans, not just a document link; if we canâ€™t trace it, we donâ€™t say it.
Third, confidence thresholds and abstention: if retrieval scores are weak, evidence is contradictory, or coverage is too narrow, the system returns â€˜insufficient evidenceâ€™ and asks for clarifying input instead of guessing.
Fourth, monitoring and audit: we log retrievalâ†’output traces, run weekly sampling, and actively detect source skew and drift so issues donâ€™t silently accumulate.â€


##Youâ€™re proposing â€œabstain rather than guess.â€ Good.

But customers hate friction.

How do you balance:

reliability (abstain)
with

usability (getting work done)

â€œThe operating principle is risk-tiered reliability. We donâ€™t use one global confidence threshold â€” we tune by workflow impact.

For low-risk workflows like internal knowledge lookup or draft generation, we allow lower confidence thresholds and return best-effort answers with citation because human review cost is low and speed matters.

For high-risk workflows â€” pricing decisions, policy interpretation, customer-facing outputs â€” we raise similarity thresholds, require multi-source corroboration, and enforce structured citation with abstention if grounding is weak. In some cases, we require explicit human approval before release.

The same model behaves differently depending on the business consequence of error.â€



AI adoption fails in large enterprises when the technology is introduced before the behavior and incentive structure is understood.

It fails when tools are layered onto existing workflows instead of embedded into them.

It fails when thereâ€™s fragmentation â€” multiple overlapping initiatives without clear differentiation.

It fails when value isnâ€™t measurable at the job level.

And it fails when fear isnâ€™t addressed â€” when employees arenâ€™t given safe, practical ways to experiment.

In my experience, adoption succeeds when you align incentives, embed into workflows, start small with measurable wins, and treat trust as a design constraint â€” not an afterthought.


----How would you prioritize AI use cases across merchandising, HR, supply chain, and stores?

Executive Answer:

I would prioritize using a three-part lens: measurable business impact, workflow integration feasibility, and risk profile.

First, I look for areas where the economic signal is clear â€” margin, inventory accuracy, markdown optimization, or time spent on repetitive tasks.

Second, I assess whether the AI solution can embed directly into existing workflows. If it requires behavior change without workflow integration, adoption will stall.

Third, I evaluate risk â€” brand, compliance, operational. In Year 1, I would focus on internal and operational use cases where ROI is measurable and brand exposure is low.

That typically means starting with merchandising analytics, forecasting support, and associate-facing internal assistants before customer-facing automation.

I believe in earning the right to scale by proving value in tightly scoped pilots with explicit success metrics and kill criteria.


---What would your first 90 days look like in this role?

Executive Answer:

The first 30 days would be diagnostic. I would meet stakeholders across merchandising, stores, HR, technology, and data to map current AI initiatives, tool sprawl, and friction points in employee workflows.

Days 30â€“60 would focus on prioritization. Iâ€™d identify two to three high-impact, low-risk pilot candidates, define measurable KPIs, and align on governance and ownership upfront.

Days 60â€“90 would be about disciplined pilot execution â€” small scope, clear success criteria, embedded feedback loops, and visible executive reporting.

My goal in the first 90 days wouldnâ€™t be scale â€” it would be clarity, alignment, and one tangible win that demonstrates the operating model works.


How do you ensure responsible, human-centered AI enablement?

Executive Answer:

Responsible AI starts with scope discipline. Not every workflow should be automated.

I focus on augmentation over replacement â€” designing systems that enhance decision-making rather than remove human judgment.

I build guardrails early: clear data boundaries, human-in-the-loop controls, auditability, and measurable performance metrics.

I also treat trust as a design requirement. Employees need safe ways to experiment, clear understanding of what the system can and cannot do, and confidence that AI supports their role rather than threatens it.

In my experience, responsible design accelerates adoption because it builds credibility with both operators and executives.



1ï¸âƒ£ How would you prioritize AI use cases across merchandising, HR, supply chain, and stores?

Executive Answer:

I prioritize using three lenses: measurable business impact, workflow integration feasibility, and risk profile.

First, I look for areas with clear economic signals â€” margin, inventory accuracy, markdown optimization, or time spent on repetitive tasks.

Second, I assess whether the AI can embed directly into existing workflows. If it requires behavior change without integration, adoption will stall.

Third, I evaluate brand, compliance, and operational risk. In Year 1, I focus on internal use cases where ROI is measurable and exposure is low.

I believe in proving value through tightly scoped pilots with explicit success metrics before scaling.

2ï¸âƒ£ What would your first 90 days look like?

Executive Answer:

Days 1â€“30: Diagnose. Map stakeholders, tool sprawl, data readiness, and workflow friction.

Days 30â€“60: Prioritize. Identify two to three pilots with measurable KPIs and clear ownership.

Days 60â€“90: Execute disciplined pilots with feedback loops and executive visibility.

The goal isnâ€™t scale in 90 days â€” itâ€™s clarity, alignment, and one tangible proof point that the operating model works.

3ï¸âƒ£ How do you ensure responsible, human-centered AI enablement?

Executive Answer:

Responsible AI starts with scope discipline. Not every workflow should be automated.

I design for augmentation over replacement, embed human-in-the-loop controls, establish clear data boundaries, and measure performance transparently.

Trust is a design constraint. When employees understand what the system can and cannot do, adoption accelerates.

4ï¸âƒ£ How do you measure ROI for enterprise AI initiatives?

Executive Answer:

I measure ROI across three categories: cost reduction, productivity lift, and revenue or margin impact.

For operational use cases, that might mean reduced manual time or improved forecast accuracy. For merchandising, improved sell-through or reduced markdown intensity.

I always define baseline metrics before launching pilots and track realized impact, not theoretical gains.

ROI should be visible, measurable, and tied to business KPIs â€” not just usage metrics.

5ï¸âƒ£ How do you handle executive pushback on AI investment?

Executive Answer:

I frame AI investment as a portfolio decision, not a moonshot.

Start small. Define measurable outcomes. Fund in phases.

If the pilot doesnâ€™t hit predefined thresholds, we stop. If it does, we scale.

That discipline reduces risk and builds executive confidence.

6ï¸âƒ£ How do you prevent tool sprawl?

Executive Answer:

Tool sprawl happens when experimentation isnâ€™t governed.

I recommend a centralized platform strategy with federated use-case ownership.

That means shared standards, observability, and security â€” but brand-level autonomy in prioritization.

We sunset overlapping tools as successful patterns emerge.

7ï¸âƒ£ How do you drive AI adoption among non-technical employees?

Executive Answer:

Adoption succeeds when AI is embedded into existing workflows and tied to incentives.

I focus on small, practical workflows â€” not large training sessions.

Champions at the team level are critical. When peers demonstrate value, adoption spreads organically.

8ï¸âƒ£ When should you NOT use AI?

Executive Answer:

When the workflow lacks clean data, when risk outweighs value, or when automation removes necessary human judgment.

AI should solve meaningful problems, not create novelty.

If a deterministic system works better, use it.

9ï¸âƒ£ How would you evaluate ChatGPT vs Copilot vs Claude for enterprise use?

Executive Answer:

I evaluate across four dimensions: capability, integration into existing enterprise systems, governance and compliance, and cost structure.

The right platform depends on workflow fit and ecosystem integration â€” not model hype.

Platform choice should follow operating model strategy.

ğŸ”Ÿ How do you balance centralization vs brand autonomy?

Executive Answer:

Centralize standards, security, and core platform infrastructure.

Federate use-case prioritization and adoption within brands.

This allows scale without suppressing brand differentiation.

11ï¸âƒ£ What are the biggest risks of AI in retail?

Executive Answer:

Brand safety, pricing errors, biased decision-making, and over-automation.

Retail margins are tight â€” small errors scale quickly.

Risk management and guardrails must precede scale.

12ï¸âƒ£ How do you embed AI into Workday or HR systems?

Executive Answer:

Start with employee-facing workflows â€” onboarding support, policy Q&A, learning recommendations.

Integrate via APIs, maintain clear data boundaries, and avoid creating shadow systems.

AI should enhance the system of record, not replace it.

13ï¸âƒ£ How do you ensure AI improves margin, not just productivity?

Executive Answer:

Tie AI directly to margin-linked KPIs â€” sell-through rate, markdown percentage, inventory turns.

If the use case doesnâ€™t connect to economic drivers, it shouldnâ€™t be prioritized.

14ï¸âƒ£ How do you scale from pilot to enterprise rollout?

Executive Answer:

Define success upfront. Measure impact. Document lessons.

Standardize governance and rollout playbooks.

Then scale deliberately â€” not emotionally.

15ï¸âƒ£ How do you handle resistance from middle management?

Executive Answer:

Resistance often reflects fear of loss of control.

I involve managers early, define success metrics collaboratively, and show how AI supports their teamâ€™s performance.

Ownership reduces resistance.

16ï¸âƒ£ What does â€œAI enablementâ€ mean to you?

Executive Answer:

It means making employees more capable at their specific jobs.

Not more tools. Not automation for its own sake.

Capability amplification.

17ï¸âƒ£ How do you prioritize across competing stakeholder demands?

Executive Answer:

Use a transparent scoring framework: impact, feasibility, risk, and data readiness.

Publish criteria. Make trade-offs visible.

That builds trust in prioritization decisions.

18ï¸âƒ£ How do you monitor AI system performance over time?

Executive Answer:

Track model performance metrics, usage trends, business impact metrics, and drift indicators.

AI isnâ€™t â€œlaunch and forget.â€ Itâ€™s monitor and iterate.

19ï¸âƒ£ How do you manage vendor relationships?

Executive Answer:

Avoid single-vendor dependence when possible.

Negotiate flexibility, ensure data portability, and align contracts to performance.

Vendor strategy should support long-term operating flexibility.

20ï¸âƒ£ What differentiates you from other AI strategists?

Executive Answer:

Iâ€™ve built and debugged production AI systems inside large enterprises.

I understand not just strategy, but adoption, governance, and real-world friction.

I design operating models that make AI usable â€” not just impressive.
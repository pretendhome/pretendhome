# Palette Curated Knowledge Library v1.0 - Production YAML
# Generated: 2026-01-27
# Purpose: RAG-ready knowledge base for Palette agents
# Status: PRODUCTION READY
# Questions: 76 (71 complete + 5 gap additions)

metadata:
  version: "1.3"
  generated_date: "2026-02-05"
  total_questions: 81
  complete_questions: 76
  gap_additions: 5
  problem_types: 7
  rius_referenced: 111
  aws_services_covered: 45
  industries_covered: 15
  authority_sources: 44
  
  problem_type_distribution:
    Intake_and_Convergence: 12
    Human_to_System_Translation: 11
    Systems_Integration: 10
    Data_Semantics_and_Quality: 11
    Reliability_and_Failure_Handling: 10
    Operationalization_and_Scaling: 11
    Trust_Governance_and_Adoption: 11
    
  difficulty_distribution:
    low: 6
    medium: 21
    high: 36
    critical: 13
    
  validation_status:
    syntax_validated: true
    cross_references_validated: true
    aws_services_current: true
    regulatory_guidance_current: true
    last_validation_date: "2026-01-27"

library_questions:
- id: LIB-001
  question: How do I force convergence when stakeholders have conflicting definitions
    of success?
  answer: Create a Convergence Brief (RIU-001) requiring written commitment to Goal,
    Roles, Constraints, Capabilities, and Non-goals. Use Stakeholder Map + RACI-lite
    (RIU-002) to identify decision authority by influence and interest, then develop
    tailored engagement strategies. Secure executive sponsorship (CIO/CFO level) early
    â€” involvement should correlate with investment size and cross-team dependencies.
    Facilitate a cross-functional workshop where stakeholders must agree on measurable
    success criteria before proceeding. Err toward higher risk ratings initially,
    refine through discussion. Establish "people champions" to drive alignment, and
    pilot solutions to build consensus through demonstrated success before scaling.
    Document agreed definitions as ONE-WAY DOOR commitments in decisions.md to prevent
    re-litigation.
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  
  difficulty: high
  industries:
  - Enterprise SaaS
  - Healthcare
  - Government
  - Fintech
  tags:
  - convergence
  - stakeholder-management
  - scope-definition
  - conflict-resolution
  sources:
  - title: AWS Cloud Adoption Framework (AWS CAF)
    url: https://aws.amazon.com/cloud-adoption-framework/
  - title: Organizational Alignment - AWS Well-Architected Framework
    url: https://docs.aws.amazon.com/wellarchitected/latest/framework/organizational-alignment.html
  - title: Building a Cloud Operating Model
    url: https://docs.aws.amazon.com/whitepapers/latest/building-cloud-operating-model/building-cloud-operating-model.html
- id: LIB-002
  question: What's the difference between a ONE-WAY DOOR and TWO-WAY DOOR decision
    in AI system architecture?
  answer: 'ONE-WAY DOOR decisions are difficult or impossible to reverse and require
    human approval before execution â€” flag these as "ðŸš¨ ONE-WAY DOOR â€” confirmation
    required" and log in decisions.md via RIU-003 (Decision Log + One-Way Door Registry).
    Examples in AI/ML: ethical AI guidelines, data governance frameworks, model architecture
    selection, production deployment commitments, and database schema for ML features.
    TWO-WAY DOOR decisions are easily reversible and support Amazon''s "Bias for Action"
    â€” agents may proceed autonomously with monitoring. Examples: hyperparameter tuning,
    prompt iterations, A/B test configurations. Key insight: don''t treat AI/ML projects
    as deterministic software; acknowledge uncertainty and establish governance templates
    before implementation. For ONE-WAY DOORs, estimate value of right decisions against
    cost of wrong ones, secure executive sponsorship, and use MLOps assessment to
    ensure architectural decisions support long-term ROI.'
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-003
  difficulty: medium
  industries:
  - All
  tags:
  - decision-framework
  - architecture
  - reversibility
  - risk-management
  sources:
  - title: Mental Models for Your Digital Transformation
    url: https://aws.amazon.com/blogs/enterprise-strategy/mental-models-for-your-digital-transformation/
  - title: AI/ML Organizational Adoption Framework
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/index.html
  - title: AWS Cloud Adoption Framework for Artificial Intelligence, Machine Learning,
      and Generative AI
    url: https://docs.aws.amazon.com/whitepapers/latest/aws-caf-for-ai/aws-caf-for-ai.html
  - title: Machine Learning Lens - AWS Well-Architected
    url: https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html
  - title: Unlocking the Business Value of Machine Learningâ€”With Organizational Learning
    url: https://aws.amazon.com/blogs/enterprise-strategy/unlocking-the-business-value-of-machine-learning-with-organizational-learning/
  - title: Eviden's Comprehensive Approach to MLOps Assessment
    url: https://aws.amazon.com/blogs/apn/develop-and-deploy-machine-learning-models-with-eviden-comprehensive-approach-to-mlops-assessment/
- id: LIB-003
  question: How do I scope an AI pilot when the customer says 'we need AI everywhere'?
  answer: 'Resist technology-first thinking. Use RIU-004 (Problem â†’ Workstream Decomposition)
    to generate broad candidate use cases without committing, then narrow using AWS''s
    Five V''s Framework: Value (high-impact opportunities), Visualize (clear success
    metrics), Validate (test against real requirements), Verify (scalable production
    path), Venture (secure long-term resources). Apply weighted shortest job first
    (WSJF) combined with responsible AI assessment â€” this may reprioritize projects
    by revealing hidden complexity. Create a Scope Freeze (RIU-005) with Phase 0/1/2
    deliverables: start with prompt engineering for general knowledge tasks, add RAG
    when factual grounding needed. Define explicit Non-goals in your Convergence Brief
    (RIU-001) to prevent "AI everywhere" creep. Select pilots with quality data, engaged
    business sponsors, measurable ROI, and TWO-WAY DOOR characteristics enabling quick
    iteration. Target 3-6 month delivery cycles to build organizational confidence
    before scaling.'
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-003
  - RIU-004
  - RIU-005
  difficulty: high
  industries:
  - Enterprise SaaS
  - Logistics
  - Operations
  tags:
  - scope-management
  - pilot-design
  - expectation-setting
  - phased-delivery
  sources:
  - title: 'Beyond pilots: A proven framework for scaling AI to production'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-pilots-a-proven-framework-for-scaling-ai-to-production/
  - title: Incorporating responsible AI into generative AI project prioritization
    url: https://aws.amazon.com/blogs/machine-learning/incorporating-responsible-ai-into-generative-ai-project-prioritization/
  - title: Generative AI Lifecycle Operational Excellence (GLOE) framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
- id: LIB-004
  question: What artifacts prove convergence has been achieved before implementation
    starts?
  answer: 'The Semantic Blueprint (Convergence Brief, RIU-001) is the primary artifact
    â€” it must contain five elements: Goal (measurable success criteria), Roles (human
    vs agent responsibilities), Capabilities (tools/agents needed), Constraints (binding
    requirements), and Non-goals (explicit exclusions). Supporting artifacts include:
    Stakeholder Map + RACI-lite (RIU-002) showing decision authority and escalation
    paths; Decision Log + One-Way Door Registry (RIU-003) documenting irreversible
    commitments; Success Metrics Charter (RIU-006) with outcome metrics and acceptance
    checks; Assumptions Register (RIU-008) listing testable assumptions with validation
    plans; and Problem â†’ Workstream Decomposition (RIU-004) showing candidate RIUs
    without premature commitment. Per AWS CAF AI, also document ethical AI guidelines,
    data governance approach, and risk evaluation. Convergence is proven when stakeholders
    have signed off on these artifacts and no open questions block implementation.
    Store in decisions.md for restartability.'
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-003
  - RIU-004
  - RIU-006
  - RIU-008
  difficulty: medium
  industries:
  - All
  tags:
  - convergence-validation
  - semantic-blueprint
  - documentation
  - checkpoints
  sources:
  - title: AWS Cloud Adoption Framework for Artificial Intelligence, Machine Learning,
      and Generative AI
    url: https://docs.aws.amazon.com/whitepapers/latest/aws-caf-for-ai/aws-caf-for-ai.html
  - title: AI/ML Organizational Adoption Framework
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/index.html
  - title: Planning a Generative AI Project
    url: https://explore.skillbuilder.aws/learn/course/external/view/elearning/17256/planning-a-generative-ai-project
- id: LIB-005
  question: How do I handle requirements that change weekly in an enterprise AI deployment?
  answer: 'Distinguish between TWO-WAY DOOR changes (reversible â€” absorb quickly)
    and ONE-WAY DOOR changes (irreversible â€” require formal re-convergence via RIU-001).
    Implement a federated operating model: central oversight for data access, model
    risk, and compliance, while lines of business iterate autonomously within defined
    boundaries. Use the Assumptions Register (RIU-008) to track volatile requirements
    as testable assumptions with expiry dates rather than fixed specs. Apply AI-DLC
    methodology with structured prompts and mob elaboration to move from requirements
    to working prototypes in hours, enabling rapid validation before commitment. Map
    organizational debt and streamline approval processes that hinder adaptation.
    Redefine the Convergence Brief (RIU-001) as a living document â€” freeze ONE-WAY
    DOORs (architecture, data schema, compliance) while allowing TWO-WAY DOORs (prompts,
    UI, thresholds) to evolve weekly. Key insight: AI systems are nondeterministic,
    requiring new trust models where "requirements" become hypotheses validated through
    continuous experimentation rather than upfront specifications.'
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-008
  
  difficulty: high
  industries:
  - Enterprise SaaS
  - Government
  - Healthcare
  tags:
  - change-management
  - agile-fde
  - scope-creep
  - stakeholder-alignment
  sources:
  - title: Generative AI operating models in enterprise organizations with Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/generative-ai-operating-models-in-enterprise-organizations-with-amazon-bedrock/
  - title: 'Beyond the technology: Workforce changes for AI'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-the-technology-workforce-changes-for-ai/
  - title: 'Tech Talk: Seeing AI-DLC Work - How AI Transforms Enterprise Development'
    url: https://broadcast.amazon.com/videos/1702552
  - title: Why agentic AI marks an inflection point for enterprise modernization
    url: https://aws.amazon.com/blogs/aws-insights/aws-why-agentic-ai-marks-an-inflection-point-for-enterprise-modernization/
  - title: AI and Digital Transformation
    url: https://aws.amazon.com/blogs/enterprise-strategy/ai-and-digital-transformation/
- id: LIB-006
  question: What's the minimum viable convergence brief for a 2-week AI proof of concept?
  answer: "Even a 2-week PoC requires all five Semantic Blueprint elements (RIU-001),\
    \ but scoped minimally:\n      \n      **Goal**: One measurable outcome (e.g.,\
    \ \"80% accuracy on 50 test cases\" or \"< 3s latency at < $0.02/request\").\n\
    \      **Roles**: One human sponsor + one technical lead â€” skip full RACI.\n \
    \     **Capabilities**: Single AI capability being validated (e.g., \"Document\
    \ classification via Bedrock Claude\").\n      **Constraints**: Hard boundaries\
    \ â€” budget cap, data restrictions, no production deployment, token limits.\n \
    \     **Non-goals**: 2-3 explicit exclusions to prevent scope creep.\n      \n\
    \      Per AWS guidance, add **exit criteria** with specific thresholds for quality,\
    \ latency, and cost â€” know when to pivot or stop. Validate only core components\
    \ before full integration to isolate failure points. Track unit economics early\
    \ (per-request cost, token usage, compute). Document ethical AI considerations\
    \ even for short pilots. Use ONE-WAY DOOR check: if PoC requires irreversible\
    \ decisions (data grants, vendor commits), get sign-off first. Brief should fit\
    \ one page â€” if not, you're overscoping."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-003
  difficulty: low
  industries:
  - All
  tags:
  - mvp
  - poc
  - rapid-delivery
  - lightweight-process
  sources:
  - title: 'Beyond pilots: A proven framework for scaling AI to production'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-pilots-a-proven-framework-for-scaling-ai-to-production/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: AI/ML Organizational Adoption Framework
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/index.html
- id: LIB-007
  question: How do I identify when a customer request is actually 3 separate problems
    disguised as one?
  answer: "**Warning signs of bundled problems:**\n      - Vague objectives like \"\
    improve productivity\" or \"add AI\" (no specific metric)\n      - Different stakeholders\
    \ define success differently for the same request\n      - No clear line of sight\
    \ from proposed feature to measurable business outcome\n      - Request spans\
    \ multiple data domains, teams, or systems\n      - Cannot answer \"how will we\
    \ measure success?\" with one metric\n      \n      **Decomposition technique:**\
    \ Use RIU-004 (Problem â†’ Workstream Decomposition) to generate broad candidate\
    \ workstreams without committing. Apply the AIR Workshop methodology: score each\
    \ potential use case (1-10) on business impact, cost, implementation complexity,\
    \ business priority, and timeline. Plot on Eisenhower matrix to separate high-impact\
    \ use cases from quick wins. Each decomposed problem must have its own line of\
    \ sight to specific, quantifiable metrics (e.g., \"reduce churn 5%\" not \"improve\
    \ customer experience\").\n      \n      **Validation test:** If a problem requires\
    \ its own Convergence Brief (RIU-001) with distinct Goal, Constraints, and Stakeholders\
    \ â€” it's a separate problem. Document in decisions.md and negotiate phased delivery\
    \ (RIU-005) rather than one mega-project."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-004
  - RIU-005
  
  difficulty: high
  industries:
  - Enterprise SaaS
  - Logistics
  - Operations
  tags:
  - problem-decomposition
  - scope-analysis
  - complexity-assessment
  sources:
  - title: AI Use Case Identification & Prioritization (AIR Workshop)
    url: https://broadcast.amazon.com/videos/1811883
  - title: Organizational AI Vision - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_1_vision_and_strategy/5_1_1_organizational_ai_vision.html
- id: LIB-008
  question: What questions surface hidden constraints that will block AI deployment
    later?
  answer: "Ask these questions during discovery and document answers in your Constraint\
    \ Profile (RIU-007):\n      \n      **Regulatory & Compliance:**\n      - Which\
    \ regulations apply? (EU AI Act, GDPR, HIPAA, CCPA, SOX, FedRAMP)\n      - Is\
    \ a Business Associate Addendum (BAA) required? (Healthcare)\n      - What model\
    \ explainability requirements exist? (EU AI Act high-risk systems)\n      - What\
    \ audit trail and traceability requirements apply?\n      \n      **Data & Privacy:**\n\
    \      - Where must data reside? (Sovereignty, cross-border transfer restrictions)\n\
    \      - What PII/PHI handling is required? (Encryption at rest/in transit, anonymization)\n\
    \      - Who owns the training data? Any licensing restrictions?\n      - Can\
    \ data leave the customer's environment for model training/inference?\n      \n\
    \      **Infrastructure & Security:**\n      - What network latency is acceptable?\
    \ (Critical for real-time AI)\n      - What authentication/IAM controls are required?\n\
    \      - Is the model supply chain auditable? (Backdoor/vulnerability risks)\n\
    \      - Is Infrastructure as Code (IaC) required for deployment?\n      \n  \
    \    **Organizational & Procurement:**\n      - How long is legal/security review?\
    \ (Often 4-12 weeks in enterprise)\n      - Are there existing vendor contracts\
    \ that constrain tool selection?\n      - Do unions or workforce agreements affect\
    \ automation deployment?\n      - Is there executive sponsorship aligned with\
    \ legal/compliance/business units?\n      - What team knowledge gaps require training\
    \ before deployment?\n      \n      Flag any answer that implies a ONE-WAY DOOR\
    \ decision (RIU-003). Use RIU-530 (AI Risk Classification) for regulated industries."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-003
  - RIU-007
  - RIU-012
  
  - RIU-530
  difficulty: critical
  industries:
  - Healthcare
  - Finance
  - Government
  tags:
  - constraint-discovery
  - risk-assessment
  - compliance
  - blockers
  sources:
  - title: Regulatory Compliance and Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_5_security_privacy/3_5_3_compliance_data_protection/3_5_3-2_regulatory_governance/regulatory_governance.html
  - title: HIPAA compliance for generative AI solutions on AWS
    url: https://aws.amazon.com/blogs/industries/hipaa-compliance-for-generative-ai-solutions-on-aws/
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
  - title: Navigating the responsible use of AI in government procurement
    url: https://aws.amazon.com/blogs/publicsector/navigating-the-responsible-use-of-ai-in-government-procurement/
- id: LIB-009
  question: How do I document tribal knowledge that exists only in stakeholder heads?
  answer: "**Elicitation techniques:**\n      - Use voice-based capture (Amazon Transcribe\
    \ + Bedrock) â€” reduces cognitive load and captures natural explanations better\
    \ than asking SMEs to write documentation\n      - Ask scenario-based questions:\
    \ \"Walk me through what you do when X happens\" rather than \"What are the rules\
    \ for X?\"\n      - Probe \"it depends\" answers: \"What specifically does it\
    \ depend on? Give me the last 3 examples\"\n      - Shadow SMEs during real work\
    \ to capture decision-making in context\n      - Use \"teach-back\" validation:\
    \ document your understanding and have SME correct it\n      \n      **4-step\
    \ capture workflow:**\n      1. **Capture**: Record conversations with experienced\
    \ workers (voice preferred)\n      2. **Transcribe**: Convert to structured text\
    \ using Amazon Transcribe\n      3. **Structure**: Use Bedrock to extract decision\
    \ trees, rules, and edge cases\n      4. **Validate**: Review with SME to confirm\
    \ accuracy before finalizing\n      \n      **PALETTE integration:**\n      -\
    \ Store validated rules in Assumptions Register (RIU-008) with testable conditions\n\
    \      - Document decision logic in Decision Log (RIU-003) for future reference\n\
    \      - Feed into Edge-Case Catalog (RIU-014) for testing\n      - Flag assumptions\
    \ that are ONE-WAY DOORs if they drive architecture decisions\n      \n      Critical:\
    \ Capture *why* decisions are made, not just *what* â€” the reasoning is often more\
    \ valuable than the rule itself."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-003
  - RIU-004
  - RIU-008
  - RIU-014
  
  difficulty: medium
  industries:
  - Operations
  - Logistics
  - Manufacturing
  tags:
  - knowledge-capture
  - documentation
  - tacit-knowledge
  - interviews
  sources:
  - title: Unlock organizational wisdom using voice-driven knowledge capture with
      Amazon Transcribe and Amazon Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/unlock-organizational-wisdom-using-voice-driven-knowledge-capture-with-amazon-transcribe-and-amazon-bedrock/
  - title: 'Bridging the Knowledge Gap: Using Generative AI on AWS to Preserve Critical
      Expertise'
    url: https://aws.amazon.com/blogs/industries/bridging-the-knowledge-gap-using-generative-ai-on-aws-to-preserve-critical-expertise/
- id: LIB-010
  question: When should I escalate to reset/reframe vs continue converging?
  answer: "**RESET signals (stop and start over with fresh framing):**\n      - No\
    \ tangible results after agreed timeline (many AI projects abandoned for this)\n\
    \      - Stakeholders still have conflicting definitions of success after 2-3\
    \ alignment attempts\n      - No clear line of sight from AI feature to measurable\
    \ business metric\n      - Technical team owns strategy without cross-functional\
    \ business involvement\n      - Exit criteria thresholds (quality, latency, cost)\
    \ consistently missed\n      \n      **REFRAME signals (change the problem statement,\
    \ keep context):**\n      - Original problem was actually 3 problems disguised\
    \ as one (see LIB-007)\n      - Production reached but scaling challenges emerge\
    \ (load balancing, observability, optimization)\n      - Value chain mapping reveals\
    \ disconnect between feature and business impact\n      - Constraints discovered\
    \ that make original approach infeasible (see LIB-008)\n      \n      **CONTINUE\
    \ CONVERGING when:**\n      - Decisions are TWO-WAY DOORs (reversible, low-cost\
    \ to change)\n      - Governance frameworks in place (user profiles, data access,\
    \ infrastructure templates)\n      - Stakeholders aligned on success criteria,\
    \ just iterating on solution\n      - Progress measurable against agreed milestones\n\
    \      \n      **PALETTE guidance:** If convergence not reached within expected\
    \ exchange window, propose Reset, Fork (try different approach), or Reframe. Silent\
    \ looping is not allowed. Escalate to executive sponsor when reset involves ONE-WAY\
    \ DOOR sunk costs. Document decision in decisions.md with rationale."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-003
  - RIU-006
  difficulty: high
  industries:
  - All
  tags:
  - escalation
  - reset-criteria
  - convergence-failure
  - decision-framework
  sources:
  - title: Practical implementation considerations to close the AI value gap
    url: https://aws.amazon.com/blogs/machine-learning/practical-implementation-considerations-to-close-the-ai-value-gap/
  - title: Organizational AI Vision - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_1_vision_and_strategy/5_1_1_organizational_ai_vision.html
  - title: Why agentic AI marks an inflection point for enterprise modernization
    url: https://aws.amazon.com/blogs/aws-insights/aws-why-agentic-ai-marks-an-inflection-point-for-enterprise-modernization/
  - title: Generative AI operating models in enterprise organizations with Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/generative-ai-operating-models-in-enterprise-organizations-with-amazon-bedrock/
- id: LIB-011
  question: How do I create a semantic blueprint that non-technical stakeholders can
    validate?
  answer: "Translate the 5 Semantic Blueprint elements (RIU-001) into business language:\n\
    \      \n      **Goal**: Use OGSM framework â€” state business outcome, not technical\
    \ metric. Say \"Reduce customer churn by 5%\" not \"Achieve 85% model accuracy.\"\
    \ Avoid vague objectives like \"improve productivity.\"\n      \n      **Roles**:\
    \ Name people and departments, not systems. \"Marketing owns data input; AI team\
    \ owns model; Legal approves before launch\" â€” skip technical architecture.\n\
    \      \n      **Capabilities**: Describe what the system *does for users*, not\
    \ how it works. \"Automatically flags high-risk accounts for review\" not \"Uses\
    \ gradient boosting classifier.\"\n      \n      **Constraints**: Frame as business\
    \ boundaries. \"Must not access customer PII without consent; Budget capped at\
    \ $50K; Must launch before Q3.\"\n      \n      **Non-goals**: Critical for scope\
    \ control. Explicitly state \"This will NOT replace human decision-making / integrate\
    \ with System X / handle edge case Y.\"\n      \n      **Validation techniques:**\n\
    \      - Use AIR workshop format with cross-functional team (business + technical)\n\
    \      - Walk through each element verbally â€” if stakeholder can't explain it\
    \ back, rewrite it\n      - Provide visual assessment showing feasibility, budget,\
    \ ROI, data quality, risks\n      - Get explicit sign-off: \"Do you agree this\
    \ is what we're building and what success looks like?\"\n      \n      **Format**:\
    \ One page maximum. Use bullet points, not paragraphs. If it requires technical\
    \ glossary, it's too complex."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-004
  - RIU-006
  difficulty: medium
  industries:
  - All
  tags:
  - communication
  - stakeholder-validation
  - documentation
  - accessibility
  sources:
  - title: Organizational AI Vision - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_1_vision_and_strategy/5_1_1_organizational_ai_vision.html
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: 'Custom Intelligence: Building AI that matches your business DNA'
    url: https://aws.amazon.com/blogs/machine-learning/custom-intelligence-building-ai-that-matches-your-business-dna/
- id: LIB-012
  question: What's the checklist for 'good enough' convergence before moving to architecture?
  answer: "Use this checklist before proceeding from Convergence to Architecture phase.\
    \ All items should be documented or explicitly marked \"N/A with rationale.\"\n\
    \      \n      **Semantic Blueprint Complete (RIU-001):**\n      - [ ] Goal: Measurable\
    \ business outcome stated (not technical metric)\n      - [ ] Roles: Decision\
    \ authority clear (RACI-lite, RIU-002)\n      - [ ] Capabilities: Required tools/agents\
    \ identified\n      - [ ] Constraints: Hard boundaries documented (budget, compliance,\
    \ timeline)\n      - [ ] Non-goals: Explicit exclusions prevent scope creep\n\
    \      \n      **Stakeholder Alignment:**\n      - [ ] No conflicting definitions\
    \ of success remain\n      - [ ] Executive sponsor identified and committed\n\
    \      - [ ] Non-technical stakeholders can explain the goal back to you\n   \
    \   - [ ] Sign-off obtained on Convergence Brief\n      \n      **Risk & Constraints\
    \ Surfaced (RIU-007, RIU-008):**\n      - [ ] ONE-WAY DOOR decisions identified\
    \ and flagged (RIU-003)\n      - [ ] Compliance requirements assessed (HIPAA,\
    \ GDPR, EU AI Act)\n      - [ ] Data governance framework confirmed (critical\
    \ ONE-WAY DOOR)\n      - [ ] Hidden constraints discovered (see LIB-008 questions)\n\
    \      - [ ] Assumptions documented with validation plan\n      \n      **Technical\
    \ Readiness:**\n      - [ ] Proof of value demonstrated (PoC exit criteria met)\n\
    \      - [ ] Data availability and quality confirmed\n      - [ ] Proven architectural\
    \ patterns identified (e.g., RAG vs. fine-tuning)\n      - [ ] Unit economics\
    \ estimated (cost per request, token usage)\n      \n      **AWS Phase Gate Alignment:**\n\
    \      Per Gen AI Adoption framework: Use Case Discovery âœ“ â†’ Business Case with\
    \ Success Criteria âœ“ â†’ Data/Model Foundation â†’ Security/Compliance â†’ Responsible\
    \ AI â†’ Development â†’ Deployment\n      \n      **Gate criteria:** If any checkbox\
    \ is incomplete and not \"N/A with rationale,\" you haven't converged â€” return\
    \ to discovery."
  problem_type: Intake_and_Convergence
  related_rius:
  - RIU-001
  - RIU-002
  - RIU-003
  - RIU-007
  - RIU-008
  difficulty: medium
  industries:
  - All
  tags:
  - convergence-validation
  - phase-gates
  - quality-criteria
  - checkpoints
  sources:
  - title: Implementation Considerations and Challenges - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/1_0_generative_ai_fundamentals/1_3_implementation_consideration_and_challenges/1_3_implementation_consideration_and_challenges.html
  - title: AI Pitch Deck for Business Decision Makers
    url: https://aws.highspot.com/items/632e4908931439bbe94f83c9#1
  - title: 'Bridging the Knowledge Gap: Using Generative AI on AWS to Preserve Critical
      Expertise'
    url: https://aws.amazon.com/blogs/industries/bridging-the-knowledge-gap-using-generative-ai-on-aws-to-preserve-critical-expertise/
  - title: Business Value and use cases - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/1_0_generative_ai_fundamentals/1_2_business_value_and_use_cases/1_2_business_value_and_use_cases.html
- id: LIB-014
  question: What's the best way to model exception-heavy workflows in AI systems?
  answer: "Exception-heavy workflows require a layered architecture that separates\
    \ the \"happy path\" from exception handling:\n      \n      **Architecture pattern:**\n\
    \      1. **Deterministic layer first**: Handle known, rule-based exceptions with\
    \ explicit IF/THEN logic â€” these are predictable and testable\n      2. **AI layer\
    \ for ambiguity**: Use LLM/agents only for cases that require judgment or context\
    \ understanding\n      3. **Human-in-the-loop escape hatch**: Route unconfident\
    \ or high-stakes exceptions to human review\n      \n      **Design principles:**\n\
    \      - Use adaptive workflows (AI-DLC pattern) that modulate depth based on\
    \ complexity â€” simple cases get fast path, exceptions get deeper analysis\n  \
    \    - Implement fault isolation: exceptions in one branch shouldn't cascade to\
    \ others\n      - Apply MCP architecture for standardized tool integration across\
    \ exception handlers\n      - Build layered protection based on user personas,\
    \ data characteristics, and failure modes\n      \n      **Exception categorization:**\n\
    \      - **Known exceptions**: Catalog in Edge-Case Catalog (RIU-014), handle\
    \ deterministically\n      - **Unknown-but-bounded**: AI handles within guardrails,\
    \ logs for review\n      - **Unbounded/novel**: Route to human immediately\n \
    \     \n      **Testing strategy:**\n      - Test guardrail effectiveness explicitly\
    \ â€” inject edge cases and verify behavior\n      - Use reinforcement learning\
    \ in simulated environments (Amazon Nova Act achieves 90%+ reliability this way)\n\
    \      - Validate that exception paths actually execute â€” dead code is common\
    \ in exception handling\n      \n      **PALETTE integration:**\n      - Document\
    \ exception categories in RIU-014 (Edge-Case Catalog)\n      - Flag exception-handling\
    \ logic that involves ONE-WAY DOOR decisions\n      - Store exception patterns\
    \ in Assumptions Register (RIU-008) for validation\n      \n      Key insight:\
    \ Don't try to handle all exceptions with AI â€” use AI for judgment, deterministic\
    \ code for known patterns, humans for novel/high-stakes cases."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-008
  - RIU-014
  
  
  
  difficulty: high
  industries:
  - Operations
  - Logistics
  - Healthcare
  - Finance
  tags:
  - exception-handling
  - workflow-modeling
  - edge-cases
  - system-design
  sources:
  - title: AI agent-driven browser automation for enterprise workflow management
    url: https://aws.amazon.com/blogs/machine-learning/ai-agent-driven-browser-automation-for-enterprise-workflow-management/
  - title: Open-Sourcing Adaptive Workflows for AI-Driven Development Life Cycle (AI-DLC)
    url: https://aws.amazon.com/blogs/devops/open-sourcing-adaptive-workflows-for-ai-driven-development-life-cycle-ai-dlc/
  - title: Streamline GitHub workflows with generative AI using Amazon Bedrock and
      MCP
    url: https://aws.amazon.com/blogs/machine-learning/streamline-github-workflows-with-generative-ai-using-amazon-bedrock-and-mcp/
  - title: 'Planning for failure: How to make generative AI workloads more resilient'
    url: https://aws.amazon.com/blogs/publicsector/planning-for-failure-how-to-make-generative-ai-workloads-more-resilient/
  - title: Build reliable AI agents for UI workflow automation with Amazon Nova Act
    url: https://aws.amazon.com/blogs/aws/build-reliable-ai-agents-for-ui-workflow-automation-with-amazon-nova-act-now-generally-available/
- id: LIB-015
  question: How do I turn 'we'll know it when we see it' into measurable acceptance
    criteria?
  answer: "\"We'll know it when we see it\" signals subjective quality expectations.\
    \ Use these techniques to make them measurable:\n      \n      **Step 1: Extract\
    \ concrete examples**\n      - Ask: \"Show me 3 examples of good output and 3\
    \ examples of bad output\"\n      - Ask: \"What specifically makes this one good?\
    \ What's missing from the bad one?\"\n      - Document these as your initial Golden\
    \ Set (RIU-021)\n      \n      **Step 2: Apply the 4 evaluation frameworks**\n\
    \      1. **LLM-as-a-Judge**: Use Amazon Bedrock Evaluations with custom metrics\
    \ â€” define your own criteria alongside built-in metrics\n      2. **Rubric-Based\
    \ Evaluation**: Create scoring rubric (1-5 scale) with explicit criteria for each\
    \ level\n      3. **Traditional Metrics**: Where applicable, add objective measures\
    \ (latency, cost, format compliance)\n      4. **Domain-Specific**: Map to business\
    \ outcomes (response time â†’ resolution rate â†’ customer satisfaction)\n      \n\
    \      **Step 3: Build measurable proxies**\n      For each subjective criterion,\
    \ identify 2-3 quantifiable proxies:\n      - \"Sounds professional\" â†’ No grammar\
    \ errors + formal tone score (LLM-judge) + no slang detected\n      - \"Helpful\
    \ response\" â†’ Contains action items + answers the question asked + user follow-up\
    \ rate\n      - \"Accurate\" â†’ Factual claims verified against source + no hallucinated\
    \ entities + citation coverage\n      \n      **Step 4: Validate with stakeholders**\n\
    \      - Run evaluation on 50+ examples, show results\n      - Ask: \"Does a score\
    \ of 4.2 on this rubric match what you'd call 'good enough'?\"\n      - Adjust\
    \ thresholds until metrics align with human judgment (target Ï > 0.8 correlation)\n\
    \      \n      **PALETTE integration:**\n      - Document criteria in Success\
    \ Metrics Charter (RIU-006)\n      - Store Golden Set in RIU-021 for regression\
    \ testing\n      - Define exit criteria: \"Acceptance requires score â‰¥ X on rubric\
    \ across Y% of test cases\"\n      \n      Key insight: Evaluation is the single\
    \ most important component for GenAI success â€” without it, you risk deploying\
    \ models that fail silently."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-001
  - RIU-006
  - RIU-021
  
  
  difficulty: critical
  industries:
  - All
  tags:
  - acceptance-criteria
  - measurement
  - validation
  - quality-definition
  sources:
  - title: Model Evaluation and Selection Criteria Overview - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_6_model_evaluation_and_selection_criteria/index.html
  - title: Evaluation Techniques - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_6_model_evaluation_and_selection_criteria/2_6_3_evaluation_technique/2_6_3_evaluation_techniques.html
  - title: Use custom metrics to evaluate your generative AI application with Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/
  - title: 'Beyond pilots: A proven framework for scaling AI to production'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-pilots-a-proven-framework-for-scaling-ai-to-production/
- id: LIB-016
  question: What prompt engineering patterns work for translating policy documents
    into LLM behavior?
  answer: "Use a layered approach combining prompt structure, guardrails, and validation:\n\
    \      \n      **Prompt structure for policies:**\n      - **System prompt**:\
    \ Set role and global constraints (\"You are a compliance assistant. Never provide\
    \ advice that violates [Policy X].\")\n      - **Context section**: Include relevant\
    \ policy excerpts (use RAG for large policy corpora)\n      - **Instruction**:\
    \ Specific task with policy reference (\"Answer the user's question following\
    \ Section 3.2 guidelines\")\n      - **Constraints**: Explicit prohibitions (\"\
    Do NOT discuss [prohibited topics]. If asked, respond with [approved deflection].\"\
    )\n      - **Examples**: Few-shot samples showing compliant vs. non-compliant\
    \ responses\n      \n      **Parameter settings for compliance:**\n      - Temperature:\
    \ 0.0-0.3 (prioritize consistency over creativity)\n      - Top_p: 0.5-0.7 (restrict\
    \ output variance)\n      - Use deterministic settings for policy-critical responses\n\
    \      \n      **Guardrails layer (Amazon Bedrock Guardrails):**\n      - Content\
    \ filters for harmful/inappropriate content\n      - Denied topics aligned with\
    \ policy prohibitions\n      - Word filters for restricted terminology\n     \
    \ - Sensitive information filters (PII/PHI detection and masking)\n      - IAM\
    \ policy-based enforcement for mandatory guardrails on every inference call\n\
    \      \n      **Validation approach:**\n      - **Positive testing**: Legitimate\
    \ policy-compliant queries pass correctly\n      - **Negative testing**: Prohibited\
    \ content/topics are blocked\n      - Test edge cases where policies conflict\
    \ or are ambiguous\n      - Version control prompts (RIU-520) for audit trail\
    \ and controlled updates\n      \n      **PALETTE integration:**\n      - Document\
    \ policy-to-prompt mapping in RIU-022 (Prompt Interface Contract)\n      - Store\
    \ prompt versions in RIU-520 (Prompt Version Control)\n      - Define guardrail\
    \ requirements in Constraint Profile (RIU-007)\n      - Flag policy interpretations\
    \ that are ONE-WAY DOORs (require legal/compliance sign-off)\n      \n      Key\
    \ insight: Prompts alone aren't sufficient â€” use Bedrock Guardrails as a defense-in-depth\
    \ layer that enforces policies even if prompts are bypassed or jailbroken."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-007
  - RIU-022
  - RIU-500
  - RIU-501
  - RIU-520
  difficulty: high
  industries:
  - Compliance
  - Legal
  - Government
  - Healthcare
  tags:
  - prompt-engineering
  - policy-translation
  - llm-behavior
  - compliance
  sources:
  - title: The Input Interface - Prompts and common LLM Parameters
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_1_key_primitives/2_1_1_prompt/2_1_1_prompt.html
  - title: Implement model-independent safety measures with Amazon Bedrock Guardrails
    url: https://aws.amazon.com/blogs/machine-learning/implement-model-independent-safety-measures-with-amazon-bedrock-guardrails/
  - title: Amazon Bedrock Guardrails announces IAM Policy-based enforcement
    url: https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-guardrails-announces-iam-policy-based-enforcement-to-deliver-safe-ai-interactions/
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
- id: LIB-017
  question: How do I capture human judgment that's based on years of experience, not
    explicit rules?
  answer: "Experiential judgment is pattern recognition that experts can't articulate\
    \ as rules. Capture it through examples, feedback loops, and calibration â€” not\
    \ interviews alone.\n      \n      **Elicitation techniques:**\n      - **Case-based\
    \ extraction**: Present real scenarios and ask \"What would you do? Why?\" Record\
    \ decisions, not just rules\n      - **Contrastive pairs**: Show two similar cases\
    \ with different outcomes â€” \"Why did you handle these differently?\"\n      -\
    \ **Think-aloud protocol**: Have expert narrate while working real cases (use\
    \ voice capture with Transcribe + Bedrock)\n      - **Calibration sessions**:\
    \ Show AI outputs to expert, ask \"Would you have done this?\" â€” disagreements\
    \ reveal tacit criteria\n      \n      **Learning architectures:**\n      1. **Feedback\
    \ Loop HITL**: Expert reviews AI outputs, corrections feed back into system\n\
    \      2. **RLHF (Reinforcement Learning from Human Feedback)**: Fine-tune models\
    \ using expert preferences on output pairs\n      3. **RLAIF**: When expert time\
    \ is limited, use AI-generated feedback (reduces SME workload ~80%)\n      4.\
    \ **Self-learning system**: Use disagreements between models as learning signals\
    \ (Amazon Catalog pattern â€” supervisor model resolves conflicts, builds hierarchical\
    \ knowledge base)\n      \n      **AWS tools:**\n      - Amazon SageMaker Ground\
    \ Truth Plus for preference datasets and demonstration data\n      - Amazon Bedrock\
    \ for fine-tuning with human feedback\n      - Amazon Transcribe for voice-based\
    \ knowledge capture\n      \n      **What CAN'T be captured:**\n      - Judgment\
    \ requiring real-time sensory input (smell, touch, visual nuance)\n      - Decisions\
    \ requiring context the system can't access\n      - Novel situations outside\
    \ training distribution\n      - Flag these for permanent human-in-the-loop (Escalation-Based\
    \ HITL pattern)\n      \n      **PALETTE integration:**\n      - Store captured\
    \ judgment patterns in Assumptions Register (RIU-008) as hypotheses to validate\n\
    \      - Document expert disagreements â€” these reveal edge cases (RIU-014)\n \
    \     - Flag judgment calls that are ONE-WAY DOORs (require human approval even\
    \ after training)\n      \n      Key insight: Don't ask experts to explain rules\
    \ â€” show them cases and capture their reactions. Judgment lives in the delta between\
    \ what they do and what a naive system would do."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-008
  - RIU-014
  
  
  - RIU-500
  difficulty: critical
  industries:
  - Operations
  - Customer Support
  - Logistics
  - Healthcare
  tags:
  - expertise-capture
  - implicit-knowledge
  - judgment-modeling
  - experience
  sources:
  - title: Human-in-the-Loop for GenAI Systems - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_1_system_and_application_design_patterns_for_genai/3_1_1_foundation_architecture_components/3_1_1_8_additional_components/3_1_1_8_1_human_in_the_loop/3_1_1_8_1_human_in_the_loop.html
  - title: Fine Tuning with Reinforcement Learning from Human Feedback (RLHF)
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_3_core_archtectural_concepts/2_3_4_fine-tuning/2_3_4-3_Preference
      Alignment/2_3_4_3_1_reinforcement_learning_from_human_feedback(RLHF)/rlhf.html
  - title: High-quality human feedback for your generative AI applications from Amazon
      SageMaker Ground Truth Plus
    url: https://aws.amazon.com/blogs/machine-learning/high-quality-human-feedback-for-your-generative-ai-applications-from-amazon-sagemaker-ground-truth-plus/
  - title: How the Amazon.com Catalog Team built self-learning generative AI at scale
    url: https://aws.amazon.com/blogs/machine-learning/how-the-amazon-com-catalog-team-built-self-learning-generative-ai-at-scale-with-amazon-bedrock/
- id: LIB-018
  question: What's the difference between deterministic rules and probabilistic AI
    for business logic?
  answer: "**Deterministic rules (Automated Reasoning):**\n      - Uses formal logic\
    \ and mathematical proofs\n      - 100% accuracy when assumptions are correct\n\
    \      - Outputs: valid / invalid / satisfiable with explanation\n      - Best\
    \ for: yes/no questions, always/never policies, compliance validation\n      -\
    \ Examples: HR policies, regulations, operational workflows, access control\n\
    \      - AWS tool: Automated Reasoning checks in Amazon Bedrock Guardrails (translates\
    \ up to 100-page policy docs into logical models)\n      - Tradeoff: Requires\
    \ well-defined rules; struggles with novel situations; maintenance burden as rules\
    \ multiply\n      \n      **Probabilistic AI (Machine Learning):**\n      - Uses\
    \ statistical patterns from data\n      - Generalized predictions, not 100% accurate\n\
    \      - Outputs: predictions with confidence scores\n      - Best for: pattern\
    \ recognition, unstructured data, complex decisions with many variables\n    \
    \  - Examples: fraud detection, demand forecasting, sentiment analysis, recommendations\n\
    \      - Tradeoff: Less explainable; requires training data; may produce unexpected\
    \ outputs\n      \n      **When to use which:**\n      | Scenario | Use Deterministic\
    \ | Use Probabilistic |\n      |----------|-------------------|-------------------|\n\
    \      | \"Is this allowed by policy?\" | âœ… | |\n      | \"What's the risk score?\"\
    \ | | âœ… |\n      | \"Does this meet compliance?\" | âœ… | |\n      | \"What will\
    \ customer likely do?\" | | âœ… |\n      | \"Is this data valid?\" | âœ… | |\n   \
    \   | \"What's the best response?\" | | âœ… |\n      \n      **Hybrid architecture\
    \ (recommended):**\n      Use RIU-023 (Deterministic-First Pipeline Split):\n\
    \      1. **Deterministic layer first**: Validate inputs, check policy compliance,\
    \ apply business rules\n      2. **Probabilistic layer second**: AI handles ambiguous\
    \ cases, generates content, makes predictions\n      3. **Deterministic guardrails\
    \ around AI**: Validate AI outputs against rules before returning\n      \n  \
    \    Key insight: Deterministic rules tell you what's *allowed*; probabilistic\
    \ AI tells you what's *likely*. Use both â€” rules as guardrails, AI for judgment."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-023
  
  
  - RIU-500
  difficulty: medium
  industries:
  - All
  tags:
  - system-design
  - deterministic-vs-probabilistic
  - architecture
  - tradeoffs
  sources:
  - title: Automated Reasoning checks on Amazon Bedrock - Technical Deep Dive
    url: https://broadcast.amazon.com/videos/1648600
  - title: Build trusted AI with Automated Reasoning checks in Bedrock Guardrails
    url: https://www.youtube.com/watch?v=FyvWSkEWkuc
  - title: Powering Business Process Automation with Machine Learning Using Pega and
      Amazon SageMaker
    url: https://aws.amazon.com/blogs/apn/powering-business-process-automation-with-machine-learning-using-pega-and-amazon-sagemaker/
- id: LIB-019
  question: How do I version control business rules that change frequently?
  answer: "Use a layered approach: store rules in versioned data stores, track changes\
    \ with event streams, and integrate with CI/CD for governance.\n      \n     \
    \ **Versioning patterns (Amazon DynamoDB):**\n      - **Time-based**: Use timestamp\
    \ in sort key (`rule_id#2024-06-15T10:30:00Z`) â€” good for audit trails\n     \
    \ - **Number-based**: Use version prefix + atomic counter (`v#0001`, `v#0002`)\
    \ â€” good for rollback\n      - **Optimistic concurrency**: Include version number\
    \ in writes, reject stale updates â€” prevents conflicts\n      \n      **Change\
    \ tracking:**\n      - Enable DynamoDB Streams to capture all rule changes\n \
    \     - Lambda function processes stream â†’ logs to audit table, triggers notifications\n\
    \      - Decouples change detection from rule execution\n      \n      **Architecture\
    \ pattern (Step Functions + DynamoDB):**\n      1. API Gateway receives rule update\
    \ request\n      2. Step Functions orchestrates validation â†’ approval â†’ deployment\n\
    \      3. DynamoDB stores rule versions with metadata (author, timestamp, approval\
    \ status)\n      4. Lambda handles audit logging and downstream notifications\n\
    \      \n      **For complex rules engines:**\n      - Store rule definitions\
    \ in Amazon S3 (JSON/YAML files) with S3 versioning enabled\n      - Store rule\
    \ configuration/weights in Amazon Aurora\n      - Store execution results in DynamoDB\n\
    \      - Enables non-technical users to update rules without IT involvement\n\
    \      \n      **CI/CD integration:**\n      - Treat rules as code: store in Git\
    \ repository\n      - PR review for rule changes (especially ONE-WAY DOOR changes)\n\
    \      - Automated testing of rule logic before deployment\n      - API-driven\
    \ deployment to production\n      \n      **PALETTE integration:**\n      - Document\
    \ rule changes in Decision Log (RIU-003) when they affect system behavior\n  \
    \    - Flag rule changes that are ONE-WAY DOORs (compliance rules, pricing logic)\n\
    \      - Use RIU-044 (Business Rules Documentation) for rule catalog\n      -\
    \ Track rule assumptions in Assumptions Register (RIU-008)\n      \n      **Rollback\
    \ strategy:**\n      - Maintain N previous versions (recommend: at least 5)\n\
    \      - Test rollback procedure before you need it\n      - Include rollback\
    \ in incident runbook (RIU-062)\n      \n      Key insight: Version the rule *definition*\
    \ separately from the rule *execution state*. You need to know what rules were\
    \ active at any point in time for audit and debugging."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-003
  - RIU-008
  
  
  - RIU-062
  - RIU-532
  difficulty: medium
  industries:
  - Operations
  - Compliance
  - Finance
  tags:
  - version-control
  - rule-management
  - change-tracking
  - governance
  sources:
  - title: Implementing version control using Amazon DynamoDB
    url: https://aws.amazon.com/blogs/database/implementing-version-control-using-amazon-dynamodb/
  - title: Using AWS Step Functions and Amazon DynamoDB for business rules orchestration
    url: https://aws.amazon.com/blogs/compute/using-aws-step-functions-and-amazon-dynamodb-for-business-rules-orchestration/
  - title: Building an Agile Business Rules Engine on AWS
    url: https://aws.amazon.com/blogs/apn/building-an-agile-business-rules-engine-on-aws/
  - title: 'Amazon QuickSight BIOps â€“ Part 2: Version control using APIs'
    url: https://aws.amazon.com/blogs/business-intelligence/amazon-quicksight-biops-part-2-version-control-using-apis/
- id: LIB-020
  question: What's the best format for documenting 'tribal knowledge' so it's machine-readable?
  answer: "Choose format based on knowledge type and consumption pattern:\n      \n\
    \      **For decision logic / business rules:**\n      ```yaml\n      rule_id:\
    \ \"escalation_001\"\n      condition: \"customer_tier == 'enterprise' AND issue_severity\
    \ >= 3\"\n      action: \"route_to_senior_support\"\n      exceptions: [\"holiday_hours\"\
    , \"maintenance_window\"]\n      source: \"SME: Jane Smith, 2024-01\"\n      confidence:\
    \ \"validated\"\n      ```\n      - Use YAML or JSON for structured rules\n  \
    \    - Include source attribution and validation status\n      - Store in version\
    \ control for audit trail\n      \n      **For procedural knowledge / how-to:**\n\
    \      - Markdown with structured headers and metadata frontmatter\n      - Chunk\
    \ by logical section (one procedure per document)\n      - Include: prerequisites,\
    \ steps, expected outcomes, common errors\n      - Optimize chunk size for RAG\
    \ retrieval (500-1000 tokens typical)\n      \n      **For Q&A / FAQ knowledge:**\n\
    \      ```json\n      {\n        \"question\": \"When do we escalate to legal?\"\
    ,\n        \"answer\": \"Escalate when...\",\n        \"keywords\": [\"legal\"\
    , \"escalation\", \"compliance\"],\n        \"source\": \"Policy Manual 3.2\"\
    ,\n        \"last_validated\": \"2024-06-01\"\n      }\n      ```\n      \n  \
    \    **Storage and retrieval options (AWS):**\n      - **Amazon Bedrock Knowledge\
    \ Bases**: Ingest documents, auto-chunk, vector embed\n      - **Amazon S3 Vectors**:\
    \ Cost-effective for large-scale RAG (90% cost reduction)\n      - **Amazon OpenSearch\
    \ Serverless**: Hybrid search (keyword + semantic)\n      - **Structured data**:\
    \ Keep in Redshift/Glue, use text-to-SQL for queries\n      \n      **Metadata\
    \ schema (always include):**\n      - `source`: Who provided this knowledge\n\
    \      - `last_validated`: When was it confirmed accurate\n      - `confidence`:\
    \ draft | validated | deprecated\n      - `domain`: Business area / topic tags\n\
    \      - `related_docs`: Links to related knowledge\n      \n      **PALETTE integration:**\n\
    \      - Store rules in Assumptions Register format (RIU-008) with testable conditions\n\
    \      - Document edge cases separately (RIU-014) for testing\n      - Use RIU-044\
    \ for rule versioning and change tracking\n      \n      Key insight: Machine-readable\
    \ â‰  machine-generated. Structure matters more than format â€” consistent schema\
    \ with metadata enables retrieval, validation, and maintenance."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-004
  - RIU-008
  - RIU-014
  
  
  difficulty: medium
  industries:
  - All
  tags:
  - documentation
  - knowledge-representation
  - machine-readable
  - formats
  sources:
  - title: Structured Data Retrieval Augmented Generation (RAG) - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_3_core_archtectural_concepts/2_3_3_RAG(retrieval
      Augmented Generation)/2_3_3-6-Structured RAG/2_3_3-6-Structured RAG.html
  - title: Building cost-effective RAG applications with Amazon Bedrock Knowledge
      Bases and Amazon S3 Vectors
    url: https://aws.amazon.com/blogs/machine-learning/building-cost-effective-rag-applications-with-amazon-bedrock-knowledge-bases-and-amazon-s3-vectors/
  - title: Choosing the right approach for generative AI-powered structured data retrieval
    url: https://aws.amazon.com/blogs/machine-learning/choosing-the-right-approach-for-generative-ai-powered-structured-data-retrieval/
  - title: Unlock organizational wisdom using voice-driven knowledge capture
    url: https://aws.amazon.com/blogs/machine-learning/unlock-organizational-wisdom-using-voice-driven-knowledge-capture-with-amazon-transcribe-and-amazon-bedrock/
- id: LIB-021
  question: How do I handle business rules that conflict across departments?
  answer: "Rule conflicts are natural in enterprises â€” don't try to eliminate tension,\
    \ establish governance to resolve it systematically.\n      \n      **Conflict\
    \ detection:**\n      - Document all rules with owning department and business\
    \ justification\n      - During rule ingestion, check for overlapping conditions\
    \ with different outcomes\n      - Flag conflicts explicitly: \"Rule A (Sales)\
    \ says X; Rule B (Compliance) says Y\"\n      - Store in centralized rule catalog\
    \ (RIU-044) with cross-references\n      \n      **Resolution governance (Hybrid\
    \ CoE model):**\n      - **Executive Sponsor**: Final arbiter for unresolved conflicts\
    \ affecting strategy\n      - **AI Governance Lead**: Day-to-day conflict triage\
    \ and resolution tracking\n      - **Cross-Functional Oversight Team**: Representatives\
    \ from each department evaluate conflicts\n      - Use AIR workshop methodology\
    \ when prioritization disputes arise\n      \n      **Priority framework:**\n\
    \      1. **Regulatory/Compliance rules** â†’ Always highest priority (non-negotiable)\n\
    \      2. **Security/Safety rules** â†’ Second priority\n      3. **Customer-facing\
    \ rules** â†’ Third priority\n      4. **Operational efficiency rules** â†’ Lowest\
    \ priority, most negotiable\n      \n      Document priority hierarchy in Constraint\
    \ Profile (RIU-007) and get executive sign-off.\n      \n      **Resolution patterns:**\n\
    \      - **Scope separation**: Rules apply to different contexts (e.g., \"Sales\
    \ rule for prospects, Compliance rule for regulated customers\")\n      - **Time-based\
    \ precedence**: Newer rule supersedes unless explicitly versioned\n      - **Escalation**:\
    \ Use \"Disagree and Commit\" â€” debate respectfully, then fully commit to decision\n\
    \      - **Merge**: Create unified rule that satisfies both intents\n      \n\
    \      **Technical implementation:**\n      - Implement rule priority field (integer)\
    \ in rule schema\n      - When conflicts detected at runtime, highest priority\
    \ wins\n      - Log all conflict resolutions for audit trail\n      - Alert when\
    \ new rules create conflicts with existing rules\n      \n      **PALETTE integration:**\n\
    \      - Document conflict resolutions in Decision Log (RIU-003) as they're often\
    \ ONE-WAY DOORs\n      - Track unresolved conflicts in Open Questions until governance\
    \ resolves\n      - Store harmonized rules in Assumptions Register (RIU-008) with\
    \ validation plan\n      \n      Key insight: The goal isn't eliminating conflicts\
    \ â€” it's making conflict resolution fast, transparent, and auditable. Establish\
    \ the governance *before* you need it."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-003
  - RIU-007
  - RIU-008
  
  
  
  difficulty: high
  industries:
  - Enterprise SaaS
  - Operations
  - Finance
  tags:
  - conflict-resolution
  - cross-functional
  - rule-harmonization
  - governance
  sources:
  - title: Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/4_0_systematic_path_to_production_framework/4_4_governance/index.html
  - title: Organizational Design and Team Structure for AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_2_organizational_design_team_structure.html
  - title: 'Governance by design: The essential guide for successful AI scaling'
    url: https://aws.amazon.com/blogs/machine-learning/governance-by-design-the-essential-guide-for-successful-ai-scaling/
  - title: Disagree and Commit - AWS Enterprise Strategy
    url: https://aws.amazon.com/blogs/enterprise-strategy/guts-part-three-having-backbone-disagreeing-and-committing/
- id: LIB-022
  question: What testing strategy validates that AI behavior matches human expert
    judgment?
  answer: "Use a multi-layered validation strategy: create expert-labeled datasets,\
    \ measure agreement, automate with LLM-as-a-Judge, and maintain human oversight.\n\
    \      \n      **Step 1: Create Golden Set with experts (RIU-021)**\n      - Generate\
    \ candidate Q&A pairs using LLM, then have experts review/correct\n      - Use\
    \ FMEval triplet format: (question, context, expected_answer)\n      - Include\
    \ edge cases and failure modes discovered during testing\n      - Amazon SageMaker\
    \ Ground Truth Plus provides expert workforce for labeling\n      - Minimum: 50-100\
    \ examples for initial validation; 500+ for robust evaluation\n      \n      **Step\
    \ 2: Measure agreement metrics**\n      - **Recall**: Does AI find what experts\
    \ find?\n      - **Precision**: Does AI avoid false positives experts would reject?\n\
    \      - **F1 Score**: Balanced measure of both\n      - **Win rate**: In head-to-head\
    \ comparison, how often does AI match/beat expert?\n      - **Confidence intervals**:\
    \ Statistical significance of agreement\n      - Target: >80% agreement with expert\
    \ judgment (Ï > 0.8 correlation)\n      \n      **Step 3: Automate with LLM-as-a-Judge**\n\
    \      - Use Amazon Nova LLM-as-a-Judge for unbiased cross-model evaluation\n\
    \      - Use judge from *different model family* to avoid self-preference bias\n\
    \      - Version control evaluation prompts in prompt registry\n      - Validate\
    \ judge outputs against human-labeled subset periodically\n      - Integrate into\
    \ CI/CD with threshold scores (stage-gate testing)\n      \n      **Step 4: Scale\
    \ with RLAIF**\n      - When expert time is limited, use RLAIF (AI-generated feedback)\n\
    \      - Reduces SME workload by ~80% while maintaining quality\n      - Still\
    \ require periodic human audit of AI judge accuracy\n      \n      **Step 5: Production\
    \ validation**\n      - Blue-green deployment: route % of traffic to new model,\
    \ compare outputs\n      - Major releases: require full or partial human evaluation\
    \ before rollout\n      - Escalation-based HITL: route low-confidence outputs\
    \ to human experts\n      - Monitor agreement metrics continuously in production\n\
    \      \n      **PALETTE integration:**\n      - Store Golden Set in RIU-021 (Golden\
    \ Set + Offline Evaluation Harness)\n      - Document evaluation thresholds in\
    \ Success Metrics Charter (RIU-006)\n      - Track expert disagreements as edge\
    \ cases (RIU-014)\n      - Log validation results in decisions.md when they affect\
    \ deployment decisions\n      \n      Key insight: Human experts are ground truth,\
    \ but they don't scale. Use experts to calibrate automated evaluation, then automate\
    \ â€” but always maintain human audit loop."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-006
  - RIU-014
  - RIU-021
  
  
  - RIU-540
  difficulty: high
  industries:
  - All
  tags:
  - testing
  - validation
  - expert-comparison
  - quality-assurance
  sources:
  - title: Ground truth generation and review best practices for evaluating generative
      AI with FMEval
    url: https://aws.amazon.com/blogs/machine-learning/ground-truth-generation-and-review-best-practices-for-evaluating-generative-ai-question-answering-with-fmeval/
  - title: Ground truth curation and metric interpretation best practices with FMEval
    url: https://aws.amazon.com/blogs/machine-learning/ground-truth-curation-and-metric-interpretation-best-practices-for-evaluating-generative-ai-question-answering-using-fmeval/
  - title: High-quality human feedback from Amazon SageMaker Ground Truth Plus
    url: https://aws.amazon.com/blogs/machine-learning/high-quality-human-feedback-for-your-generative-ai-applications-from-amazon-sagemaker-ground-truth-plus/
  - title: Evaluating generative AI models with Amazon Nova LLM-as-a-Judge
    url: https://aws.amazon.com/blogs/machine-learning/evaluating-generative-ai-models-with-amazon-nova-llm-as-a-judge-on-amazon-sagemaker-ai/
  - title: Model Evaluation and Selection Criteria Overview - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_6_model_evaluation_and_selection_criteria/index.html
- id: LIB-023
  question: How do I build a decision tree from unstructured interview transcripts?
  answer: "Use a 4-stage pipeline: Capture â†’ Extract â†’ Structure â†’ Validate.\n   \
    \   \n      **Stage 1: Capture and transcribe**\n      - Record SME interviews\
    \ (voice capture reduces cognitive load)\n      - Use Amazon Transcribe for speech-to-text\
    \ conversion\n      - Preserve speaker identification for multi-person interviews\n\
    \      - Store raw transcripts in S3 for audit trail\n      \n      **Stage 2:\
    \ Extract decision logic with LLM**\n      Use structured prompts to identify\
    \ decision points:\n      ```\n      Analyze this interview transcript and extract:\n\
    \      1. All decision points where the expert chooses between options\n     \
    \ 2. The conditions/factors they consider for each decision\n      3. The outcomes/actions\
    \ for each branch\n      4. Any exceptions or edge cases mentioned\n      \n \
    \     Format as: IF [conditions] THEN [action] ELSE [alternative]\n      Flag\
    \ any \"it depends\" statements for follow-up.\n      ```\n      \n      - Use\
    \ Amazon Bedrock (Claude, Nova) for extraction\n      - Multi-agent pipeline:\
    \ one agent for classification, one for rule extraction, one for validation\n\
    \      - Extract entities and relationships for knowledge graph (Neo4j + Bedrock)\n\
    \      \n      **Stage 3: Structure into decision tree**\n      - Convert extracted\
    \ IF/THEN statements into tree format:\n      ```yaml\n      decision_node:\n\
    \        id: \"escalation_decision\"\n        question: \"Is customer tier enterprise?\"\
    \n        conditions:\n          - branch: \"yes\"\n            next: \"check_severity\"\
    \n          - branch: \"no\"\n            action: \"standard_support\"\n     \
    \   source: \"transcript_001, timestamp 12:34\"\n      ```\n      - Link nodes\
    \ to form complete tree\n      - Identify gaps where branches are undefined\n\
    \      - Store in machine-readable format (YAML/JSON) per LIB-020\n      \n  \
    \    **Stage 4: Validate with SME**\n      - Walk through extracted tree with\
    \ original expert\n      - Present edge cases: \"If X and Y, the tree says Z â€”\
    \ is that correct?\"\n      - Probe gaps: \"What happens when [undefined condition]?\"\
    \n      - Update tree based on corrections\n      - Document validation in Assumptions\
    \ Register (RIU-008)\n      \n      **Quality checks:**\n      - Every leaf node\
    \ has an action (no dead ends)\n      - Every condition has both true/false branches\n\
    \      - Source attribution for each rule (traceability)\n      - Edge cases cataloged\
    \ separately (RIU-014)\n      \n      **PALETTE integration:**\n      - Store\
    \ validated decision trees in RIU-044 (Business Rules Documentation)\n      -\
    \ Track unvalidated branches as Assumptions (RIU-008)\n      - Flag rules that\
    \ are ONE-WAY DOORs (compliance, safety decisions)\n      - Use extracted trees\
    \ to build Golden Set for testing (RIU-021)\n      \n      Key insight: LLMs extract\
    \ *candidate* decision logic; SMEs *validate* it. Never deploy an extracted tree\
    \ without expert review â€” transcripts contain noise, tangents, and incomplete\
    \ thoughts."
  problem_type: Human_to_System_Translation
  related_rius:
  - RIU-008
  - RIU-014
  - RIU-021
  
  
  
  difficulty: medium
  industries:
  - Operations
  - Customer Support
  - Compliance
  tags:
  - decision-trees
  - interview-analysis
  - knowledge-extraction
  - modeling
  sources:
  - title: Unearth insights from audio transcripts using Amazon Transcribe and Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/unearth-insights-from-audio-transcripts-generated-by-amazon-transcribe-using-amazon-bedrock/
  - title: 'Build a domain-aware data preprocessing pipeline: A multi-agent collaboration
      approach'
    url: https://aws.amazon.com/blogs/machine-learning/build-a-domainâ€aware-data-preprocessing-pipeline-a-multiâ€agent-collaboration-approach/
  - title: Leveraging Neo4j and Amazon Bedrock for knowledge graphs
    url: https://aws.amazon.com/blogs/apn/leveraging-neo4j-and-amazon-bedrock-for-an-explainable-secure-and-connected-generative-ai-solution/
  - title: Extract data from documents using multimodal Generative AI models
    url: https://builderspace.aws.dev/project/9123af96-986b-466c-952e-92aeaabdadf6
- id: LIB-024
  question: How do I integrate with an undocumented legacy API that 'just works' in
    production?
  answer: "Treat this as a discovery + documentation + safe integration problem. Never\
    \ assume â€” observe, document, then integrate.\n      \n      **Phase 1: Discovery\
    \ (reverse engineering)**\n      - **Traffic capture**: Use network monitoring\
    \ tools to observe actual API calls in production\n        - Capture request/response\
    \ pairs for all known operations\n        - Note headers, authentication patterns,\
    \ content types\n        - Record timing characteristics (latency, timeouts)\n\
    \      - **Interview SMEs**: Find the people who built it or maintain it\n   \
    \     - \"What breaks it? What are the edge cases?\"\n        - \"What's the expected\
    \ load? What happens under stress?\"\n      - **Code archaeology**: If source\
    \ available, trace API handlers\n      - **AWS Mainframe Modernization + Micro\
    \ Focus**: For mainframe legacy, generates dependency analysis and interactive\
    \ reports\n      \n      **Phase 2: Document what you learn (RIU-017 Connector\
    \ Spec)**\n      Create a contract even if one doesn't exist:\n      ```yaml\n\
    \      endpoint: \"/api/v1/orders\"\n      method: POST\n      auth: Basic Auth\
    \ (header: Authorization)\n      request_schema: (inferred from observations)\n\
    \      response_codes: [200, 400, 500] # observed\n      timeout: 30s # observed\
    \ p99\n      rate_limit: unknown # test carefully\n      known_edge_cases:\n \
    \       - \"Returns 500 for order_id > 10 digits\"\n        - \"Timezone assumed\
    \ UTC despite no documentation\"\n      confidence: low # until validated\n  \
    \    ```\n      \n      **Phase 3: Safe integration patterns**\n      - **API\
    \ Gateway + Lambda proxy**: Create facade that normalizes legacy behavior\n  \
    \      - Add retry logic, circuit breakers, timeout handling\n        - Transform\
    \ formats (SOAPâ†”REST, XMLâ†”JSON) if needed\n        - Log all requests/responses\
    \ for debugging\n      - **Leave-and-layer pattern**: Use EventBridge to add capabilities\
    \ without modifying legacy\n      - **Strangler pattern**: Gradually route traffic\
    \ through new facade\n      \n      **Phase 4: Defensive coding**\n      - **Assume\
    \ nothing**: Validate all responses, even \"successful\" ones\n      - **Timeouts**:\
    \ Set explicit timeouts (legacy systems often hang)\n      - **Circuit breakers**:\
    \ Fail fast when legacy system degrades\n      - **Async processing**: For slow\
    \ legacy APIs, use async Lambda + DynamoDB tracking\n      - **Fallbacks**: Define\
    \ behavior when legacy is unavailable\n      \n      **Phase 5: Validation (before\
    \ production)**\n      - **Shadow traffic**: Mirror production requests to new\
    \ integration, compare results\n      - **Smoke tests**: RIU-081 â€” test critical\
    \ paths in prod-like environment\n      - **Load testing**: Verify legacy can\
    \ handle expected traffic (often the bottleneck)\n      - **Failure injection**:\
    \ Test circuit breakers and fallbacks\n      \n      **PALETTE integration:**\n\
    \      - Document discovered contract in RIU-017 (Connector Spec)\n      - Flag\
    \ integration as ONE-WAY DOOR until validated\n      - Track unknowns in Assumptions\
    \ Register (RIU-008) with validation plan\n      - Add to Risk Register (RIU-009):\
    \ \"Undocumented API behavior may change without notice\"\n      - Create incident\
    \ runbook (RIU-062) for legacy system failures\n      \n      Key insight: \"\
    Just works in production\" means \"works for current use cases under current load.\"\
    \ Your integration may trigger behavior nobody has seen. Observe before you act,\
    \ document everything, and build defensive."
  problem_type: Systems_Integration
  related_rius:
  - RIU-008
  - RIU-009
  - RIU-017
  - RIU-060
  - RIU-061
  - RIU-062
  - RIU-081
  difficulty: critical
  industries:
  - Enterprise IT
  - Finance
  - Healthcare
  - Logistics
  tags:
  - legacy-systems
  - reverse-engineering
  - api-integration
  - documentation
  sources:
  - title: Modernizing SOAP applications using Amazon API Gateway and AWS Lambda
    url: https://aws.amazon.com/blogs/compute/modernizing-soap-applications-using-amazon-api-gateway-and-aws-lambda/
  - title: Seamlessly migrate on-premises legacy workloads using a strangler pattern
    url: https://aws.amazon.com/blogs/architecture/seamlessly-migrate-on-premises-legacy-workloads-using-a-strangler-pattern/
  - title: 'Modernizing Legacy Applications with Event-Driven Architecture: The Leave-and-Layer
      Pattern'
    url: https://aws.amazon.com/blogs/migration-and-modernization/modernizing-legacy-applications-with-event-driven-architecture-the-leave-and-layer-pattern/
  - title: Analyzing legacy applications with AWS Mainframe Modernization and Micro
      Focus
    url: https://aws.amazon.com/blogs/mt/analyzing-legacy-applications-on-demand-with-aws-mainframe-modernization-and-micro-focus/
- id: LIB-025
  question: What's the best strategy for handling API rate limits in real-time AI
    systems?
  answer: "Use a layered approach: prevent hitting limits, handle limits gracefully\
    \ when hit, and degrade gracefully when overwhelmed.\n      \n      **Layer 1:\
    \ Prevention (stay under limits)**\n      - **Token budget management**: Implement\
    \ cost sentry with rate limiter workflow to enforce limits before they're hit\n\
    \      - **Prompt caching**: Use Amazon Bedrock's built-in prompt caching + client-side\
    \ caching for repeated queries\n      - **Request batching**: Combine multiple\
    \ small requests where possible\n      - **Model routing**: Route to smaller/faster\
    \ models for simple queries, reserve large models for complex ones\n      - **Application\
    \ inference profiles**: Track usage per tenant/use-case with Bedrock inference\
    \ profiles\n      \n      **Layer 2: Rate control mechanisms**\n      - **Token\
    \ bucket algorithm**: Track tokens/requests per tenant, enforce fair sharing\n\
    \      - **API Gateway throttling**: Set account-level and per-client limits\n\
    \      - **SQS/Kinesis buffering**: Queue requests during spikes, process at controlled\
    \ rate\n      - **Concurrency limits**: Configure Lambda reserved concurrency\
    \ to cap parallel executions\n      \n      **Layer 3: Graceful handling when\
    \ limits hit**\n      - **Exponential backoff with jitter**: Retry with increasing\
    \ delays + randomization to prevent thundering herd\n      ```python\n      delay\
    \ = min(base_delay * (2 ** attempt) + random_jitter, max_delay)\n      ```\n \
    \     - **Streaming responses**: Break long generations into chunks, evaluate\
    \ incrementally\n      - **Circuit breaker**: After N failures, stop retrying\
    \ for cooldown period\n      \n      **Layer 4: Graceful degradation**\n     \
    \ - **Fallback models**: If primary model rate-limited, route to backup (e.g.,\
    \ Nova Micro when Claude unavailable)\n      - **Cached responses**: Serve cached\
    \ results for common queries during rate limit events\n      - **Reduced functionality**:\
    \ Disable non-critical AI features, maintain core functionality\n      - **Queue\
    \ and notify**: Accept request, queue for later processing, notify user of delay\n\
    \      \n      **Monitoring and alerting (RIU-061)**\n      - CloudWatch dashboards\
    \ tracking: tokens used, requests/second, error rates, latency\n      - Alerts\
    \ at 70%, 85%, 95% of rate limits\n      - Cost alerts using AWS Budgets + Cost\
    \ Explorer\n      - Track by inference profile ARN for per-tenant visibility\n\
    \      \n      **Cost optimization:**\n      - Batch mode for non-real-time workloads\
    \ (significant cost savings)\n      - Small, focused agents vs. monolithic prompts\n\
    \      - Provisioned throughput for predictable high-volume workloads\n      \n\
    \      **PALETTE integration:**\n      - Document rate limits in Constraint Profile\
    \ (RIU-007)\n      - Define fallback behavior in Incident Runbook (RIU-062)\n\
    \      - Track token budgets in RIU-522 (Token Budget Management)\n      - Monitor\
    \ with RIU-061 (Observability Baseline)\n      \n      Key insight: Real-time\
    \ AI systems need proactive rate management, not just reactive handling. Budget\
    \ enforcement *before* limits are hit is cheaper than graceful degradation *after*."
  problem_type: Systems_Integration
  related_rius:
  - RIU-007
  - RIU-061
  - RIU-062
  - RIU-063
  - RIU-520
  - RIU-522
  difficulty: high
  industries:
  - All
  tags:
  - rate-limiting
  - api-design
  - performance
  - reliability
  sources:
  - title: Build a proactive AI cost management system for Amazon Bedrock â€“ Part 1
    url: https://aws.amazon.com/blogs/machine-learning/build-a-proactive-ai-cost-management-system-for-amazon-bedrock-part-1/
  - title: Rate Limiting Strategies for Serverless Applications
    url: https://aws.amazon.com/blogs/architecture/rate-limiting-strategies-for-serverless-applications/
  - title: Track, allocate, and manage your generative AI cost and usage with Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/track-allocate-and-manage-your-generative-ai-cost-and-usage-with-amazon-bedrock/
  - title: Effective cost optimization strategies for Amazon Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/effective-cost-optimization-strategies-for-amazon-bedrock/
  - title: Managing and monitoring API throttling in your workloads
    url: https://aws.amazon.com/blogs/mt/managing-monitoring-api-throttling-in-workloads/
- id: LIB-026
  question: How do I design data contracts between AI services and legacy systems?
  answer: "Data contracts define the agreement between systems on data format, semantics,\
    \ and behavior. For AIâ†”legacy integration, design for compatibility, validation,\
    \ and evolution.\n      \n      **Contract structure (RIU-011 Data Contract Freeze):**\n\
    \      ```yaml\n      contract_id: \"order-ai-enrichment-v2\"\n      version:\
    \ \"2.1.0\"\n      producer: \"legacy-order-system\"\n      consumer: \"ai-enrichment-service\"\
    \n      \n      schema:\n        type: object\n        required: [order_id, customer_id,\
    \ items]\n        properties:\n          order_id: {type: string, pattern: \"\
    ^ORD-[0-9]{10}$\"}\n          customer_id: {type: string}\n          items: {type:\
    \ array, items: {$ref: \"#/definitions/LineItem\"}}\n          # AI-added fields\
    \ (optional for legacy compatibility)\n          ai_risk_score: {type: number,\
    \ minimum: 0, maximum: 1}\n          ai_category: {type: string, enum: [standard,\
    \ priority, review]}\n      \n      compatibility_mode: BACKWARD  # New consumer\
    \ can read old data\n      validation: strict\n      sla:\n        latency_p99:\
    \ 500ms\n        availability: 99.9%\n      ```\n      \n      **Schema management\
    \ with AWS Glue Schema Registry:**\n      - Register schemas for all data exchanges\n\
    \      - Enable compatibility checking (BACKWARD, FORWARD, FULL)\n      - Auto-validate\
    \ on serialization/deserialization\n      - Version tracking with IAM-controlled\
    \ access\n      - Works with MSK, Kinesis, and custom applications\n      \n \
    \     **Compatibility strategies:**\n      | Strategy | When to Use | Rule |\n\
    \      |----------|-------------|------|\n      | BACKWARD | AI adds fields to\
    \ legacy data | New fields must be optional |\n      | FORWARD | Legacy must accept\
    \ AI output | Consumers ignore unknown fields |\n      | FULL | Bidirectional\
    \ compatibility | Both rules apply |\n      | NONE | Breaking changes allowed\
    \ | Coordinate deployment |\n      \n      **Handling AI-specific challenges:**\n\
    \      - **Non-deterministic outputs**: Define acceptable ranges, not exact values\n\
    \      - **Confidence scores**: Include as optional fields with documented semantics\n\
    \      - **Nullable AI fields**: Legacy may not populate fields AI expects â€” handle\
    \ gracefully\n      - **Format mismatches**: Use transformation layer (Lambda,\
    \ Step Functions) between systems\n      \n      **Validation and testing (RIU-080\
    \ Contract Tests):**\n      - **Schema validation**: Validate all messages against\
    \ registered schema\n      - **Contract tests**: Producer tests verify output\
    \ matches contract; consumer tests verify handling\n      - **Sample payloads**:\
    \ Include representative examples in contract definition\n      - **Edge cases**:\
    \ Document and test boundary conditions\n      \n      **Evolution process:**\n\
    \      1. Propose schema change with compatibility analysis\n      2. Register\
    \ new version in Schema Registry\n      3. Update consumer to handle new + old\
    \ versions\n      4. Update producer to emit new version\n      5. Deprecate old\
    \ version after migration period\n      \n      **PALETTE integration:**\n   \
    \   - Document contracts in RIU-011 (Data Contract Freeze)\n      - Test with\
    \ RIU-080 (Contract Tests)\n      - Track schema changes in Decision Log (RIU-003)\
    \ â€” often ONE-WAY DOORs\n      - Define SLAs in RIU-070 (SLO/SLI Definition)\n\
    \      \n      Key insight: Legacy systems can't change quickly â€” design contracts\
    \ with BACKWARD compatibility so AI services can evolve without breaking legacy\
    \ consumers. The contract is the API between teams, not just systems."
  problem_type: Systems_Integration
  related_rius:
  - RIU-003
  - RIU-011
  - RIU-060
  - RIU-061
  - RIU-070
  - RIU-080
  difficulty: high
  industries:
  - Enterprise IT
  - Finance
  - Healthcare
  tags:
  - data-contracts
  - schema-design
  - integration
  - compatibility
  sources:
  - title: Evolve JSON Schemas in Amazon MSK and Amazon Kinesis Data Streams with
      the AWS Glue Schema Registry
    url: https://aws.amazon.com/blogs/big-data/evolve-json-schemas-in-amazon-msk-and-amazon-kinesis-data-streams-with-the-aws-glue-schema-registry/
  - title: Modern data strategy for government tax and labor systems
    url: https://aws.amazon.com/blogs/publicsector/modern-data-strategy-for-government-tax-and-labor-systems/
  - title: Organizational Design and Team Structure for AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_2_organizational_design_team_structure.html
- id: LIB-027
  question: What failure handling patterns work for brittle third-party integrations?
  answer: "Brittle integrations fail unpredictably â€” design for failure with defense\
    \ in depth: prevent, detect, handle, recover.\n      \n      **Layer 1: Prevent\
    \ cascading failures**\n      - **Circuit Breaker**: Stop calling failing service\
    \ temporarily\n        - States: CLOSED (normal) â†’ OPEN (failing, reject calls)\
    \ â†’ HALF-OPEN (test recovery)\n        - Prevents your system from being dragged\
    \ down by third-party failures\n      - **Timeouts**: Set explicit, aggressive\
    \ timeouts (don't wait forever)\n      - **Bulkheads**: Isolate third-party calls\
    \ so one failure doesn't exhaust all resources\n      \n      **Layer 2: Buffer\
    \ and decouple**\n      - **Queue-based load leveling**: SQS between your system\
    \ and third-party\n        - Absorbs spikes, survives temporary outages\n    \
    \    - Lambda processes queue with controlled concurrency\n      - **Async where\
    \ possible**: Don't block user requests on third-party calls\n      \n      **Layer\
    \ 3: Retry safely**\n      - **Exponential backoff with jitter**: `delay = base\
    \ * 2^attempt + random()`\n      - **Idempotency**: Use AWS Lambda Powertools\
    \ idempotency decorator\n        - Stores idempotency keys in DynamoDB\n     \
    \   - Prevents duplicate operations on retry (critical for payments, orders)\n\
    \      - **Retry limits**: Cap retries (e.g., 3-5 attempts), then fail to DLQ\n\
    \      \n      **Layer 4: Fallback strategies**\n      | Pattern | Description\
    \ | Use When |\n      |---------|-------------|----------|\n      | Cross-Region\
    \ Fallback | Same service, different region | Regional outage |\n      | Multi-Model\
    \ Fallback | Different AI model, same provider | Model-specific issues |\n   \
    \   | Multi-Provider Fallback | Different provider entirely | Provider outage\
    \ |\n      | Cached Response | Return stale data | Freshness not critical |\n\
    \      | Degraded Mode | Disable feature, continue core | Non-essential integration\
    \ |\n      | Manual Fallback | Route to human | High-stakes decisions |\n    \
    \  \n      **Layer 5: Capture and recover**\n      - **Dead Letter Queues (DLQ)**:\
    \ Capture failed messages for later processing\n      - **Lambda Destinations**:\
    \ Get detailed failure info (better than DLQ alone)\n      - **Recovery jobs**:\
    \ Scheduled process to retry DLQ messages when service recovers\n      \n    \
    \  **Monitoring and alerting (RIU-061)**\n      - Alert on: error rate spike,\
    \ latency increase, circuit breaker state change\n      - CloudWatch Rules â†’ SNS\
    \ for immediate notification\n      - Dashboard: success rate, p99 latency, DLQ\
    \ depth, circuit breaker status\n      - Test failover in non-production regularly\n\
    \      \n      **PALETTE integration:**\n      - Document failure modes in Risk\
    \ Register (RIU-009)\n      - Define fallback behavior in Incident Runbook (RIU-062)\n\
    \      - Test failure scenarios with RIU-081 (Smoke Tests)\n      - Track third-party\
    \ SLAs in RIU-070 (SLO/SLI Definition)\n      \n      Key insight: Third-party\
    \ integrations are *always* brittle â€” even \"reliable\" services fail. Design\
    \ assuming the integration will fail, and your system will be resilient when it\
    \ inevitably does."
  problem_type: Systems_Integration
  related_rius:
  - RIU-009
  - RIU-061
  - RIU-062
  - RIU-063
  - RIU-070
  - RIU-081
  - RIU-100
  difficulty: high
  industries:
  - All
  tags:
  - failure-handling
  - resilience
  - third-party
  - error-recovery
  sources:
  - title: Amazon Bedrock Reliability Patterns
    url: https://github.com/aws-samples/sample-amazon-bedrock-reliability-patterns
  - title: Queue Integration with Third-party Services on AWS
    url: https://aws.amazon.com/blogs/architecture/queue-integration-with-third-party-services-on-aws/
  - title: Handling Lambda functions idempotency with AWS Lambda Powertools
    url: https://aws.amazon.com/blogs/compute/handling-lambda-functions-idempotency-with-aws-lambda-powertools/
  - title: Implementing AWS Lambda error handling patterns
    url: https://aws.amazon.com/blogs/compute/implementing-aws-lambda-error-handling-patterns/
  - title: Handling Errors, Retries, and adding Alerting to Step Function State Machine
      Executions
    url: https://aws.amazon.com/blogs/developer/handling-errors-retries-and-adding-alerting-to-step-function-state-machine-executions/
- id: LIB-028
  question: How do I version APIs when both AI and legacy systems depend on them?
  answer: "API versioning with mixed consumers (AI + legacy) requires balancing innovation\
    \ speed with stability. Use a combination of versioning strategy, compatibility\
    \ layers, and clear deprecation policies.\n      \n      **Versioning strategies:**\n\
    \      | Strategy | Example | Pros | Cons | Best For |\n      |----------|---------|------|------|----------|\n\
    \      | URL Path | `/v1/orders`, `/v2/orders` | Clear, cacheable | URL proliferation\
    \ | Public APIs |\n      | Header | `X-API-Version: 2` | Clean URLs | Hidden versioning\
    \ | Internal APIs |\n      | Query Param | `/orders?version=2` | Easy to test\
    \ | Cache complications | Debug/testing |\n      \n      **Recommended: Header-based\
    \ with CloudFront + Lambda@Edge**\n      - Route requests to different backends\
    \ based on `X-API-Version` header\n      - Store version configuration in DynamoDB,\
    \ cache in Lambda@Edge\n      - Preserve clean URLs while supporting multiple\
    \ versions\n      \n      **Compatibility layer (API Gateway mapping templates):**\n\
    \      - Transform requests/responses to maintain compatibility\n      - Backend\
    \ can evolve while old consumers see consistent interface\n      - Clone API to\
    \ create v2, use mapping templates to bridge differences\n      - Both versions\
    \ coexist, consumers migrate on their schedule\n      \n      **Version lifecycle\
    \ (adapt Kubernetes model):**\n      1. **Alpha**: Experimental, may change without\
    \ notice, AI services only\n      2. **Beta**: Stable interface, may have bugs,\
    \ early adopters\n      3. **Stable**: Production-ready, backward-compatible changes\
    \ only\n      4. **Deprecated**: Still works, sunset date announced, warnings\
    \ emitted\n      5. **Removed**: No longer available\n      \n      **Deprecation\
    \ policy:**\n      - Announce deprecation minimum 6 months before removal (longer\
    \ for legacy)\n      - Emit deprecation warnings in response headers\n      -\
    \ Provide migration guide and tooling to identify deprecated usage\n      - Monitor\
    \ deprecated endpoint usage â€” don't remove until traffic drops\n      - Document\
    \ in API changelog and notify consumers directly\n      \n      **AI-specific\
    \ considerations:**\n      - AI outputs may be non-deterministic â€” version the\
    \ *contract*, not the exact output\n      - Include model version in response\
    \ metadata for debugging\n      - AI services can consume newer versions faster\
    \ â€” use them as early adopters\n      - Legacy systems need longer deprecation\
    \ windows â€” plan accordingly\n      \n      **Implementation with AWS:**\n   \
    \   - **API Gateway**: Create separate stages or cloned APIs per version\n   \
    \   - **CloudFront + Lambda@Edge**: Route based on headers\n      - **Mapping\
    \ templates**: Transform between versions without backend changes\n      - **Strangler\
    \ pattern**: Facade provides uniform access during migration\n      \n      **PALETTE\
    \ integration:**\n      - Document API versions in RIU-016 (API Contract Review\
    \ + Versioning Plan)\n      - Track breaking changes as ONE-WAY DOORs in Decision\
    \ Log (RIU-003)\n      - Define deprecation timeline in Constraint Profile (RIU-007)\n\
    \      - Test all supported versions with RIU-080 (Contract Tests)\n      \n \
    \     Key insight: Legacy systems can't upgrade quickly â€” maintain N-1 (or N-2)\
    \ version support. AI systems can move faster â€” use them to validate new versions\
    \ before legacy consumers migrate."
  problem_type: Systems_Integration
  related_rius:
  - RIU-003
  - RIU-007
  - RIU-016
  - RIU-061
  - RIU-062
  - RIU-080
  - RIU-532
  difficulty: medium
  industries:
  - Enterprise IT
  - SaaS
  - Finance
  tags:
  - api-versioning
  - backward-compatibility
  - change-management
  - governance
  sources:
  - title: Implementing header-based API Gateway versioning with Amazon CloudFront
    url: https://aws.amazon.com/blogs/compute/implementing-header-based-api-gateway-versioning-with-amazon-cloudfront/
  - title: Using API Gateway mapping templates to handle changes in your back-end
      APIs
    url: https://aws.amazon.com/blogs/compute/using-api-gateway-mapping-templates-to-handle-changes-in-your-back-end-apis/
  - title: Modernizing SOAP applications using Amazon API Gateway and AWS Lambda
    url: https://aws.amazon.com/blogs/compute/modernizing-soap-applications-using-amazon-api-gateway-and-aws-lambda/
  - title: Preparing for Kubernetes API deprecations
    url: https://aws.amazon.com/blogs/containers/preparing-for-kubernetes-api-deprecations-when-going-from-1-15-to-1-16/
- id: LIB-029
  question: What's the minimum viable integration test suite for AI-to-legacy connections?
  answer: "A minimum viable integration test suite validates that AI and legacy systems\
    \ communicate correctly without requiring exhaustive coverage. Focus on critical\
    \ paths, failure modes, and contract compliance.\n      \n      **Minimum test\
    \ categories (must have all 5):**\n      \n      **1. Contract Tests (RIU-080)**\n\
    \      - Validate AI service outputs match expected schema\n      - Validate legacy\
    \ system accepts AI-formatted requests\n      - Version-controlled contracts define\
    \ expected requests/responses\n      - Run on every code change (shift-left)\n\
    \      ```\n      Test: AI enrichment output matches contract v2.1\n      Input:\
    \ Sample order payload\n      Assert: Response validates against JSON schema\n\
    \      Assert: Required fields present (order_id, ai_risk_score)\n      ```\n\
    \      \n      **2. Connectivity Smoke Tests (RIU-081)**\n      - AI service can\
    \ reach legacy endpoint\n      - Authentication succeeds (tokens, API keys, certificates)\n\
    \      - Basic request/response round-trip works\n      - Run pre/post deployment\n\
    \      ```\n      Test: Legacy order API reachable\n      Assert: GET /health\
    \ returns 200 within 5s\n      Assert: POST /orders with valid payload returns\
    \ 201\n      ```\n      \n      **3. Data Flow Tests**\n      - End-to-end data\
    \ pipeline: ingestion â†’ processing â†’ AI â†’ legacy\n      - For RAG: document ingestion\
    \ â†’ embedding â†’ vector store â†’ retrieval\n      - Verify data transformations\
    \ preserve required fields\n      ```\n      Test: Order flows from legacy to\
    \ AI enrichment to legacy update\n      Assert: Original order_id preserved\n\
    \      Assert: AI-added fields present in final record\n      ```\n      \n  \
    \    **4. Error Handling Tests**\n      - Legacy timeout â†’ AI handles gracefully\n\
    \      - Legacy returns error â†’ AI logs and falls back\n      - Invalid data from\
    \ legacy â†’ AI rejects with clear message\n      - Use service virtualization to\
    \ simulate failure scenarios\n      ```\n      Test: Legacy API returns 500\n\
    \      Assert: AI service returns degraded response (not 500)\n      Assert: Error\
    \ logged with correlation ID\n      Assert: Retry attempted with backoff\n   \
    \   ```\n      \n      **5. Auth/Authz Tests**\n      - Service-to-service authentication\
    \ works\n      - Role-based access controls enforced\n      - Token refresh/rotation\
    \ handled\n      ```\n      Test: AI service authenticates to legacy with service\
    \ account\n      Assert: Valid token accepted\n      Assert: Expired token triggers\
    \ refresh\n      Assert: Invalid token rejected with 401\n      ```\n      \n\
    \      **Optional but recommended:**\n      - **Load/Performance**: OLAF or similar\
    \ for SageMaker endpoints â€” verify latency under expected load\n      - **UI Journey**:\
    \ Amazon Nova Act headless mode for end-to-end user flows\n      - **Model Accuracy**:\
    \ Smoke test new model versions before production\n      \n      **Test data strategy:**\n\
    \      - Create fixtures representing common cases + known edge cases\n      -\
    \ Use service virtualization (mocks) for legacy system simulation\n      - Sanitize\
    \ production data for realistic test scenarios\n      - Store fixtures alongside\
    \ tests in version control\n      \n      **CI/CD integration:**\n      - Contract\
    \ tests: Every commit (fast, <5 min)\n      - Smoke tests: Every deployment (medium,\
    \ <15 min)\n      - Full integration: Nightly or pre-release (longer, <1 hour)\n\
    \      \n      **\"Minimum viable\" criteria:**\n      - [ ] All 5 test categories\
    \ have at least 1 test each\n      - [ ] Tests run automatically in CI/CD\n  \
    \    - [ ] Critical path (happy path) covered\n      - [ ] At least 1 failure\
    \ scenario per integration point\n      - [ ] Tests pass in staging before production\
    \ deployment\n      \n      **PALETTE integration:**\n      - Define test suite\
    \ in RIU-081 (E2E Smoke Tests)\n      - Contract tests per RIU-080\n      - Document\
    \ test coverage gaps in Assumptions Register (RIU-008)\n      - Include test execution\
    \ in Deployment Readiness (RIU-060)"
  problem_type: Systems_Integration
  related_rius:
  - RIU-060
  - RIU-062
  - RIU-063
  - RIU-080
  - RIU-081
  - RIU-540
  difficulty: medium
  industries:
  - All
  tags:
  - testing
  - integration-testing
  - quality-assurance
  - validation
  sources:
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Testing approaches for Amazon SageMaker ML models
    url: https://aws.amazon.com/blogs/machine-learning/testing-approaches-for-amazon-sagemaker-ml-models/
  - title: Implement automated smoke testing using Amazon Nova Act headless mode
    url: https://aws.amazon.com/blogs/machine-learning/implement-automated-smoke-testing-using-amazon-nova-act-headless-mode/
  - title: 'Speed meets scale: Load testing SageMaker AI endpoints with OLAF'
    url: https://aws.amazon.com/blogs/machine-learning/speed-meets-scale-load-testing-sagemakerai-endpoints-with-observe-ais-testing-tool/
- id: LIB-030
  question: How do I handle schema mismatches between AI output and legacy system
    input?
  answer: "Schema mismatches are inevitable when connecting AI (flexible, evolving)\
    \ to legacy (rigid, stable). Use a transformation layer with validation at boundaries.\n\
    \      \n      **Architecture pattern: Transformation Layer**\n      ```\n   \
    \   AI Service â†’ [Transformation Layer] â†’ Legacy System\n                    \
    \     â†“\n                   - Schema mapping\n                   - Field conversion\n\
    \                   - Validation\n                   - Default values\n      \
    \             - Error handling\n      ```\n      \n      **Mismatch types and\
    \ solutions:**\n      \n      | Mismatch Type | Example | Solution |\n      |---------------|---------|----------|\n\
    \      | Field naming | `aiRiskScore` vs `RISK_SCORE` | Field mapping in transformation\
    \ |\n      | Data types | String \"123\" vs Integer 123 | Type coercion with validation\
    \ |\n      | Missing fields | AI omits optional field | Default values or null\
    \ handling |\n      | Extra fields | AI adds fields legacy ignores | Filter to\
    \ expected schema |\n      | Format differences | JSON vs XML | API Gateway +\
    \ Lambda transformation |\n      | Nested vs flat | `{address: {city}}` vs `address_city`\
    \ | Flatten/unflatten logic |\n      | Enum mismatches | \"HIGH\" vs \"3\" | Lookup\
    \ table conversion |\n      \n      **Control AI output schema (prevent mismatches\
    \ at source):**\n      - **Amazon Nova constrained decoding**: Define output schema\
    \ in tool configuration, use Converse API\n      - **Prompt templates as contracts**:\
    \ Enforce output format in system prompt\n      - **JSON mode**: Request structured\
    \ JSON output matching legacy schema\n      ```python\n      # Nova constrained\
    \ output example\n      tool_config = {\n          \"output_schema\": {\n    \
    \          \"type\": \"object\",\n              \"properties\": {\n          \
    \        \"RISK_SCORE\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\":\
    \ 100},\n                  \"CATEGORY_CODE\": {\"type\": \"string\", \"enum\"\
    : [\"A\", \"B\", \"C\"]}\n              },\n              \"required\": [\"RISK_SCORE\"\
    , \"CATEGORY_CODE\"]\n          }\n      }\n      ```\n      \n      **Transformation\
    \ implementation (AWS):**\n      - **Simple transformations**: API Gateway mapping\
    \ templates (no code)\n      - **Complex transformations**: Lambda function between\
    \ AI and legacy\n      - **Streaming data**: Glue Schema Registry for validation\
    \ + SerDe for conversion\n      - **Batch data**: Glue ETL jobs with schema mapping\n\
    \      \n      **Validation at boundaries (AWS Glue Data Quality):**\n      -\
    \ Schema matching: Output conforms to expected structure\n      - Referential\
    \ integrity: Foreign keys exist in legacy system\n      - Data type validation:\
    \ Values within expected ranges\n      - Completeness: Required fields present\n\
    \      ```\n      Rules: [\n        SchemaMatch \"ai_output\" \"legacy_input_schema\"\
    ,\n        ColumnValues \"RISK_SCORE\" between 0 and 100,\n        IsComplete\
    \ \"ORDER_ID\",\n        ReferentialIntegrity \"CUSTOMER_ID\" \"legacy.customers.id\"\
    \n      ]\n      ```\n      \n      **Error handling for mismatches:**\n     \
    \ 1. **Validation failure**: Log error with details, route to DLQ\n      2. **Transformation\
    \ failure**: Return clear error message, don't fail silently\n      3. **Partial\
    \ success**: Decide policy â€” reject entire record or accept partial?\n      4.\
    \ **Unknown fields**: Log warning, strip and continue (don't break on extras)\n\
    \      \n      **Testing schema compatibility (RIU-080):**\n      - Contract tests\
    \ validate transformation outputs\n      - Test with edge cases: nulls, empty\
    \ strings, boundary values\n      - Test with malformed AI outputs (defensive)\n\
    \      - Regression tests when either schema changes\n      \n      **PALETTE\
    \ integration:**\n      - Document schema mappings in RIU-011 (Data Contract Freeze)\n\
    \      - Validate with RIU-084 (Data Quality Checks)\n      - Test transformations\
    \ with RIU-080 (Contract Tests)\n      - Track schema changes as potential ONE-WAY\
    \ DOORs\n      \n      Key insight: Don't trust AI output blindly â€” validate at\
    \ the boundary before sending to legacy. Constrained decoding prevents most mismatches;\
    \ transformation layer handles the rest."
  problem_type: Systems_Integration
  related_rius:
  - RIU-011
  - RIU-060
  - RIU-061
  - RIU-080
  - RIU-084
  difficulty: high
  industries:
  - Enterprise IT
  - Finance
  - Healthcare
  tags:
  - schema-mapping
  - data-transformation
  - compatibility
  - integration
  sources:
  - title: 'Structured outputs with Amazon Nova: A guide for builders'
    url: https://aws.amazon.com/blogs/machine-learning/structured-outputs-with-amazon-nova-a-guide-for-builders/
  - title: Validate, evolve, and control schemas with AWS Glue Schema Registry
    url: https://aws.amazon.com/blogs/big-data/validate-evolve-and-control-schemas-in-amazon-msk-and-amazon-kinesis-data-streams-with-aws-glue-schema-registry/
  - title: Set up advanced rules to validate quality with AWS Glue Data Quality
    url: https://aws.amazon.com/blogs/big-data/set-up-advanced-rules-to-validate-quality-of-multiple-datasets-with-aws-glue-data-quality/
  - title: Modernizing SOAP applications using Amazon API Gateway and AWS Lambda
    url: https://aws.amazon.com/blogs/compute/modernizing-soap-applications-using-amazon-api-gateway-and-aws-lambda/
- id: LIB-031
  question: What monitoring tells me an integration is degrading before it fails completely?
  answer: "Degradation signals appear before failures â€” monitor leading indicators,\
    \ not just errors. Focus on trends and percentiles, not just averages.\n     \
    \ \n      **Leading indicators (early warning signals):**\n      \n      | Indicator\
    \ | What It Signals | Alert Threshold |\n      |-----------|-----------------|-----------------|\n\
    \      | Latency p99 increasing | Slowdown before timeout | >2x baseline |\n \
    \     | Error rate trending up | Intermittent failures beginning | >1% or 2x baseline\
    \ |\n      | Queue depth growing | Processing falling behind | >2x normal depth\
    \ |\n      | Retry rate increasing | Transient failures increasing | >5% of requests\
    \ |\n      | Rate limit % consumed | Approaching throttling | >70%, >85%, >95%\
    \ |\n      | Connection pool exhaustion | Resource contention | >80% utilization\
    \ |\n      | Network RTT increasing | Infrastructure degradation | >1.5x baseline\
    \ |\n      | Token/quota consumption | AI cost/rate limits approaching | >70%\
    \ of budget |\n      \n      **SLI/SLO framework (RIU-070):**\n      Define Service\
    \ Level Indicators and Objectives:\n      ```yaml\n      integration_name: \"\
    legacy-order-api\"\n      slis:\n        - name: availability\n          query:\
    \ \"successful_requests / total_requests\"\n          target: 99.9%\n        -\
    \ name: latency_p99\n          query: \"percentile(latency, 99)\"\n          target:\
    \ 500ms\n        - name: error_rate\n          query: \"error_count / total_requests\"\
    \n          target: <0.1%\n      slo_window: 30d\n      burn_rate_alert: 10x \
    \ # Alert if burning error budget 10x faster than sustainable\n      ```\n   \
    \   Use CloudWatch Application Signals for SLI/SLO tracking.\n      \n      **Infrastructure-level\
    \ monitoring:**\n      - **Network Flow Monitor**: Visualize network performance\
    \ across AWS workloads, detect RTT degradation\n      - **EBS latency monitoring**:\
    \ Track storage I/O latency â€” often hidden cause of integration slowdowns\n  \
    \    - **Lambda debugging**: Monitor for unintended function versions, infinite\
    \ loops, downstream availability\n      - **SQS throttling/backpressure**: Queue\
    \ metrics indicate processing falling behind\n      \n      **Distributed tracing\
    \ (find degradation source):**\n      - **OpenTelemetry**: Auto-instrument with\
    \ Java Agent (no code changes)\n      - **AWS X-Ray + ServiceLens**: Connect metrics,\
    \ logs, and traces\n      - **Trace slow requests**: Identify which integration\
    \ hop is degrading\n      - **Correlation IDs**: Track requests across AI â†’ legacy\
    \ boundaries\n      \n      **CloudWatch monitoring setup:**\n      ```\n    \
    \  # Anomaly detection for latency\n      ANOMALY_DETECTION_BAND(latency_p99,\
    \ 2)\n      \n      # Metrics Insights for dynamic monitoring\n      SELECT AVG(latency),\
    \ COUNT(*) as requests, \n             SUM(CASE WHEN status >= 500 THEN 1 ELSE\
    \ 0 END) as errors\n      FROM integration_metrics\n      GROUP BY integration_name\n\
    \      ```\n      \n      **Alert tiering (escalation):**\n      | Severity |\
    \ Trigger | Action |\n      |----------|---------|--------|\n      | Info | p99\
    \ >1.5x baseline | Log, dashboard highlight |\n      | Warning | p99 >2x OR error\
    \ rate >1% | Page on-call, investigate |\n      | Critical | p99 >3x OR error\
    \ rate >5% OR SLO breach | Immediate response, consider failover |\n      \n \
    \     **AI-specific degradation signals:**\n      - Token consumption rate increasing\
    \ (prompts getting longer/retries)\n      - Model latency increasing (provider\
    \ degradation)\n      - Confidence scores dropping (model quality issues)\n  \
    \    - Guardrail block rate spiking (input quality degrading)\n      - Cost per\
    \ request increasing unexpectedly\n      \n      **Dashboard essentials (RIU-061):**\n\
    \      - Real-time: Request rate, error rate, p50/p95/p99 latency\n      - Trends:\
    \ Hour-over-hour, day-over-day comparisons\n      - Capacity: Queue depth, connection\
    \ pools, rate limit %\n      - Infrastructure: Network RTT, EBS latency, Lambda\
    \ concurrency\n      - Dependencies: Upstream/downstream health status\n     \
    \ - Cost: Token usage, API call costs (for AI integrations)\n      \n      **PALETTE\
    \ integration:**\n      - Define SLIs/SLOs in RIU-070 (SLO/SLI Definition)\n \
    \     - Configure alerts in RIU-061 (Observability Baseline)\n      - Document\
    \ escalation in RIU-062 (Incident Runbook)\n      - Track degradation patterns\
    \ in RIU-063 (Performance Baselines)\n      \n      Key insight: By the time you\
    \ see errors, users already experienced failures. Monitor *latency percentiles*\
    \ and *trends* â€” they degrade before errors spike. Set alerts at 70% of your failure\
    \ threshold, not 100%."
  problem_type: Systems_Integration
  related_rius:
  - RIU-061
  - RIU-062
  - RIU-063
  - RIU-070
  - RIU-532
  - RIU-533
  difficulty: high
  industries:
  - All
  tags:
  - monitoring
  - observability
  - early-warning
  - reliability
  sources:
  - title: How to monitor application health using SLOs with Amazon CloudWatch Application
      Signals
    url: https://aws.amazon.com/blogs/mt/how-to-monitor-application-health-using-slos-with-amazon-cloudwatch-application-signals/
  - title: Distributed tracing with OpenTelemetry
    url: https://aws.amazon.com/blogs/opensource/distributed-tracing-with-opentelemetry/
  - title: Visualizing network performance with Network Flow Monitor
    url: https://aws.amazon.com/blogs/networking-and-content-delivery/visualizing-network-performance-of-your-aws-cloud-workloads-with-network-flow-monitor/
  - title: 'Operating Lambda: Debugging configurations â€“ Part 3'
    url: https://aws.amazon.com/blogs/compute/operating-lambda-debugging-integrations-part-3/
  - title: Understanding and monitoring latency for Amazon EBS volumes
    url: https://aws.amazon.com/blogs/storage/understanding-and-monitoring-latency-for-amazon-ebs-volumes-using-amazon-cloudwatch/
  - title: Integrate Amazon CloudWatch alarms with Amazon CloudWatch Metrics Insights
    url: https://aws.amazon.com/blogs/mt/integrate-amazon-cloudwatch-alarms-with-amazon-cloudwatch-metrics-insights/
- id: LIB-032
  question: How do I document integration contracts so they're enforceable and testable?
  answer: "Enforceable contracts are machine-readable, version-controlled, and automatically\
    \ tested. Document the contract, not just the API.\n      \n      **Contract documentation\
    \ structure (RIU-015):**\n      ```yaml\n      contract_id: \"order-enrichment-v2\"\
    \n      version: \"2.1.0\"\n      owner: \"ai-platform-team\"\n      consumers:\
    \ [\"legacy-orders\", \"reporting-service\"]\n      last_updated: \"2024-06-15\"\
    \n      \n      # What this integration does\n      description: \"AI enrichment\
    \ of order data with risk scoring\"\n      \n      # Request specification\n \
    \     request:\n        schema: \"$ref: ./schemas/order-input.json\"\n       \
    \ required_fields: [order_id, customer_id, items]\n        content_type: \"application/json\"\
    \n        \n      # Response specification  \n      response:\n        schema:\
    \ \"$ref: ./schemas/enriched-order.json\"\n        success_codes: [200, 202]\n\
    \        error_codes: [400, 422, 500]\n        \n      # Non-functional requirements\n\
    \      sla:\n        latency_p99: 500ms\n        availability: 99.9%\n       \
    \ rate_limit: 1000/min\n        \n      # Behavioral contract\n      invariants:\n\
    \        - \"order_id in response == order_id in request\"\n        - \"ai_risk_score\
    \ is always between 0 and 1\"\n        - \"response time < 30s or timeout with\
    \ 504\"\n      ```\n      \n      **Standard formats by integration type:**\n\
    \      | Integration Type | Format | Tool |\n      |------------------|--------|------|\n\
    \      | REST APIs | OpenAPI 3.x | Swagger, Stoplight |\n      | Async/Events\
    \ | AsyncAPI | AsyncAPI Studio |\n      | Data schemas | JSON Schema | Glue Schema\
    \ Registry |\n      | GraphQL | GraphQL SDL | Apollo |\n      | gRPC | Protocol\
    \ Buffers | protoc |\n      \n      **Making contracts enforceable:**\n      \n\
    \      1. **Schema validation at runtime**\n         - API Gateway request/response\
    \ validation\n         - Glue Schema Registry for streaming data\n         - Lambda\
    \ middleware for custom validation\n      \n      2. **Contract tests in CI/CD\
    \ (RIU-080)**\n         - **Pact** (industry standard): Consumer-driven contract\
    \ testing\n         - Producer tests: \"My output matches the contract\"\n   \
    \      - Consumer tests: \"I can handle expected outputs\"\n         - Run on\
    \ every commit (fast, <5 min)\n      \n      3. **Breaking change detection**\n\
    \         - Schema Registry compatibility modes (BACKWARD, FORWARD, FULL)\n  \
    \       - PR checks that compare contract versions\n         - Block deployment\
    \ if compatibility broken\n      \n      **Consumer-driven contract testing (Pact\
    \ pattern):**\n      ```\n      Consumer defines expected interactions\n     \
    \       â†“\n      Contract published to Pact Broker\n            â†“\n      Provider\
    \ verifies it can fulfill contract\n            â†“\n      Both sides deploy independently\
    \ with confidence\n      ```\n      \n      **Testing pyramid for contracts:**\n\
    \      - **Unit tests**: Schema validation (fastest)\n      - **Contract tests**:\
    \ Pact consumer/provider tests\n      - **Integration tests**: Service virtualization\
    \ with mocks\n      - **E2E tests**: Full integration in staging (slowest)\n \
    \     \n      **Version control and governance:**\n      - Store contracts in\
    \ Git alongside code\n      - Require PR review for contract changes\n      -\
    \ Tag contracts with semantic versioning\n      - Maintain changelog of breaking\
    \ vs. non-breaking changes\n      - Flag breaking changes as ONE-WAY DOORs (RIU-003)\n\
    \      \n      **AI-specific contract considerations:**\n      - Document expected\
    \ output variability (non-deterministic)\n      - Include confidence score ranges\
    \ in contract\n      - Specify model version in response metadata\n      - Define\
    \ acceptable hallucination handling\n      \n      **PALETTE integration:**\n\
    \      - Document contracts in RIU-015 (Contract for Outputs)\n      - Test with\
    \ RIU-080 (Contract Tests)\n      - Version with RIU-016 (API Contract Review\
    \ + Versioning Plan)\n      - Validate data with RIU-084 (Data Quality Checks)\n\
    \      \n      Key insight: A contract that isn't tested automatically isn't enforceable.\
    \ If it's not in CI/CD, it's just documentation â€” and documentation drifts."
  problem_type: Systems_Integration
  related_rius:
  - RIU-003
  - RIU-004
  - RIU-015
  - RIU-016
  - RIU-060
  - RIU-061
  - RIU-080
  - RIU-084
  difficulty: medium
  industries:
  - All
  tags:
  - documentation
  - contracts
  - testing
  - governance
  sources:
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Validate, evolve, and control schemas with AWS Glue Schema Registry
    url: https://aws.amazon.com/blogs/big-data/validate-evolve-and-control-schemas-in-amazon-msk-and-amazon-kinesis-data-streams-with-aws-glue-schema-registry/
  - title: Automated Contract Compliance Analysis
    url: https://awslabs.github.io/generative-ai-atlas/topics/6_0_example_application_and_reference_code/6_1_reference_applications_by_industry/6_1_5_cross_industry/automated_contract_analysis.html
  - title: Building a serverless distributed application using a saga orchestration
      pattern
    url: https://aws.amazon.com/blogs/compute/building-a-serverless-distributed-application-using-a-saga-orchestration-pattern/
  - title: 'Application integration patterns for microservices: Orchestration and
      coordination'
    url: https://aws.amazon.com/blogs/compute/application-integration-patterns-for-microservices-orchestration-and-coordination/
  - title: 'Understanding idempotency: The art of doing a task once and only once'
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/94007ed4-af54-4bdc-bf93-00b320d03925
  - title: Handle unpredictable processing times with operational consistency using
      Step Functions
    url: https://aws.amazon.com/blogs/compute/handle-unpredictable-processing-times-with-operational-consistency-when-integrating-asynchronous-aws-services-with-an-aws-step-functions-state-machine/
  - title: A multi-dimensional approach helps you proactively prepare for failures
    url: https://aws.amazon.com/blogs/architecture/a-multi-dimensional-approach-helps-you-proactively-prepare-for-failures-part-1-application-layer/
  - title: Detecting data drift using Amazon SageMaker
    url: https://aws.amazon.com/blogs/architecture/detecting-data-drift-using-amazon-sagemaker/
  - title: Detect NLP data drift using custom Amazon SageMaker Model Monitor
    url: https://aws.amazon.com/blogs/machine-learning/detect-nlp-data-drift-using-custom-amazon-sagemaker-model-monitor/
  - title: Automate model retraining with Amazon SageMaker Pipelines when drift is
      detected
    url: https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/
  - title: Bring your own container to project model accuracy drift with Amazon SageMaker
      Model Monitor
    url: https://aws.amazon.com/blogs/machine-learning/bring-your-own-container-to-project-model-accuracy-drift-with-amazon-sagemaker-model-monitor/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
- id: LIB-035
  question: What's the minimum viable data dictionary for an AI deployment?
  answer: "A minimum viable data dictionary documents what data exists, what it means,\
    \ and how it can be used. For AI, also include quality metrics and lineage.\n\
    \      \n      **Minimum data dictionary structure:**\n      \n      ```yaml\n\
    \      data_dictionary:\n        dataset_name: \"customer_orders\"\n        version:\
    \ \"1.2.0\"\n        owner: \"data-platform-team\"\n        last_updated: \"2024-06-15\"\
    \n        \n        # Dataset-level metadata\n        description: \"Customer\
    \ order records used for AI risk scoring\"\n        source_system: \"legacy-orders-db\"\
    \n        refresh_frequency: \"hourly\"\n        retention_period: \"7 years\"\
    \n        classification: \"confidential\"  # public | internal | confidential\
    \ | restricted\n        \n        # Field definitions (minimum viable = every\
    \ field used by AI)\n        fields:\n          - name: \"order_id\"\n       \
    \     type: \"string\"\n            description: \"Unique order identifier\"\n\
    \            format: \"ORD-[0-9]{10}\"\n            nullable: false\n        \
    \    pii: false\n            example: \"ORD-1234567890\"\n            \n     \
    \     - name: \"customer_id\"\n            type: \"string\"\n            description:\
    \ \"Customer account identifier\"\n            nullable: false\n            pii:\
    \ true  # Quasi-identifier\n            pii_handling: \"hash before AI processing\"\
    \n            \n          - name: \"order_total\"\n            type: \"decimal\"\
    \n            description: \"Total order value in USD\"\n            unit: \"\
    USD\"\n            range: [0, 1000000]\n            nullable: false\n        \
    \    \n          - name: \"order_date\"\n            type: \"timestamp\"\n   \
    \         description: \"When order was placed\"\n            timezone: \"UTC\"\
    \n            format: \"ISO 8601\"\n            \n        # Data quality baseline\n\
    \        quality_metrics:\n          completeness: 99.5%  # % non-null for required\
    \ fields\n          freshness: \"< 1 hour from source\"\n          volume_baseline:\
    \ \"50,000-100,000 records/day\"\n          \n        # Lineage\n        lineage:\n\
    \          upstream: [\"legacy-orders-db.orders\", \"crm.customers\"]\n      \
    \    downstream: [\"ai-risk-model\", \"reporting-dashboard\"]\n          transformations:\
    \ [\"PII hashing\", \"currency normalization\"]\n      ```\n      \n      **Required\
    \ fields for each data element:**\n      | Field | Required | Why |\n      |-------|----------|-----|\n\
    \      | name | âœ… | Identification |\n      | type | âœ… | Schema validation |\n\
    \      | description | âœ… | Business meaning |\n      | nullable | âœ… | Quality\
    \ checks |\n      | pii | âœ… | Compliance |\n      | example | Recommended | Understanding\
    \ |\n      | range/enum | Recommended | Validation |\n      \n      **AI-specific\
    \ additions (beyond traditional data dictionary):**\n      - **pii_handling**:\
    \ How PII is protected before AI processing\n      - **embedding_model**: If field\
    \ is embedded, which model\n      - **drift_sensitivity**: How sensitive AI is\
    \ to changes in this field\n      - **feature_importance**: Relative importance\
    \ to model (if known)\n      - **quality_thresholds**: Alert if quality drops\
    \ below threshold\n      \n      **For GenAI/RAG systems, also document:**\n \
    \     - **chunk_strategy**: How documents are chunked\n      - **embedding_dimensions**:\
    \ Vector size\n      - **metadata_extracted**: What metadata accompanies embeddings\n\
    \      - **update_frequency**: How often knowledge base refreshes\n      \n  \
    \    **Storage and governance:**\n      - Store in AWS Glue Data Catalog for discoverability\n\
    \      - Use Glue Schema Registry for schema validation\n      - Version control\
    \ dictionary alongside code (Git)\n      - Link to data contracts (RIU-011)\n\
    \      \n      **\"Minimum viable\" criteria:**\n      - [ ] Every field consumed\
    \ by AI model is documented\n      - [ ] PII fields identified with handling instructions\n\
    \      - [ ] Data types and formats specified\n      - [ ] Source system and refresh\
    \ frequency documented\n      - [ ] Quality baseline established (completeness,\
    \ freshness)\n      - [ ] Owner and contact identified\n      \n      **PALETTE\
    \ integration:**\n      - Document in RIU-011 (Data Contract Freeze)\n      -\
    \ Validate with RIU-084 (Data Quality Checks)\n      - Track lineage for RIU-012\
    \ (PII/Sensitive Data Map)\n      - Reference in RIU-004 (Workstream Decomposition)\n\
    \      \n      Key insight: \"Minimum viable\" means every field the AI touches\
    \ has a definition. Unknown fields are technical debt â€” you can't debug data issues\
    \ if you don't know what the data means."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-004
  - RIU-011
  - RIU-012
  - RIU-080
  - RIU-081
  - RIU-084
  difficulty: medium
  industries:
  - All
  tags:
  - data-dictionary
  - documentation
  - semantics
  - data-governance
  sources:
  - title: 'Implementing data governance on AWS: Automation, tagging, and lifecycle
      strategy'
    url: https://aws.amazon.com/blogs/security/implementing-data-governance-on-aws-automation-tagging-and-lifecycle-strategy-part-1/
  - title: Data governance in the age of generative AI
    url: https://aws.amazon.com/blogs/big-data/data-governance-in-the-age-of-generative-ai/
  - title: Validate, evolve, and control schemas with AWS Glue Schema Registry
    url: https://aws.amazon.com/blogs/big-data/validate-evolve-and-control-schemas-in-amazon-msk-and-amazon-kinesis-data-streams-with-aws-glue-schema-registry/
  - title: AI/ML Organizational Adoption Framework
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/index.html
- id: LIB-036
  question: How do I validate that training data labels mean what stakeholders think
    they mean?
  answer: "Label validation has two dimensions: syntactic (is the label correctly\
    \ applied?) and semantic (does the label mean what we think it means?). Most tools\
    \ focus on syntactic â€” you must explicitly validate semantic alignment with stakeholders.\n\
    \      \n      **Semantic validation process:**\n      \n      **Step 1: Label\
    \ definition workshop (before labeling starts)**\n      - Gather stakeholders\
    \ (business, SMEs, data scientists, labelers)\n      - For each label/category,\
    \ document:\n        - **Definition**: What does this label mean in business terms?\n\
    \        - **Inclusion criteria**: What must be true for this label?\n       \
    \ - **Exclusion criteria**: What disqualifies this label?\n        - **Boundary\
    \ examples**: Edge cases that are barely in/out\n        - **Common confusions**:\
    \ Labels this might be mistaken for\n      ```yaml\n      label: \"high_risk_order\"\
    \n      definition: \"Order with elevated fraud probability requiring manual review\"\
    \n      inclusion:\n        - \"Order value > $5000 AND new customer\"\n     \
    \   - \"Shipping address differs from billing by > 500 miles\"\n      exclusion:\n\
    \        - \"Existing customer with 3+ successful orders\"\n      boundary_examples:\n\
    \        - \"Repeat customer, high value, different address\" â†’ NOT high_risk\n\
    \        - \"New customer, $4900, same address\" â†’ NOT high_risk (borderline)\n\
    \      confused_with: \"flagged_order\" (different - that's fraud detected, not\
    \ suspected)\n      ```\n      \n      **Step 2: Pilot labeling with SME calibration**\n\
    \      - Label 50-100 samples with both SMEs and labelers\n      - Compare labels:\
    \ Where do they disagree?\n      - Disagreements reveal semantic ambiguity in\
    \ definitions\n      - Refine guidelines based on confusion patterns\n      \n\
    \      **Step 3: Inter-annotator agreement measurement**\n      - Use modified\
    \ Dawid-Skene (MDS) model for consolidation (20% fewer errors than majority voting)\n\
    \      - Metrics to track:\n        - **Cohen's Kappa**: Agreement accounting\
    \ for chance (target: >0.8)\n        - **Krippendorff's Alpha**: Multi-annotator\
    \ agreement (target: >0.8)\n        - **Confusion matrix by labeler**: Which labels\
    \ are confused most?\n      - Low agreement = definition problem, not labeler\
    \ problem\n      \n      **Step 4: Stakeholder validation checkpoint**\n     \
    \ - Present labeled samples to business stakeholders\n      - Ask: \"Is this what\
    \ you meant by [label]?\"\n      - Show edge cases and boundary decisions\n  \
    \    - Get explicit sign-off before full labeling proceeds\n      - Document as\
    \ ONE-WAY DOOR decision (RIU-003)\n      \n      **Step 5: Continuous validation\
    \ during labeling**\n      - Sample 5-10% for quality audit\n      - Use SageMaker\
    \ Ground Truth Review UI for inspection\n      - Track consensus checks: same\
    \ samples to multiple labelers\n      - Implement feedback loops: labelers can\
    \ flag ambiguous cases\n      \n      **AWS tools for label validation:**\n  \
    \    - **SageMaker Ground Truth**: Verification and adjustment workflows\n   \
    \   - **Ground Truth Plus**: Review UI with filtering and feedback\n      - **Label\
    \ chaining**: Verify labels from previous jobs\n      - **Consolidation algorithms**:\
    \ MDS for intelligent aggregation\n      \n      **Red flags that labels don't\
    \ mean what stakeholders think:**\n      - Low inter-annotator agreement (<0.7\
    \ kappa)\n      - High SME override rate on reviewed samples\n      - Model predictions\
    \ stakeholders call \"wrong\" despite correct labels\n      - Different departments\
    \ using same label differently\n      - Labelers frequently asking clarifying\
    \ questions\n      \n      **For GenAI/LLM evaluation:**\n      - Same principles\
    \ apply to preference labels (chosen/rejected)\n      - Document what \"better\
    \ response\" means explicitly\n      - Calibrate evaluators on edge cases before\
    \ full evaluation\n      - Use LLM-as-a-judge to check alignment with human preferences\n\
    \      \n      **PALETTE integration:**\n      - Document label definitions in\
    \ RIU-082 (Label/Category Alignment Check)\n      - Track agreement metrics in\
    \ RIU-084 (Data Quality Checks)\n      - Store boundary examples in RIU-014 (Edge-Case\
    \ Catalog)\n      - Get stakeholder sign-off in decisions.md as ONE-WAY DOOR\n\
    \      \n      Key insight: If two experts disagree on a label, the definition\
    \ is ambiguous â€” fix the definition before blaming the labelers. Semantic validation\
    \ happens in workshops and reviews, not in code."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-003
  - RIU-014
  - RIU-080
  - RIU-081
  - RIU-082
  - RIU-084
  difficulty: critical
  industries:
  - AI/ML
  - Analytics
  - Healthcare
  - Finance
  tags:
  - label-validation
  - data-quality
  - semantic-validation
  - training-data
  sources:
  - title: Use the wisdom of crowds with Amazon SageMaker Ground Truth to annotate
      data more accurately
    url: https://aws.amazon.com/blogs/machine-learning/use-the-wisdom-of-crowds-with-amazon-sagemaker-ground-truth-to-annotate-data-more-accurately/
  - title: Verifying and adjusting your data labels with Amazon SageMaker Ground Truth
    url: https://aws.amazon.com/blogs/machine-learning/verifying-and-adjusting-your-data-labels-to-create-higher-quality-training-datasets-with-amazon-sagemaker-ground-truth/
  - title: Inspect your data labels with Amazon SageMaker Ground Truth Plus
    url: https://aws.amazon.com/blogs/machine-learning/inspect-your-data-labels-with-a-visual-no-code-tool-to-create-high-quality-training-datasets-with-amazon-sagemaker-ground-truth-plus/
  - title: Data Management - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_datamanagement.html
- id: LIB-037
  question: What evaluation metrics actually predict production AI performance?
  answer: "Most offline metrics don't predict production success. Focus on metrics\
    \ that correlate with business outcomes and user satisfaction, not just technical\
    \ accuracy.\n      \n      **The metric hierarchy (predictive power):**\n    \
    \  \n      | Metric Type | Predicts Production Success? | Why |\n      |-------------|------------------------------|-----|\n\
    \      | Business outcome metrics | âœ… High | Directly measures what matters |\n\
    \      | User satisfaction proxies | âœ… High | Correlates with adoption |\n   \
    \   | Task completion rate | âœ… Medium-High | Measures real utility |\n      |\
    \ Domain-specific quality | âœ… Medium | Captures use-case fit |\n      | Generic\
    \ accuracy benchmarks | âš ï¸ Low | May not reflect your data/use case |\n      |\
    \ Perplexity/loss | âŒ Very Low | Technical, not business-relevant |\n      \n\
    \      **Three-dimensional evaluation framework:**\n      \n      **1. Behavior\
    \ metrics (does it work correctly?)**\n      - **Correctness**: Factual accuracy,\
    \ verifiable claims\n      - **Completeness**: Answers the full question\n   \
    \   - **Faithfulness**: Grounded in provided context (RAG)\n      - **Coherence**:\
    \ Logical, well-structured responses\n      - **Safety**: Toxicity, bias, harmful\
    \ content detection\n      - **Brand voice**: Tone and style alignment\n     \
    \ \n      **2. Cost metrics (is it economically viable?)**\n      - Cost per request\
    \ (tokens + compute)\n      - Cost per successful task completion\n      - Token\
    \ efficiency (output quality per token)\n      - Infrastructure cost at projected\
    \ scale\n      \n      **3. Speed metrics (is it fast enough?)**\n      - Time\
    \ to first token (TTFT)\n      - End-to-end latency (p50, p95, p99)\n      - Throughput\
    \ at peak load\n      \n      **Metrics by application type:**\n      \n     \
    \ | Application | Priority Metrics |\n      |-------------|------------------|\n\
    \      | RAG/Q&A | Context relevance, correctness, faithfulness, citation accuracy\
    \ |\n      | Healthcare | Correctness, completeness, helpfulness, logical coherence\
    \ |\n      | Customer support | Resolution rate, escalation rate, CSAT correlation\
    \ |\n      | Content generation | Brand voice, originality, factual accuracy |\n\
    \      | Classification | Precision, recall, F1 (but validate on production distribution)\
    \ |\n      | Agents | Task completion rate, tool use accuracy, error recovery\
    \ |\n      \n      **Metrics that actually predict production success:**\n   \
    \   \n      1. **Task completion rate on realistic scenarios**\n         - Not\
    \ synthetic benchmarks â€” real user intents\n         - Include edge cases from\
    \ production logs\n      \n      2. **Human preference alignment**\n         -\
    \ Win rate vs. baseline in blind comparisons\n         - Correlation coefficient\
    \ with human ratings (target: Ï > 0.8)\n      \n      3. **Error rate on high-stakes\
    \ decisions**\n         - Where mistakes have real consequences\n         - False\
    \ positive/negative rates for your use case\n      \n      4. **Latency under\
    \ production-like load**\n         - Not just average â€” p99 matters for user experience\n\
    \      \n      5. **Cost per successful outcome**\n         - Not cost per request\
    \ â€” cost per value delivered\n      \n      **Validating offline metrics predict\
    \ production:**\n      ```\n      1. Deploy to shadow/canary environment\n   \
    \   2. Collect production inputs, run through new model\n      3. Compare offline\
    \ evaluation scores to:\n         - User feedback (thumbs up/down, escalations)\n\
    \         - Task completion rates\n         - Business metrics (conversion, resolution)\n\
    \      4. Calculate correlation â€” if low, your offline metrics are wrong\n   \
    \   ```\n      \n      **AWS implementation:**\n      - **Amazon Bedrock Evaluations**:\
    \ Programmatic + model-as-judge\n      - **Custom metrics**: Define business-specific\
    \ criteria\n      - **Automated pipelines**: Amazon Nova for continuous evaluation\n\
    \      - **CloudWatch**: Cost and latency monitoring\n      \n      **Red flags\
    \ your metrics don't predict production:**\n      - High offline scores but poor\
    \ user feedback\n      - Model \"wins\" on benchmarks but users prefer old system\n\
    \      - Metrics improve but business KPIs don't move\n      - Different ranking\
    \ on test set vs. production sample\n      \n      **PALETTE integration:**\n\
    \      - Define metrics in RIU-083 (Evaluation Metric Selection)\n      - Track\
    \ in RIU-063 (Performance Baselines)\n      - Validate with RIU-021 (Golden Set\
    \ + Offline Evaluation)\n      - Monitor production correlation in RIU-540 (Evaluation\
    \ Harness)\n      \n      Key insight: The best metric is the one that, when it\
    \ improves offline, business outcomes improve in production. If you don't know\
    \ this correlation, you're optimizing blind."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-021
  - RIU-063
  - RIU-082
  - RIU-083
  - RIU-540
  difficulty: high
  industries:
  - AI/ML
  - All
  tags:
  - evaluation
  - metrics
  - performance-prediction
  - validation
  sources:
  - title: 'Going beyond vibes: Evaluating your Amazon Bedrock workloads for production'
    url: https://aws.amazon.com/blogs/publicsector/going-beyond-vibes-evaluating-your-amazon-bedrock-workloads-for-production/
  - title: Use custom metrics to evaluate your generative AI application with Amazon
      Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/
  - title: Evaluate healthcare generative AI applications using LLM-as-a-judge on
      AWS
    url: https://aws.amazon.com/blogs/machine-learning/evaluate-healthcare-generative-ai-applications-using-llm-as-a-judge-on-aws/
  - title: Build an automated generative AI solution evaluation pipeline with Amazon
      Nova
    url: https://aws.amazon.com/blogs/machine-learning/build-an-automated-generative-ai-solution-evaluation-pipeline-with-amazon-nova/
- id: LIB-038
  question: How do I build continuous evaluation loops for AI systems in production?
  answer: "Continuous evaluation closes the loop: observe production behavior â†’ evaluate\
    \ quality â†’ improve the system â†’ repeat. Without loop closure, you're just monitoring,\
    \ not improving.\n      \n      **The continuous evaluation loop:**\n      ```\n\
    \      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚  \
    \                                                       â”‚\n      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n      â”‚    â”‚ OBSERVE  â”‚â”€â”€â”€â–¶â”‚ EVALUATE\
    \ â”‚â”€â”€â”€â–¶â”‚ IMPROVE  â”‚        â”‚\n      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \        â”‚\n      â”‚         â–²                                   â”‚           â”‚\n\
    \      â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n      â”‚  \
    \                                                       â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **1. OBSERVE: Collect production data**\n      \n  \
    \    **Explicit feedback:**\n      - Thumbs up/down on responses\n      - User\
    \ corrections/edits to AI output\n      - Escalation to human (implicit negative\
    \ signal)\n      - Report buttons for errors/issues\n      \n      **Implicit\
    \ feedback:**\n      - Task completion rate (did user finish their goal?)\n  \
    \    - Follow-up queries (confused user = bad response)\n      - Session duration\
    \ and engagement\n      - Copy/paste behavior (useful response)\n      - Regeneration\
    \ requests (unsatisfied)\n      \n      **Logging requirements:**\n      ```yaml\n\
    \      log_entry:\n        request_id: \"uuid\"  # Link feedback to specific request\n\
    \        timestamp: \"ISO8601\"\n        user_id: \"anonymized\"\n        input:\
    \ \"user query\"\n        output: \"model response\"\n        model_version: \"\
    v1.2.3\"\n        latency_ms: 450\n        token_count: {input: 50, output: 200}\n\
    \        feedback: null  # Populated later if received\n        metadata: {session_id,\
    \ device, etc.}\n      ```\n      \n      **2. EVALUATE: Assess quality continuously**\n\
    \      \n      **Sampling strategy:**\n      - 100% logging, sampled evaluation\n\
    \      - Random sample: 1-5% for baseline quality\n      - Stratified sample:\
    \ Oversample high-stakes or low-confidence\n      - Triggered sample: 100% of\
    \ flagged/escalated cases\n      \n      **Evaluation methods:**\n      | Method\
    \ | Use Case | Frequency |\n      |--------|----------|-----------|\n      | Automated\
    \ metrics | All traffic | Real-time |\n      | LLM-as-a-judge | Quality assessment\
    \ | Hourly/daily batch |\n      | Human review | Ground truth calibration | Weekly\
    \ sample |\n      | A/B comparison | Model updates | Per deployment |\n      \n\
    \      **AWS implementation:**\n      ```\n      Production Logs â†’ Kinesis â†’ S3\
    \ (raw)\n                                    â†“\n                          Step\
    \ Functions pipeline\n                                    â†“\n                \
    \    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â†“               â†“\
    \               â†“\n              FMEval         LLM-as-Judge      Ragas (RAG)\n\
    \                    â†“               â†“               â†“\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \                                    â†“\n                          CloudWatch Dashboards\n\
    \                                    â†“\n                          Alerts on degradation\n\
    \      ```\n      \n      **3. IMPROVE: Close the loop**\n      \n      **Automated\
    \ improvement triggers:**\n      - **Drift detected** â†’ Alert + auto-retrain pipeline\n\
    \      - **New failure pattern** â†’ Add to test suite automatically\n      - **Low-scoring\
    \ responses** â†’ Queue for human review\n      - **High-value feedback** â†’ Create\
    \ new golden set examples\n      \n      **Feedback â†’ test case automation:**\n\
    \      ```python\n      # When user reports failure with explanation\n      if\
    \ feedback.type == \"error_report\" and feedback.explanation:\n          new_test_case\
    \ = {\n              \"input\": original_request.input,\n              \"expected\"\
    : feedback.correction or \"should_not_match\",\n              \"source\": f\"\
    user_feedback_{feedback.id}\",\n              \"priority\": \"high\"\n       \
    \   }\n          add_to_evaluation_dataset(new_test_case)\n      ```\n      \n\
    \      **Loop closure mechanisms:**\n      | Signal | Action | Timeline |\n  \
    \    |--------|--------|----------|\n      | Quality score drops | Alert on-call\
    \ | Minutes |\n      | Repeated failure pattern | Add regression tests | Hours\
    \ |\n      | User corrections | Fine-tune or update prompts | Days |\n      |\
    \ Drift detected | Trigger retraining | Hours-Days |\n      | New edge cases |\
    \ Update golden set | Weekly |\n      \n      **4. Governance and ownership**\n\
    \      \n      - Assign clear owner for each loop stage\n      - Define SLAs:\
    \ \"Issues detected â†’ action within X hours\"\n      - Regular review: Weekly\
    \ evaluation review meeting\n      - Track loop metrics: Time from detection to\
    \ improvement\n      \n      **PALETTE integration:**\n      - Define evaluation\
    \ pipeline in RIU-540 (Evaluation Harness)\n      - Track metrics in RIU-083 (Evaluation\
    \ Metric Selection)\n      - Store golden set updates in RIU-021\n      - Document\
    \ improvement actions in RIU-532 (Model Registry)\n      - Alert thresholds in\
    \ RIU-061 (Observability Baseline)\n      \n      Key insight: The value isn't\
    \ in collecting feedback â€” it's in systematically acting on it. An \"observe-evaluate-improve\"\
    \ loop with clear ownership reduces intervention time from weeks to hours."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-021
  - RIU-061
  - RIU-082
  - RIU-083
  - RIU-532
  - RIU-540
  difficulty: high
  industries:
  - AI/ML
  - All
  tags:
  - continuous-evaluation
  - monitoring
  - feedback-loops
  - quality-assurance
  sources:
  - title: Build an automated generative AI solution evaluation pipeline with Amazon
      Nova
    url: https://aws.amazon.com/blogs/machine-learning/build-an-automated-generative-ai-solution-evaluation-pipeline-with-amazon-nova/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Model Evaluation and Selection Criteria Overview
    url: https://awslabs.github.io/generative-ai-atlas/topics/2_0_technical_foundations_and_patterns/2_6_model_evaluation_and_selection_criteria/index.html
  - title: Deploying generative AI applications
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_deployment.html
- id: LIB-039
  question: What's the best way to handle inconsistent data definitions across departments?
  answer: "Inconsistent definitions are a governance problem, not a technical problem.\
    \ You can't automate your way out â€” you need shared vocabulary with enforcement.\n\
    \      \n      **The problem:**\n      - Sales says \"customer\" = anyone with\
    \ an account\n      - Finance says \"customer\" = anyone who has paid\n      -\
    \ Support says \"customer\" = anyone who has contacted us\n      - AI model trained\
    \ on \"customer\" data â€” which definition?\n      \n      **Solution: Canonical\
    \ business glossary + enforcement**\n      \n      **Step 1: Establish governance\
    \ structure**\n      - **Data governance champion**: Single accountable owner\n\
    \      - **Cross-functional governance committee**: Representatives from each\
    \ department\n      - **Domain owners**: Department-level authority for their\
    \ data\n      - Use Amazon DataZone domain units to organize by business unit\n\
    \      \n      **Step 2: Create canonical business glossary**\n      ```yaml\n\
    \      glossary_term:\n        term: \"customer\"\n        canonical_definition:\
    \ \"Entity with at least one completed paid transaction\"\n        owner: \"Finance\"\
    \n        approved_date: \"2024-03-15\"\n        \n        department_mappings:\n\
    \          sales: \n            local_term: \"account\"\n            relationship:\
    \ \"superset\"  # All customers are accounts, not all accounts are customers\n\
    \          support:\n            local_term: \"contact\"\n            relationship:\
    \ \"overlapping\"  # Some contacts are customers, some aren't\n            \n\
    \        usage_guidance: \"For AI training on 'customer' data, use this definition\
    \ unless explicitly scoped otherwise\"\n        \n        related_terms: [\"prospect\"\
    , \"lead\", \"account\", \"user\"]\n      ```\n      \n      **Step 3: Map conflicting\
    \ definitions**\n      | Department | Their Term | Canonical Term | Relationship\
    \ | Transformation |\n      |------------|-----------|----------------|--------------|----------------|\n\
    \      | Sales | account | customer | superset | Filter: has_paid = true |\n \
    \     | Support | contact | customer | overlapping | Join with transactions |\n\
    \      | Marketing | lead | prospect | equivalent | Direct mapping |\n      \n\
    \      **Step 4: Enforce through tooling**\n      - **Amazon SageMaker Catalog**:\
    \ Metadata enforcement rules require glossary terms before publishing\n      -\
    \ **Amazon DataZone**: Unified portal with business context and access governance\n\
    \      - **AWS Glue Schema Registry**: Schema validation with canonical field\
    \ names\n      - **Collibra integration**: Bidirectional sync for enterprise-wide\
    \ consistency\n      \n      **Step 5: Technical implementation**\n      ```\n\
    \      Source Data (dept definitions)\n              â†“\n      Transformation Layer\
    \ (mapping rules)\n              â†“\n      Canonical Data Layer (glossary-aligned)\n\
    \              â†“\n      AI/Analytics Consumption\n      ```\n      \n      **Resolution\
    \ process for conflicts:**\n      1. **Identify conflict**: Same term, different\
    \ meanings\n      2. **Document both definitions**: What does each department\
    \ actually mean?\n      3. **Determine canonical**: Which definition serves enterprise-wide\
    \ use?\n      4. **Create mappings**: How to transform from local to canonical\n\
    \      5. **Get sign-off**: Cross-functional committee approval\n      6. **Enforce**:\
    \ Metadata rules require canonical terms for shared assets\n      \n      **Common\
    \ pitfalls:**\n      - Creating glossary but not enforcing it (becomes shelfware)\n\
    \      - Forcing one department's definition on others (creates resistance)\n\
    \      - Not documenting mappings (breaks downstream when source changes)\n  \
    \    - Treating this as IT problem (it's a business alignment problem)\n     \
    \ \n      **For AI specifically:**\n      - Document which definition was used\
    \ for training data\n      - Include glossary version in model metadata\n    \
    \  - Alert if source data definition changes (potential drift)\n      - Validate\
    \ that inference data uses same definition as training\n      \n      **PALETTE\
    \ integration:**\n      - Document canonical definitions in RIU-042 (Taxonomy\
    \ Alignment)\n      - Track definition changes as potential ONE-WAY DOORs (RIU-003)\n\
    \      - Validate alignment with RIU-082 (Label/Category Alignment Check)\n  \
    \    - Include in data dictionary (RIU-011)\n      \n      Key insight: The goal\
    \ isn't to force everyone to use the same definition â€” it's to know which definition\
    \ applies in each context and transform accordingly. Map, don't mandate."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-003
  - RIU-011
  
  - RIU-080
  - RIU-081
  - RIU-082
  difficulty: high
  industries:
  - Enterprise SaaS
  - Finance
  - Healthcare
  tags:
  - data-governance
  - semantic-harmonization
  - cross-functional
  - standards
  sources:
  - title: Organize content across business units with Amazon DataZone domain units
    url: https://aws.amazon.com/blogs/big-data/organize-content-across-business-units-with-enterprise-wide-data-governance-using-amazon-datazone-domain-units-and-authorization-policies/
  - title: Enforce business glossary classification rules in Amazon SageMaker Catalog
    url: https://aws.amazon.com/blogs/big-data/enforce-business-glossary-classification-rules-in-amazon-sagemaker-catalog/
  - title: Amazon DataZone Now Generally Available
    url: https://aws.amazon.com/blogs/aws/amazon-datazone-now-generally-available-collaborate-on-data-projects-across-organizational-boundaries/
  - title: Unifying metadata governance across Amazon SageMaker and Collibra
    url: https://aws.amazon.com/blogs/big-data/unifying-metadata-governance-across-amazon-sagemaker-and-collibra/
- id: LIB-040
  question: How do I set data quality thresholds that balance accuracy and velocity?
  answer: "Quality thresholds aren't one-size-fits-all. Set them based on business\
    \ impact: tighter for high-stakes data, looser for speed-critical flows.\n   \
    \   \n      **The tradeoff framework:**\n      ```\n      Accuracy â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’\
    \ Velocity\n      \n      Higher thresholds:           Lower thresholds:\n   \
    \   - More data rejected         - More data passes\n      - Higher quality output\
    \      - More noise in output\n      - Slower throughput          - Faster throughput\n\
    \      - Higher investigation cost  - Higher error cost downstream\n      ```\n\
    \      \n      **Step 1: Classify data by business impact**\n      \n      | Tier\
    \ | Description | Example | Threshold Approach |\n      |------|-------------|---------|-------------------|\n\
    \      | Critical | Errors cause financial/legal harm | Financial transactions,\
    \ PII | Strict, block on failure |\n      | Important | Errors degrade user experience\
    \ | Customer-facing AI outputs | Moderate, alert + continue |\n      | Standard\
    \ | Errors are inconvenient | Internal analytics | Relaxed, log only |\n     \
    \ | Experimental | Errors are expected | Dev/test data | Minimal checks |\n  \
    \    \n      **Step 2: Establish baselines**\n      Before setting thresholds,\
    \ measure current state:\n      ```yaml\n      baseline_metrics:\n        completeness:\
    \ 98.5%  # % non-null for required fields\n        uniqueness: 99.9%    # % unique\
    \ for key fields\n        validity: 97.2%      # % matching format/range rules\n\
    \        freshness: \"< 1 hour\"\n        volume: \"45,000-55,000 records/day\"\
    \n      ```\n      \n      **Step 3: Set thresholds by tier**\n      \n      ```yaml\n\
    \      # AWS Glue DQDL example\n      Rules = [\n        # Critical tier - strict\n\
    \        Completeness \"customer_id\" >= 99.9,\n        IsUnique \"transaction_id\"\
    ,\n        ColumnValues \"amount\" between 0 and 1000000,\n        \n        #\
    \ Important tier - moderate  \n        Completeness \"email\" >= 95.0,\n     \
    \   ColumnValues \"status\" in [\"active\", \"pending\", \"closed\"],\n      \
    \  \n        # Standard tier - relaxed\n        Completeness \"notes\" >= 80.0\n\
    \      ]\n      ```\n      \n      **Step 4: Define threshold types**\n      \n\
    \      | Type | Use Case | Example |\n      |------|----------|---------|\n  \
    \    | Absolute | Known business rules | `amount >= 0` |\n      | Statistical\
    \ | Detect anomalies | `mean(amount) within 2 std of baseline` |\n      | Relative\
    \ | Detect drift | `today's completeness >= 95% of 7-day avg` |\n      | Dynamic\
    \ | Adapt to patterns | `compare to same day last week` |\n      \n      **Step\
    \ 5: Configure actions by severity**\n      \n      ```yaml\n      threshold_actions:\n\
    \        critical_failure:\n          action: \"block\"\n          notify: [\"\
    on-call\", \"data-owner\"]\n          quarantine: true\n          \n        warning:\n\
    \          action: \"continue\"\n          notify: [\"data-team\"]\n         \
    \ log: true\n          \n        info:\n          action: \"continue\"\n     \
    \     log: true\n      ```\n      \n      **Implementation with AWS Glue Data\
    \ Quality:**\n      - Use DQDL labels to tag rules by priority/owner\n      -\
    \ Route failed records to separate S3 bucket (quarantine)\n      - Emit metrics\
    \ to CloudWatch for dashboards/alerts\n      - Set up dynamic rules comparing\
    \ to historical values\n      \n      **Threshold tuning methodology:**\n    \
    \  1. Start with baselines from historical data\n      2. Set initial thresholds\
    \ at baseline - 2 standard deviations\n      3. Run in \"alert only\" mode for\
    \ 2 weeks\n      4. Analyze alerts: false positives? missed issues?\n      5.\
    \ Adjust thresholds based on business feedback\n      6. Gradually tighten for\
    \ critical data\n      \n      **Balancing accuracy vs. velocity:**\n      \n\
    \      | Scenario | Favor Accuracy | Favor Velocity |\n      |----------|----------------|----------------|\n\
    \      | Real-time AI inference | | âœ… |\n      | Financial reporting | âœ… | |\n\
    \      | Customer-facing features | Balance | Balance |\n      | Training data\
    \ pipelines | âœ… | |\n      | Exploratory analytics | | âœ… |\n      | Compliance/audit\
    \ data | âœ… | |\n      \n      **PALETTE integration:**\n      - Define thresholds\
    \ in RIU-084 (Data Quality Checks)\n      - Document baseline in RIU-081 (Smoke\
    \ Tests)\n      - Track quality metrics in RIU-063 (Performance Baselines)\n \
    \     - Alert on breaches via RIU-061 (Observability Baseline)\n      \n     \
    \ Key insight: The right threshold makes the cost of false positives (good data\
    \ rejected) roughly equal to the cost of false negatives (bad data accepted).\
    \ If you're constantly overriding alerts, thresholds are too tight. If errors\
    \ reach users, they're too loose."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-061
  - RIU-063
  - RIU-081
  - RIU-082
  - RIU-083
  - RIU-084
  difficulty: high
  industries:
  - All
  tags:
  - quality-thresholds
  - tradeoffs
  - velocity
  - accuracy
  sources:
  - title: Enable strategic data quality management with AWS Glue DQDL labels
    url: https://aws.amazon.com/blogs/big-data/enable-strategic-data-quality-management-with-aws-glue-dqdl-labels/
  - title: Accelerate your data quality journey for lakehouse architecture
    url: https://aws.amazon.com/blogs/big-data/accelerate-your-data-quality-journey-for-lakehouse-architecture-with-amazon-sagemaker-apache-iceberg-on-aws-amazon-s3-tables-and-aws-glue-data-quality/
  - title: AWS Glue Data Quality Workshop
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/bd8bdbc7-11cb-4d16-9e76-1404e6d37e53
  - title: Business Value and use cases - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/1_0_generative_ai_fundamentals/1_2_business_value_and_use_cases/1_2_business_value_and_use_cases.html
- id: LIB-041
  question: What testing strategy catches semantic data bugs before production?
  answer: "Semantic bugs are data that passes schema validation but is meaningfully\
    \ wrong (swapped columns, wrong units, misinterpreted categories). Catch them\
    \ with \"unit tests for data\" that verify business meaning, not just structure.\n\
    \      \n      **Types of semantic data bugs:**\n      | Bug Type | Example |\
    \ Schema Catches? | Semantic Test Catches? |\n      |----------|---------|-----------------|------------------------|\n\
    \      | Swapped columns | customer_id in order_id field | âŒ (both strings) |\
    \ âœ… (format check) |\n      | Wrong units | Pounds stored as kilograms | âŒ (both\
    \ numbers) | âœ… (range check) |\n      | Misinterpreted enum | \"HIGH\"=3 vs \"\
    HIGH\"=1 | âŒ (valid enum) | âœ… (business rule) |\n      | Stale reference | customer_id\
    \ doesn't exist | âŒ (valid format) | âœ… (referential check) |\n      | Aggregation\
    \ error | Sum doesn't equal parts | âŒ (valid number) | âœ… (consistency check) |\n\
    \      | Timezone confusion | UTC stored as local time | âŒ (valid timestamp) |\
    \ âœ… (range/distribution) |\n      \n      **Testing pyramid for data quality:**\n\
    \      ```\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Manual\
    \ Review  â”‚  â† Sample inspection\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  \
    \                  â”‚ Cross-Dataset   â”‚  â† Compare sources\n                  \
    \  â”‚  Validation     â”‚\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n            \
    \        â”‚ Business Rule   â”‚  â† Domain constraints\n                    â”‚    Tests\
    \        â”‚\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                    â”‚ Statistical\
    \     â”‚  â† Distribution checks\n                    â”‚    Tests        â”‚\n    \
    \                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                    â”‚ Schema Tests    â”‚\
    \  â† Type/format (base)\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      ```\n\
    \      \n      **Layer 1: Unit tests for data (Deequ / AWS Glue Data Quality)**\n\
    \      ```python\n      # Deequ example - semantic assertions\n      from pydeequ.checks\
    \ import Check\n      \n      check = Check(spark, CheckLevel.Error, \"Semantic\
    \ Validation\") \\\n          # Format checks (catch swapped columns)\n      \
    \    .hasPattern(\"order_id\", r\"^ORD-\\d{10}$\") \\\n          .hasPattern(\"\
    customer_id\", r\"^CUST-\\d{8}$\") \\\n          \n          # Range checks (catch\
    \ unit errors)\n          .isNonNegative(\"amount\") \\\n          .isLessThanOrEqualTo(\"\
    amount\", 1000000) \\\n          .isContainedIn(\"currency\", [\"USD\", \"EUR\"\
    , \"GBP\"]) \\\n          \n          # Business rule checks\n          .isContainedIn(\"\
    priority\", [\"high\", \"low\"]) \\\n          .satisfies(\"discount <= amount\"\
    , \"discount can't exceed amount\") \\\n          \n          # Referential checks\n\
    \          .isContainedIn(\"customer_id\", valid_customer_ids) \\\n          \n\
    \          # Consistency checks\n          .satisfies(\"line_total == quantity\
    \ * unit_price\", \"line math\")\n      ```\n      \n      **Layer 2: Statistical\
    \ tests (catch distribution shifts)**\n      ```yaml\n      statistical_checks:\n\
    \        - metric: \"mean(amount)\"\n          expected_range: [100, 500]  # Based\
    \ on historical baseline\n          \n        - metric: \"null_rate(email)\"\n\
    \          max_threshold: 0.05  # No more than 5% nulls\n          \n        -\
    \ metric: \"distinct_count(category)\"\n          expected: 12  # Should always\
    \ have exactly 12 categories\n          \n        - metric: \"value_distribution(status)\"\
    \n          expected:\n            active: 0.70-0.80\n            pending: 0.15-0.25\n\
    \            closed: 0.05-0.10\n      ```\n      \n      **Layer 3: Cross-dataset\
    \ validation (catch integration bugs)**\n      - Compare source and target after\
    \ transformation\n      - Verify row counts match (or explain difference)\n  \
    \    - Check that joins don't duplicate/lose records\n      - Use Apache Griffin\
    \ for large-scale dataset comparison\n      ```python\n      # Griffin-style comparison\n\
    \      assert count(source) == count(target), \"Row count mismatch\"\n      assert\
    \ sum(source.amount) == sum(target.amount), \"Amount sum mismatch\"\n      assert\
    \ distinct(source.customer_id) == distinct(target.customer_id)\n      ```\n  \
    \    \n      **Layer 4: Example-based tests (golden set)**\n      - Curate specific\
    \ examples with known correct outputs\n      - Include edge cases and boundary\
    \ conditions\n      - Run as regression tests on every pipeline change\n     \
    \ ```yaml\n      golden_examples:\n        - input: {order_id: \"ORD-0000000001\"\
    , amount: 0}\n          expected: {risk_score: \"low\"}  # Zero-value order =\
    \ low risk\n          \n        - input: {order_id: \"ORD-9999999999\", amount:\
    \ 999999}\n          expected: {risk_score: \"high\"}  # Max-value order = high\
    \ risk\n          \n        - input: {customer_id: null}\n          expected:\
    \ {should_fail_validation: true}\n      ```\n      \n      **CI/CD integration:**\n\
    \      - Run data unit tests on every pipeline change\n      - Block deployment\
    \ if semantic tests fail\n      - Sample production data into test environment\
    \ weekly\n      - Compare test results against known-good baseline\n      \n \
    \     **AWS implementation:**\n      - **Deequ**: Unit tests for Spark-based pipelines\n\
    \      - **AWS Glue Data Quality**: DQDL rules in Glue jobs\n      - **Apache\
    \ Griffin on EMR**: Large-scale dataset comparison\n      - **CloudWatch**: Alert\
    \ on test failures\n      \n      **PALETTE integration:**\n      - Define semantic\
    \ tests in RIU-082 (Label/Category Alignment Check)\n      - Store golden examples\
    \ in RIU-021 (Golden Set)\n      - Run as part of RIU-081 (E2E Smoke Tests)\n\
    \      - Document business rules in RIU-044 (Business Rules Documentation)\n \
    \     \n      Key insight: Schema tests are necessary but not sufficient. The\
    \ most dangerous bugs are semantically wrong data that looks structurally correct.\
    \ Test what the data *means*, not just what it *looks like*."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-021
  
  - RIU-081
  - RIU-082
  - RIU-084
  - RIU-540
  difficulty: high
  industries:
  - All
  tags:
  - testing
  - semantic-validation
  - pre-production
  - quality-assurance
  sources:
  - title: Deequ - Unit tests for data
    url: https://github.com/awslabs/deequ
  - title: AWS Glue Data Quality Workshop
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/bd8bdbc7-11cb-4d16-9e76-1404e6d37e53
  - title: Automate large-scale data validation using Amazon EMR and Apache Griffin
    url: https://aws.amazon.com/blogs/big-data/automate-large-scale-data-validation-using-amazon-emr-and-apache-griffin/
- id: LIB-042
  question: How do I version control datasets and models together for reproducibility?
  answer: "Reproducibility requires linking exact dataset versions to exact model\
    \ versions. Version them together with shared lineage, not separately.\n     \
    \ \n      **The reproducibility equation:**\n      ```\n      Model v1.2.3 = Code\
    \ v1.2.3 + Data v2024-06-15 + Config v1.2.3\n      \n      If any component changes\
    \ without versioning â†’ reproducibility broken\n      ```\n      \n      **Version\
    \ control strategy:**\n      \n      | Component | Tool | What to Track |\n  \
    \    |-----------|------|---------------|\n      | Code | Git | Training scripts,\
    \ prompts, configs |\n      | Small datasets (<1GB) | Git LFS or DVC | CSVs, JSONs,\
    \ evaluation sets |\n      | Large datasets | LakeFS, S3 versioning, DVC | Training\
    \ data, embeddings |\n      | Models | SageMaker Model Registry | Model artifacts,\
    \ hyperparameters |\n      | Experiments | SageMaker Experiments | Metrics, parameters,\
    \ lineage |\n      \n      **Dataset versioning patterns:**\n      \n      **Pattern\
    \ 1: Immutable snapshots (recommended)**\n      ```\n      s3://data-bucket/datasets/\n\
    \        â”œâ”€â”€ customers/\n        â”‚   â”œâ”€â”€ v2024-06-01/\n        â”‚   â”œâ”€â”€ v2024-06-15/\
    \  â† Training data for model v1.2.3\n        â”‚   â””â”€â”€ v2024-07-01/\n        â””â”€â”€\
    \ orders/\n            â”œâ”€â”€ v2024-06-01/\n            â””â”€â”€ v2024-06-15/\n      ```\n\
    \      - Never modify existing versions\n      - Create new version for any change\n\
    \      - Reference by version ID in training config\n      \n      **Pattern 2:\
    \ Git-like branches with LakeFS**\n      ```\n      lakefs://repo/main/datasets/customers/\n\
    \      lakefs://repo/experiment-123/datasets/customers/  â† Branch for experiment\n\
    \      ```\n      - Branch for experiments, merge when validated\n      - Full\
    \ Git semantics (commit, diff, merge)\n      - Works with existing S3-compatible\
    \ tools\n      \n      **Pattern 3: DVC + Git (code + data together)**\n     \
    \ ```bash\n      # Track data with DVC, metadata in Git\n      dvc add data/training.csv\n\
    \      git add data/training.csv.dvc\n      git commit -m \"Training data v2024-06-15\"\
    \n      \n      # Reproduce exact experiment\n      git checkout v1.2.3\n    \
    \  dvc checkout  # Pulls matching data version\n      ```\n      \n      **Linking\
    \ models to data (lineage):**\n      \n      ```yaml\n      model_metadata:\n\
    \        model_id: \"order-risk-v1.2.3\"\n        model_artifact: \"s3://models/order-risk/v1.2.3/\"\
    \n        \n        training_data:\n          dataset: \"customers\"\n       \
    \   version: \"v2024-06-15\"\n          s3_path: \"s3://data-bucket/datasets/customers/v2024-06-15/\"\
    \n          row_count: 1250000\n          hash: \"sha256:abc123...\"\n       \
    \   \n        evaluation_data:\n          dataset: \"eval-set-v3\"\n         \
    \ version: \"v2024-06-10\"\n          \n        code:\n          git_commit: \"\
    abc123def456\"\n          git_repo: \"https://github.com/org/ml-pipeline\"\n \
    \         \n        config:\n          hyperparameters: {learning_rate: 0.001,\
    \ epochs: 10}\n          prompt_version: \"v2.1\"\n      ```\n      \n      **AWS\
    \ implementation with SageMaker:**\n      ```python\n      # Register model with\
    \ lineage\n      from sagemaker.model import Model\n      from sagemaker.model_registry\
    \ import ModelPackage\n      \n      model_package = ModelPackage(\n         \
    \ model_package_arn=model_arn,\n          model_data=model_s3_uri,\n         \
    \ \n          # Link to data version\n          customer_metadata_properties={\n\
    \              \"training_data_version\": \"v2024-06-15\",\n              \"training_data_s3\"\
    : \"s3://bucket/datasets/v2024-06-15/\",\n              \"training_data_hash\"\
    : \"sha256:abc123...\",\n              \"git_commit\": \"abc123def456\"\n    \
    \      }\n      )\n      ```\n      \n      **For GenAI/RAG systems:**\n     \
    \ - Version source documents separately from embeddings\n      - Track embedding\
    \ model version (changing it invalidates all vectors)\n      - Include chunk strategy\
    \ and parameters in version metadata\n      ```yaml\n      rag_version:\n    \
    \    knowledge_base_id: \"kb-v2024-06-15\"\n        source_documents: \"s3://docs/v2024-06-01/\"\
    \n        embedding_model: \"amazon.titan-embed-text-v2\"\n        chunk_size:\
    \ 512\n        chunk_overlap: 50\n        vector_store_snapshot: \"opensearch-index-v2024-06-15\"\
    \n      ```\n      \n      **Minimum viable versioning checklist:**\n      - [\
    \ ] Dataset versions are immutable (never modified in place)\n      - [ ] Model\
    \ metadata includes exact dataset version used\n      - [ ] Code commit hash recorded\
    \ with each training run\n      - [ ] Hyperparameters and configs versioned\n\
    \      - [ ] Can reproduce any past model from stored artifacts\n      \n    \
    \  **PALETTE integration:**\n      - Track dataset versions in RIU-520 (Prompt/Data\
    \ Version Control)\n      - Register models in RIU-532 (Model Registry Integration)\n\
    \      - Document lineage in RIU-083 (Evaluation Metric Selection)\n      - Store\
    \ evaluation sets in RIU-021 (Golden Set)\n      \n      Key insight: \"Versioning\"\
    \ means nothing if you can't answer: \"What exact data trained this exact model?\"\
    \ If you don't record that link, you can't reproduce or debug."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-021
  - RIU-083
  - RIU-520
  - RIU-532
  difficulty: medium
  industries:
  - AI/ML
  - All
  tags:
  - version-control
  - reproducibility
  - mlops
  - data-lineage
  sources:
  - title: Data Management - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_datamanagement.html
  - title: Tracking and managing assets used in AI development with Amazon SageMaker
      AI
    url: https://aws.amazon.com/blogs/machine-learning/tracking-and-managing-assets-used-in-ai-development-with-amazon-sagemaker-ai/
  - title: Track your ML experiments end to end with Data Version Control and Amazon
      SageMaker Experiments
    url: https://aws.amazon.com/blogs/machine-learning/track-your-ml-experiments-end-to-end-with-data-version-control-and-amazon-sagemaker-experiments/
- id: LIB-043
  question: What's the checklist for 'production-ready' data quality in AI systems?
  answer: "Use this checklist before deploying AI systems to production. All items\
    \ should be \"PASS\" or explicitly \"N/A with rationale\" before go-live.\n  \
    \    \n      **SECTION 1: Security & Privacy (MUST PASS ALL)**\n      \n     \
    \ - [ ] **PII inventory complete**: All PII/PHI fields identified and documented\
    \ (RIU-012)\n      - [ ] **PII handling implemented**: Anonymization, masking,\
    \ or encryption in place\n      - [ ] **Legal/security approval**: Explicit sign-off\
    \ for any sensitive data use\n      - [ ] **Access controls configured**: IAM\
    \ policies, fine-grained permissions\n      - [ ] **Audit logging enabled**: CloudTrail\
    \ tracking all data access\n      - [ ] **Vector embeddings secured**: Encryption\
    \ + access controls for RAG systems\n      \n      **SECTION 2: Data Documentation\
    \ (MUST PASS ALL)**\n      \n      - [ ] **Data dictionary exists**: All fields\
    \ used by AI are documented (LIB-035)\n      - [ ] **Data lineage documented**:\
    \ Origin, transformations, dependencies tracked\n      - [ ] **Data catalog entry**:\
    \ Dataset registered in DataZone/Glue Catalog\n      - [ ] **Data owner identified**:\
    \ Clear accountability for each dataset\n      - [ ] **Retention policy defined**:\
    \ How long data is kept, when deleted\n      \n      **SECTION 3: Data Quality\
    \ Rules (MUST PASS ALL)**\n      \n      - [ ] **Schema validation**: All fields\
    \ match expected types/formats\n      - [ ] **Completeness thresholds**: Required\
    \ fields meet minimum fill rates\n        ```\n        Example: customer_id completeness\
    \ >= 99.9%\n        ```\n      - [ ] **Validity rules**: Values within expected\
    \ ranges/enums\n        ```\n        Example: status IN ('active', 'pending',\
    \ 'closed')\n        ```\n      - [ ] **Uniqueness constraints**: Key fields are\
    \ unique where required\n      - [ ] **Referential integrity**: Foreign keys exist\
    \ in referenced tables\n      - [ ] **Semantic validation**: Business rules verified\
    \ (LIB-041)\n      \n      **SECTION 4: Data Quality Baselines (MUST PASS ALL)**\n\
    \      \n      - [ ] **Baseline metrics established**:\n        ```yaml\n    \
    \    baseline:\n          completeness: 98.5%\n          validity: 97.2%\n   \
    \       freshness: \"< 1 hour\"\n          volume: \"45,000-55,000 records/day\"\
    \n        ```\n      - [ ] **Quality thresholds defined**: Warning and critical\
    \ levels set (LIB-040)\n      - [ ] **Monitoring configured**: CloudWatch dashboards\
    \ and alerts\n      - [ ] **Drift detection enabled**: Statistical monitoring\
    \ for distribution shifts\n      \n      **SECTION 5: Data Pipeline Quality (MUST\
    \ PASS ALL)**\n      \n      - [ ] **Quality gates implemented**: Bad data routed\
    \ to rejected layer\n      - [ ] **Quality checks automated**: Run on every pipeline\
    \ execution\n      - [ ] **Quarantine process defined**: How rejected data is\
    \ reviewed/fixed\n      - [ ] **Alerting configured**: Notifications when quality\
    \ drops\n      - [ ] **Recovery procedures documented**: How to reprocess failed\
    \ data\n      \n      **SECTION 6: Evaluation Data (MUST PASS for AI/ML)**\n \
    \     \n      - [ ] **Golden set exists**: Curated evaluation dataset (RIU-021)\n\
    \      - [ ] **Golden set versioned**: Immutable snapshots with version IDs\n\
    \      - [ ] **Label quality validated**: Inter-annotator agreement measured (LIB-036)\n\
    \      - [ ] **Edge cases included**: Boundary conditions and known failures\n\
    \      - [ ] **Distribution representative**: Evaluation data reflects production\
    \ distribution\n      \n      **SECTION 7: Operational Readiness (MUST PASS ALL)**\n\
    \      \n      - [ ] **Data freshness acceptable**: Latency from source meets\
    \ requirements\n      - [ ] **Volume tested**: Pipeline handles expected + 2x\
    \ peak load\n      - [ ] **Failure handling tested**: Pipeline recovers from source\
    \ outages\n      - [ ] **Runbook exists**: Documented procedures for data issues\
    \ (RIU-062)\n      - [ ] **On-call identified**: Clear ownership for data quality\
    \ incidents\n      \n      **Scoring:**\n      ```\n      PASS: All checkboxes\
    \ in section are âœ“ or N/A with documented rationale\n      FAIL: Any checkbox\
    \ unchecked without rationale\n      \n      Production readiness: ALL sections\
    \ must PASS\n      ```\n      \n      **Quick reference thresholds:**\n      |\
    \ Metric | Minimum for Production |\n      |--------|------------------------|\n\
    \      | Schema compliance | 100% |\n      | Required field completeness | 99%+\
    \ |\n      | Validity rate | 95%+ |\n      | Uniqueness (keys) | 100% |\n    \
    \  | Freshness | Per SLA |\n      | Quality check automation | 100% coverage |\n\
    \      \n      **PALETTE integration:**\n      - Document quality rules in RIU-084\
    \ (Data Quality Checks)\n      - Store baselines in RIU-081 (E2E Smoke Tests)\n\
    \      - Track in Deployment Readiness (RIU-060)\n      - Include in go-live gate\
    \ review\n      \n      Key insight: \"Production-ready\" isn't a feeling â€” it's\
    \ this checklist with evidence for each item. If you can't show proof, you're\
    \ not ready."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-012
  - RIU-021
  - RIU-060
  - RIU-062
  - RIU-081
  - RIU-082
  - RIU-083
  - RIU-084
  difficulty: medium
  industries:
  - All
  tags:
  - production-readiness
  - quality-criteria
  - checklist
  - standards
  sources:
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Data governance in the age of generative AI
    url: https://aws.amazon.com/blogs/big-data/data-governance-in-the-age-of-generative-ai/
  - title: 'From raw to refined: building a data quality pipeline with AWS Glue and
      Amazon S3 Tables'
    url: https://aws.amazon.com/blogs/storage/from-raw-to-refined-building-a-data-quality-pipeline-with-aws-glue-and-amazon-s3-tables/
  - title: Implementing data governance on AWS
    url: https://aws.amazon.com/blogs/security/implementing-data-governance-on-aws-automation-tagging-and-lifecycle-strategy-part-1/
- id: LIB-044
  question: How do I handle data quality issues discovered after model deployment?
  answer: "Post-deployment data quality issues require structured incident response:\
    \ Detect â†’ Assess â†’ Contain â†’ Remediate â†’ Prevent. Speed matters â€” bad data compounds\
    \ downstream.\n      \n      **Incident response phases:**\n      \n      **PHASE\
    \ 1: DETECT (Minutes)**\n      Already covered by monitoring (LIB-034, LIB-038):\n\
    \      - SageMaker Model Monitor alerts on data drift\n      - Glue Data Quality\
    \ anomaly detection\n      - CloudWatch alarms on quality metrics\n      - User\
    \ reports / escalations\n      \n      **PHASE 2: ASSESS SEVERITY (< 30 minutes)**\n\
    \      \n      Triage questions:\n      1. **Scope**: How much data is affected?\
    \ (% of records)\n      2. **Impact**: What decisions were made with bad data?\n\
    \      3. **Duration**: How long has this been happening?\n      4. **Reversibility**:\
    \ Can affected outputs be corrected?\n      5. **Visibility**: Have users/customers\
    \ been impacted?\n      \n      Severity classification:\n      | Severity | Criteria\
    \ | Response Time |\n      |----------|----------|---------------|\n      | Critical\
    \ | Customer-facing, financial, or safety impact | Immediate |\n      | High |\
    \ Significant business process affected | < 4 hours |\n      | Medium | Internal\
    \ processes affected, workaround exists | < 24 hours |\n      | Low | Minimal\
    \ impact, cosmetic issues | Next sprint |\n      \n      **PHASE 3: CONTAIN (<\
    \ 1 hour for Critical/High)**\n      \n      ```\n      Decision tree:\n     \
    \ \n      Is bad data still flowing?\n      â”œâ”€â”€ YES â†’ Stop the source\n      â”‚\
    \   â”œâ”€â”€ Pause data pipeline\n      â”‚   â”œâ”€â”€ Quarantine incoming data\n      â”‚ \
    \  â””â”€â”€ Switch to backup data source (if available)\n      â”‚\n      â””â”€â”€ NO â†’ Assess\
    \ blast radius\n          â”œâ”€â”€ Identify all downstream systems affected\n     \
    \     â””â”€â”€ Document affected time range\n      \n      Are AI outputs still being\
    \ served?\n      â”œâ”€â”€ YES, and outputs are dangerous â†’ Rollback model\n      â”‚\
    \   â”œâ”€â”€ Revert to last known good model version\n      â”‚   â””â”€â”€ Enable fallback\
    \ behavior\n      â”‚\n      â””â”€â”€ YES, but outputs are degraded â†’ Consider options\n\
    \          â”œâ”€â”€ Continue with degraded quality (communicate)\n          â”œâ”€â”€ Route\
    \ to human review (A2I)\n          â””â”€â”€ Return error/uncertainty indicator\n  \
    \    ```\n      \n      **PHASE 4: DECIDE - Rollback vs. Fix-Forward**\n     \
    \ \n      | Factor | Favor Rollback | Favor Fix-Forward |\n      |--------|----------------|-------------------|\n\
    \      | User impact | High/visible | Low/internal |\n      | Fix complexity |\
    \ Unknown/complex | Simple/understood |\n      | Time to fix | > 4 hours | < 1\
    \ hour |\n      | Previous version quality | Good | Also degraded |\n      | Business\
    \ criticality | Revenue/safety | Analytics/internal |\n      \n      **Rollback\
    \ procedure:**\n      1. Switch model to previous version (SageMaker endpoint\
    \ update)\n      2. Revert data pipeline to last good state\n      3. Mark affected\
    \ outputs as potentially invalid\n      4. Communicate to stakeholders\n     \
    \ \n      **Fix-forward procedure:**\n      1. Implement data fix (quarantine\
    \ bad, reprocess)\n      2. Test fix in staging\n      3. Deploy to production\n\
    \      4. Validate quality metrics return to baseline\n      5. Reprocess affected\
    \ data if needed\n      \n      **PHASE 5: REMEDIATE (Hours to Days)**\n     \
    \ \n      For affected data:\n      - [ ] Identify all records affected (time\
    \ range, criteria)\n      - [ ] Quarantine or flag affected records\n      - [\
    \ ] Determine if reprocessing is needed\n      - [ ] Reprocess with corrected\
    \ data/model\n      - [ ] Validate outputs against known-good examples\n     \
    \ \n      For affected users/decisions:\n      - [ ] Identify decisions made with\
    \ bad data\n      - [ ] Assess if decisions need to be reversed\n      - [ ] Communicate\
    \ with affected stakeholders\n      - [ ] Document business impact for post-mortem\n\
    \      \n      **PHASE 6: PREVENT (Post-incident)**\n      \n      Root cause\
    \ analysis:\n      ```yaml\n      incident_post_mortem:\n        incident_id:\
    \ \"DQ-2024-001\"\n        summary: \"Training data contained duplicate records\
    \ causing model bias\"\n        \n        timeline:\n          detected: \"2024-06-15\
    \ 14:30\"\n          contained: \"2024-06-15 15:00\"\n          resolved: \"2024-06-15\
    \ 18:00\"\n          \n        root_cause: \"ETL job failure left partial data,\
    \ dedup not run\"\n        \n        impact:\n          records_affected: 50000\n\
    \          users_impacted: 200\n          business_cost: \"$5000 in incorrect\
    \ recommendations\"\n          \n        actions:\n          - action: \"Add dedup\
    \ validation to pipeline\"\n            owner: \"data-team\"\n            due:\
    \ \"2024-06-22\"\n            \n          - action: \"Add monitoring for record\
    \ count anomalies\"\n            owner: \"mlops-team\"\n            due: \"2024-06-20\"\
    \n            \n          - action: \"Update runbook with dedup failure scenario\"\
    \n            owner: \"on-call\"\n            due: \"2024-06-18\"\n      ```\n\
    \      \n      **Runbook template (RIU-062):**\n      ```yaml\n      data_quality_incident_runbook:\n\
    \        detection_sources:\n          - SageMaker Model Monitor alerts\n    \
    \      - CloudWatch quality metric alarms\n          - User escalations\n    \
    \      \n        immediate_actions:\n          - Acknowledge alert, start incident\
    \ channel\n          - Assess severity using triage questions\n          - Notify\
    \ on-call and data owner\n          \n        containment_options:\n         \
    \ - Pause data pipeline: \"[link to procedure]\"\n          - Rollback model:\
    \ \"[link to procedure]\"\n          - Enable fallback: \"[link to procedure]\"\
    \n          \n        escalation_contacts:\n          critical: [\"on-call-primary\"\
    , \"data-owner\", \"product-lead\"]\n          high: [\"on-call-primary\", \"\
    data-owner\"]\n      ```\n      \n      **PALETTE integration:**\n      - Document\
    \ incidents in RIU-100 (Incident Log)\n      - Update runbook in RIU-062 (Incident\
    \ Containment)\n      - Track model versions in RIU-532 (Model Registry)\n   \
    \   - Add failed scenarios to RIU-014 (Edge-Case Catalog)\n      - Update quality\
    \ tests in RIU-084 (Data Quality Checks)\n      \n      Key insight: The goal\
    \ isn't zero data quality issues â€” it's fast detection and contained blast radius.\
    \ Every incident should result in a new test that prevents recurrence."
  problem_type: Data_Semantics_and_Quality
  related_rius:
  - RIU-014
  - RIU-062
  - RIU-081
  - RIU-084
  - RIU-100
  - RIU-532
  difficulty: critical
  industries:
  - All
  tags:
  - incident-response
  - data-quality
  - post-deployment
  - remediation
  sources:
  - title: AWS DevOps Agent helps you accelerate incident response
    url: https://aws.amazon.com/blogs/aws/aws-devops-agent-helps-you-accelerate-incident-response-and-improve-system-reliability-preview/
  - title: Introducing AWS Glue Data Quality anomaly detection
    url: https://aws.amazon.com/blogs/big-data/introducing-aws-glue-data-quality-anomaly-detection/
  - title: Monitoring data quality in third-party models with Amazon SageMaker Model
      Monitor
    url: https://aws.amazon.com/blogs/awsmarketplace/monitoring-data-quality-in-third-party-models-with-amazon-sagemaker-model-monitor/
  - title: Automated monitoring with SageMaker Model Monitor and Amazon A2I
    url: https://aws.amazon.com/blogs/machine-learning/automated-monitoring-of-your-machine-learning-models-with-amazon-sagemaker-model-monitor-and-sending-predictions-to-human-review-workflows-using-amazon-a2i/
- id: LIB-045
  question: How do I design runbooks for AI systems that fail in non-obvious ways?
  answer: "AI systems fail differently than traditional software â€” they degrade silently,\
    \ produce plausible-but-wrong outputs, and fail in ways that look like success.\
    \ Design runbooks around these unique failure modes.\n      \n      **AI-specific\
    \ failure modes (design runbooks for each):**\n      \n      | Failure Mode |\
    \ How It Manifests | Why Non-Obvious |\n      |--------------|------------------|-----------------|\n\
    \      | Silent degradation | Quality drops gradually | No errors, just worse\
    \ outputs |\n      | Hallucination | Confident wrong answers | Looks correct,\
    \ passes validation |\n      | Drift (data/concept) | Model accuracy declines\
    \ | Works for old patterns, fails on new |\n      | Retrieval failure (RAG) |\
    \ Missing or wrong context | Answer is coherent but grounded in wrong data |\n\
    \      | Prompt injection | Unexpected behavior | Malicious input bypasses guardrails\
    \ |\n      | Latency degradation | Slow responses | No errors, just timeout risk\
    \ |\n      | Cost explosion | Token/compute overuse | Functional but unsustainable\
    \ |\n      \n      **Runbook structure for AI systems:**\n      \n      ```yaml\n\
    \      runbook:\n        id: \"AI-RUN-001\"\n        title: \"AI Output Quality\
    \ Degradation\"\n        failure_mode: \"silent_degradation\"\n        \n    \
    \    # How to detect this failure\n        detection:\n          signals:\n  \
    \          - \"Quality score drops below threshold (current: X, threshold: Y)\"\
    \n            - \"User feedback rate increases (thumbs down > 10%)\"\n       \
    \     - \"Confidence scores skewing low\"\n            - \"Regeneration request\
    \ rate increasing\"\n          monitoring:\n            - \"CloudWatch alarm:\
    \ ai-quality-score-low\"\n            - \"Dashboard: ai-ops/quality-metrics\"\n\
    \        \n        # Severity assessment\n        triage:\n          questions:\n\
    \            - \"What % of outputs are affected?\"\n            - \"Are affected\
    \ outputs customer-facing?\"\n            - \"Is there a pattern (time, input\
    \ type, user segment)?\"\n            - \"When did metrics start degrading?\"\n\
    \          severity_matrix:\n            critical: \">20% affected AND customer-facing\"\
    \n            high: \">10% affected OR customer-facing\"\n            medium:\
    \ \"<10% affected AND internal\"\n        \n        # Root cause investigation\n\
    \        diagnosis:\n          step_1_prompt_orchestration:\n            check:\
    \ \"Has prompt template changed recently?\"\n            action: \"Compare current\
    \ vs. last-known-good prompt version\"\n            tool: \"Prompt registry diff\
    \ (RIU-520)\"\n            \n          step_2_knowledge_retrieval:\n         \
    \   check: \"Is RAG returning relevant context?\"\n            action: \"Sample\
    \ 10 failed queries, inspect retrieved chunks\"\n            tool: \"RAG evaluation\
    \ dashboard\"\n            \n          step_3_data_drift:\n            check:\
    \ \"Has input distribution changed?\"\n            action: \"Compare recent inputs\
    \ to training distribution\"\n            tool: \"SageMaker Model Monitor drift\
    \ report\"\n            \n          step_4_model_issues:\n            check: \"\
    Is foundation model behaving differently?\"\n            action: \"Run golden\
    \ set evaluation, compare to baseline\"\n            tool: \"Bedrock Evaluations\"\
    \n        \n        # Remediation options\n        remediation:\n          immediate_containment:\n\
    \            - action: \"Route low-confidence outputs to human review\"\n    \
    \          command: \"Enable A2I workflow for confidence < 0.7\"\n           \
    \   \n            - action: \"Increase output validation strictness\"\n      \
    \        command: \"Set guardrail threshold to HIGH\"\n              \n      \
    \      - action: \"Rollback to previous model/prompt version\"\n             \
    \ command: \"sagemaker update-endpoint --version v1.2.2\"\n              requires_approval:\
    \ true\n          \n          fix_forward:\n            prompt_issue:\n      \
    \        - \"Revert prompt to last-known-good version\"\n              - \"Test\
    \ fix in staging with golden set\"\n              - \"Deploy with canary rollout\"\
    \n              \n            retrieval_issue:\n              - \"Identify missing/incorrect\
    \ knowledge base content\"\n              - \"Update knowledge base\"\n      \
    \        - \"Re-index and validate retrieval quality\"\n              \n     \
    \       drift_issue:\n              - \"Collect recent production samples\"\n\
    \              - \"Add to training/fine-tuning dataset\"\n              - \"Trigger\
    \ retraining pipeline\"\n        \n        # Escalation\n        escalation:\n\
    \          on_call: \"@ai-ops-oncall\"\n          data_owner: \"@data-team-lead\"\
    \n          model_owner: \"@ml-platform-lead\"\n          escalate_to_leadership_if:\
    \ \"Customer-facing impact > 1 hour\"\n        \n        # Post-incident\n   \
    \     post_incident:\n          - \"Add failed examples to golden set (RIU-021)\"\
    \n          - \"Update drift detection thresholds\"\n          - \"Document in\
    \ incident log (RIU-100)\"\n          - \"Schedule post-mortem within 48 hours\"\
    \n      ```\n      \n      **Failure source diagnostic tree:**\n      ```\n  \
    \    AI output is wrong/degraded\n      â”‚\n      â”œâ”€ Check: Did prompt/orchestration\
    \ change?\n      â”‚  â””â”€ YES â†’ Revert prompt, compare outputs\n      â”‚\n      â”œâ”€\
    \ Check: Is retrieved context relevant? (RAG)\n      â”‚  â””â”€ NO â†’ Knowledge base\
    \ issue â†’ Update/re-index\n      â”‚\n      â”œâ”€ Check: Has input distribution shifted?\n\
    \      â”‚  â””â”€ YES â†’ Data drift â†’ Retrain or adapt\n      â”‚\n      â””â”€ Check: Is\
    \ model itself degraded?\n         â””â”€ YES â†’ Model issue â†’ Rollback or switch models\n\
    \      ```\n      \n      **Non-obvious failure detection techniques:**\n    \
    \  \n      | Technique | Detects | Implementation |\n      |-----------|---------|----------------|\n\
    \      | Golden set regression | Quality drop | Run nightly, alert on score drop\
    \ |\n      | User feedback correlation | Silent failures | Track thumbs-down patterns\
    \ |\n      | Confidence score monitoring | Uncertainty increase | Alert when avg\
    \ confidence drops |\n      | Output length anomalies | Prompt issues | Alert\
    \ on unusual response lengths |\n      | Latency percentile tracking | Performance\
    \ degradation | Alert on p99 increase |\n      | Cost per request monitoring |\
    \ Efficiency issues | Alert on token usage spikes |\n      \n      **Key runbook\
    \ design principles for AI:**\n      \n      1. **Assume failure is silent**:\
    \ Include proactive checks, not just error handling\n      2. **Include golden\
    \ set validation**: \"Is the system still working?\" test\n      3. **Trace from\
    \ output to input**: Use request IDs to investigate full context\n      4. **Have\
    \ rollback ready**: Know how to revert to last-known-good state\n      5. **Include\
    \ human escalation**: AI failures often need human judgment\n      6. **Document\
    \ what \"normal\" looks like**: Can't detect anomaly without baseline\n      \n\
    \      **PALETTE integration:**\n      - Store runbooks in RIU-062 (Incident Containment\
    \ Playbook)\n      - Track incidents in RIU-100 (Incident Log)\n      - Link to\
    \ RIU-069 (Runbook) for operational procedures\n      - Update RIU-014 (Edge-Case\
    \ Catalog) with new failure patterns\n      \n      Key insight: Traditional runbooks\
    \ assume failures are loud (errors, crashes). AI runbooks must assume failures\
    \ are quiet (wrong outputs, degraded quality). Design detection into the runbook,\
    \ not just response."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-014
  - RIU-021
  - RIU-062
  - RIU-069
  - RIU-100
  - RIU-101
  - RIU-102
  difficulty: critical
  industries:
  - All
  tags:
  - runbooks
  - incident-response
  - failure-handling
  - operations
  sources:
  - title: Build resilient generative AI agents
    url: https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Create a Generative AI runbook to resolve security findings
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/943dd78a-d351-49bc-ae84-1b1a25edff7b
  - title: Build resilient generative AI agents
    url: https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/
  - title: 'Planning for failure: How to make generative AI workloads more resilient'
    url: https://aws.amazon.com/blogs/publicsector/planning-for-failure-how-to-make-generative-ai-workloads-more-resilient/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: AI Ops Overview - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/index.html
- id: LIB-047
  question: How do I reduce MTTR when AI failures require domain expert diagnosis?
  answer: "MTTR for expert-dependent failures has three components: Time to Engage\
    \ Expert + Expert Investigation Time + Fix Implementation Time. Attack all three.\n\
    \      \n      **MTTR breakdown for AI incidents:**\n      ```\n      Total MTTR\
    \ = Detection â†’ Triage â†’ Engage Expert â†’ Investigate â†’ Fix â†’ Verify\n        \
    \           ~~~~~~~~   ~~~~~~   ~~~~~~~~~~~~~   ~~~~~~~~~~~   ~~~   ~~~~~~\n \
    \                  Automated  L1 team  BOTTLENECK      BOTTLENECK    Team  Automated\n\
    \      ```\n      \n      **Strategy 1: Reduce Time to Engage Expert**\n     \
    \ \n      **Clear escalation criteria (know WHEN to escalate):**\n      ```yaml\n\
    \      escalation_matrix:\n        l1_can_handle:\n          - \"Known issues\
    \ with documented runbook\"\n          - \"Infrastructure failures (restart, failover)\"\
    \n          - \"Rate limiting / quota issues\"\n          \n        escalate_to_domain_expert:\n\
    \          - \"Quality degradation without obvious cause\"\n          - \"Novel\
    \ failure mode not in runbook\"\n          - \"Business logic questions about\
    \ AI behavior\"\n          - \"Model output correctness disputes\"\n         \
    \ \n        escalate_immediately:\n          - \"Customer-facing impact > 15 minutes\"\
    \n          - \"Data quality issue affecting training\"\n          - \"Potential\
    \ compliance/safety concern\"\n      ```\n      \n      **On-call rotation for\
    \ domain experts:**\n      - Primary: ML engineer (model/prompt issues)\n    \
    \  - Secondary: Data engineer (data/retrieval issues)\n      - Tertiary: Domain\
    \ SME (business logic questions)\n      - Rotation: Weekly, with handoff documentation\n\
    \      \n      **Contact mechanisms:**\n      - PagerDuty/Opsgenie integration\n\
    \      - Slack channel with @mention\n      - Backup mobile contacts\n      -\
    \ Time-zone coverage map\n      \n      **Strategy 2: Reduce Expert Investigation\
    \ Time**\n      \n      **Pre-computed diagnostics (have answers ready before\
    \ expert arrives):**\n      ```yaml\n      incident_package:\n        # Auto-collected\
    \ when alert fires\n        basic_info:\n          - Alert name and trigger condition\n\
    \          - Time range of issue\n          - Affected endpoints/services\n  \
    \        \n        model_diagnostics:\n          - Recent prompt/model version\
    \ changes\n          - Quality score trend (last 24 hours)\n          - Sample\
    \ of affected outputs (5-10 examples)\n          - Confidence score distribution\n\
    \          \n        data_diagnostics:\n          - Data drift report (last 7\
    \ days)\n          - RAG retrieval samples for affected queries\n          - Knowledge\
    \ base last update time\n          \n        system_diagnostics:\n          -\
    \ Error rates and types\n          - Latency percentiles\n          - Token usage\
    \ patterns\n          - Resource utilization\n          \n        context:\n \
    \         - Similar past incidents (from knowledge base)\n          - Recent deployments/changes\n\
    \          - Relevant runbook links\n      ```\n      \n      **AI-assisted investigation:**\n\
    \      - **AWS DevOps Agent**: Automated evidence gathering, root cause suggestions\n\
    \      - **Amazon Q Business**: Query documentation, past incidents, operational\
    \ data\n      - **RAG-based telemetry search**: \"Show me similar failures in\
    \ the last month\"\n      \n      **Strategy 3: Reduce Fix Implementation Time**\n\
    \      \n      **Pre-approved remediation actions:**\n      ```yaml\n      pre_approved_actions:\n\
    \        - action: \"Rollback to previous model version\"\n          approval:\
    \ \"Pre-approved for quality score < 80%\"\n          command: \"invoke-rollback\
    \ --version previous\"\n          \n        - action: \"Increase guardrail strictness\"\
    \n          approval: \"Pre-approved\"\n          command: \"set-guardrail --level\
    \ high\"\n          \n        - action: \"Route to human review\"\n          approval:\
    \ \"Pre-approved\"\n          command: \"enable-a2i --confidence-threshold 0.7\"\
    \n          \n        - action: \"Retrain model\"\n          approval: \"Requires\
    \ expert sign-off\"\n          command: \"trigger-retraining-pipeline\"\n    \
    \  ```\n      \n      **Strategy 4: Reduce Future Expert Dependency**\n      \n\
    \      **Knowledge capture from every incident:**\n      ```yaml\n      post_incident_capture:\n\
    \        - root_cause: \"RAG retrieval returning outdated documents\"\n      \
    \  - diagnosis_steps: |\n            1. Checked prompt version - no changes\n\
    \            2. Sampled retrieval results - found stale docs\n            3. Verified\
    \ KB update pipeline - failed silently 2 days ago\n        - fix_applied: \"Restarted\
    \ KB sync, added monitoring for sync failures\"\n        - runbook_update: \"\
    Added 'check KB sync status' to quality degradation runbook\"\n        - automation_opportunity:\
    \ \"Add CloudWatch alarm for KB sync failures\"\n      ```\n      \n      **Build\
    \ hierarchical knowledge base:**\n      - Store incident â†’ diagnosis â†’ fix mappings\n\
    \      - Enable semantic search: \"What caused quality drops before?\"\n     \
    \ - Feed learnings into automated triage\n      - Track which issues L1 can now\
    \ handle independently\n      \n      **Metrics to track MTTR improvement:**\n\
    \      | Metric | Target | How to Improve |\n      |--------|--------|----------------|\n\
    \      | Time to engage expert | < 15 min | Clear escalation criteria, fast paging\
    \ |\n      | Expert investigation time | < 30 min | Pre-computed diagnostics,\
    \ AI assistance |\n      | Fix implementation time | < 30 min | Pre-approved actions,\
    \ automation |\n      | % incidents requiring expert | Decreasing | Knowledge\
    \ capture, runbook updates |\n      \n      **PALETTE integration:**\n      -\
    \ Document escalation criteria in RIU-102 (Escalation Matrix)\n      - Store diagnostic\
    \ package spec in RIU-069 (Runbook)\n      - Track incidents in RIU-100 (Incident\
    \ Log) with root cause\n      - Capture learnings in RIU-101 (Failure Mode Catalog)\n\
    \      \n      Key insight: The goal isn't to eliminate expert involvement â€” it's\
    \ to maximize expert efficiency. When they arrive, they should have context, options,\
    \ and pre-approval to act. Every incident should make the next one faster."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-069
  - RIU-100
  - RIU-101
  - RIU-102
  difficulty: critical
  industries:
  - All
  tags:
  - mttr
  - incident-response
  - expert-escalation
  - operations
  sources:
  - title: Reducing Mean Time to Repair (MTTR) with Amazon Q Business
    url: https://aws.amazon.com/blogs/industries/reducing-mttr-with-amazon-q-business/
  - title: AWS DevOps Agent helps you accelerate incident response
    url: https://aws.amazon.com/blogs/aws/aws-devops-agent-helps-you-accelerate-incident-response-and-improve-system-reliability-preview/
  - title: Accelerate investigations with AWS Security Incident Response AI-powered
      capabilities
    url: https://aws.amazon.com/blogs/security/accelerate-investigations-with-aws-security-incident-response-ai-powered-capabilities/
  - title: Methodology for incident response on generative AI workloads
    url: https://aws.amazon.com/blogs/security/methodology-for-incident-response-on-generative-ai-workloads/
- id: LIB-048
  question: What monitoring alerts actually predict AI system failures vs noise?
  answer: "Most AI alerts are noise. Focus on leading indicators that predict failures\
    \ before user impact, not lagging indicators that confirm failures already happened.\n\
    \      \n      **The alert quality problem:**\n      - Security teams face 3,000+\
    \ daily alerts, 60-90% uninvestigated\n      - Alert fatigue leads to ignored\
    \ critical signals\n      - AI systems generate MORE noise (non-deterministic,\
    \ gradual degradation)\n      \n      **Alert classification: Signal vs. Noise**\n\
    \      \n      | Alert Type | Signal (Actionable) | Noise (Ignore/Tune) |\n  \
    \    |------------|---------------------|---------------------|\n      | Error\
    \ rate spike | âœ… >2x baseline in 5 min | âŒ Minor fluctuation |\n      | Latency\
    \ increase | âœ… p99 >3x baseline | âŒ p50 within normal |\n      | Quality score\
    \ drop | âœ… Sustained >10% drop | âŒ Single low-score response |\n      | Token\
    \ usage spike | âœ… >2x sustained | âŒ Brief spike (one request) |\n      | User\
    \ feedback | âœ… Negative trend over hours | âŒ Individual complaint |\n      | Cost\
    \ threshold | âœ… Projected to exceed budget | âŒ Within 10% of baseline |\n    \
    \  \n      **Leading indicators (predict failures):**\n      \n      | Indicator\
    \ | What It Predicts | Alert Threshold |\n      |-----------|------------------|-----------------|\n\
    \      | **p99 latency trending up** | Timeout failures imminent | >1.5x baseline\
    \ sustained 10min |\n      | **Token usage increasing** | Cost explosion / prompt\
    \ issues | >2x baseline sustained |\n      | **Confidence scores dropping** |\
    \ Quality degradation | Avg confidence <0.7 |\n      | **Retry rate increasing**\
    \ | Transient failures becoming persistent | >5% of requests |\n      | **Queue\
    \ depth growing** | Processing falling behind | >2x normal depth |\n      | **Embedding\
    \ distance increasing** | Data drift / semantic shift | Cosine similarity <0.9\
    \ |\n      | **Guardrail trigger rate up** | Input quality degrading | >5% of\
    \ inputs blocked |\n      \n      **Lagging indicators (confirm failures â€” still\
    \ useful but reactive):**\n      - Error count\n      - User complaints\n    \
    \  - Escalation rate\n      - Failed evaluations\n      \n      **Alert prioritization\
    \ framework:**\n      \n      ```yaml\n      alert_tiers:\n        critical:\n\
    \          criteria:\n            - \"Customer-facing error rate > 5%\"\n    \
    \        - \"Security/compliance breach detected\"\n            - \"Complete service\
    \ outage\"\n          response: \"Immediate page, drop everything\"\n        \
    \  \n        high:\n          criteria:\n            - \"Quality score < 70% sustained\
    \ 15 min\"\n            - \"p99 latency > 3x baseline\"\n            - \"Cost\
    \ projection > 150% of budget\"\n          response: \"Page on-call within 15\
    \ min\"\n          \n        warning:\n          criteria:\n            - \"Leading\
    \ indicators trending negative\"\n            - \"Quality score dropped 10%\"\n\
    \            - \"Anomaly detected but not confirmed\"\n          response: \"\
    Review in next 4 hours, investigate trend\"\n          \n        info:\n     \
    \     criteria:\n            - \"Minor fluctuations within expected range\"\n\
    \            - \"Single instance anomalies\"\n          response: \"Log for context,\
    \ no immediate action\"\n      ```\n      \n      **Noise reduction techniques:**\n\
    \      \n      1. **Use anomaly detection, not static thresholds**\n         ```\n\
    \         # Bad: Static threshold\n         ALARM: latency > 500ms\n         \n\
    \         # Good: Anomaly band\n         ALARM: latency > ANOMALY_DETECTION_BAND(latency,\
    \ 2)\n         ```\n      \n      2. **Require sustained violations**\n      \
    \   ```\n         # Bad: Alert on single data point\n         ALARM IF error_rate\
    \ > 5%\n         \n         # Good: Require sustained violation\n         ALARM\
    \ IF error_rate > 5% FOR 3 consecutive minutes\n         ```\n      \n      3.\
    \ **Correlate related metrics**\n         - Don't alert on latency AND errors\
    \ separately\n         - Alert on combined signal: \"latency high AND error rate\
    \ rising\"\n      \n      4. **Suppress during known events**\n         - Deployment\
    \ windows\n         - Scheduled maintenance\n         - Expected traffic spikes\n\
    \      \n      5. **Auto-resolve transient alerts**\n         - If condition clears\
    \ within 5 minutes, log but don't page\n      \n      **Effective CloudWatch alarm\
    \ patterns:**\n      \n      ```yaml\n      # Leading indicator: Latency trending\
    \ up\n      - alarm_name: \"AI-LatencyTrending\"\n        metric: \"InvocationLatency\"\
    \n        statistic: \"p99\"\n        threshold: \"ANOMALY_DETECTION_BAND(2)\"\
    \n        period: 300\n        evaluation_periods: 3\n        treat_missing: \"\
    notBreaching\"\n        \n      # Leading indicator: Token usage spike\n     \
    \ - alarm_name: \"AI-TokenUsageSpike\"\n        metric: \"InputTokenCount + OutputTokenCount\"\
    \n        statistic: \"Sum\"\n        threshold: \"> 2x 7-day average\"\n    \
    \    period: 300\n        evaluation_periods: 2\n        \n      # Composite:\
    \ Multiple signals\n      - alarm_name: \"AI-QualityDegradation-Composite\"\n\
    \        type: \"composite\"\n        rule: \"AI-LatencyTrending AND AI-ConfidenceLow\"\
    \n        description: \"Multiple quality signals degrading together\"\n     \
    \ ```\n      \n      **Metrics that are usually noise:**\n      - Individual request\
    \ failures (expected in distributed systems)\n      - Brief latency spikes (often\
    \ just cold starts)\n      - Single user complaints (without pattern)\n      -\
    \ Minor fluctuations in token usage\n      - Scheduled job completion variations\n\
    \      \n      **PALETTE integration:**\n      - Define alert tiers in RIU-061\
    \ (Observability Baseline)\n      - Document leading indicators in RIU-063 (Performance\
    \ Baselines)\n      - Track alert effectiveness in RIU-100 (Incident Log) â€” were\
    \ alerts useful?\n      - Tune thresholds based on false positive rate\n     \
    \ \n      Key insight: A good alert is one that, when it fires, you always investigate\
    \ and usually find a real problem. If you're ignoring alerts, you have too many\
    \ or wrong thresholds. Track false positive rate and tune ruthlessly."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-061
  - RIU-063
  - RIU-100
  - RIU-532
  - RIU-533
  difficulty: high
  industries:
  - All
  tags:
  - monitoring
  - alerting
  - signal-vs-noise
  - observability
  sources:
  - title: Application Performance Monitoring for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_4_scalability_performance/3_4_1_application_runtime_optimization/3_4_1_1_application_performance/3_4_1_1_2_application_performance_monitoring.html
  - title: Application Observability for GenAI Systems
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_1_system_and_application_design_patterns_for_genai/3_1_1_foundation_architecture_components/3_1_1_7_application_observability/index.html
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
  - title: Build resilient generative AI agents
    url: https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/
- id: LIB-049
  question: How do I handle cascading failures in multi-model AI pipelines?
  answer: "Cascading failures occur when one component's failure propagates to dependent\
    \ components. In multi-model pipelines, this is especially dangerous â€” Model A's\
    \ timeout can exhaust Model B's connection pool, which crashes Model C. Design\
    \ for isolation, not just redundancy.\n      \n      **Cascade failure patterns\
    \ in AI pipelines:**\n      ```\n      Model A fails (timeout)\n           â†“\n\
    \      Model B retries exhaustively (no backoff)\n           â†“\n      Model B\
    \ exhausts connection pool / hits rate limit\n           â†“\n      Model C waiting\
    \ on B times out\n           â†“\n      Entire pipeline fails\n           â†“\n  \
    \    Users retry â†’ amplifies load â†’ system collapse\n      ```\n      \n     \
    \ **Defense-in-depth architecture:**\n      ```\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \                    â”‚           LOAD SHEDDING                 â”‚\n           \
    \         â”‚    (reject excess requests early)       â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \                                      â†“\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \                    â”‚           BULKHEADS                     â”‚\n           \
    \         â”‚    (isolate resources per model)        â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \                                      â†“\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚   Model A    â”‚    â”‚   Model B    â”‚    â”‚   Model\
    \ C    â”‚\n      â”‚   + Circuit  â”‚    â”‚   + Circuit  â”‚    â”‚   + Circuit  â”‚\n   \
    \   â”‚   Breaker    â”‚    â”‚   Breaker    â”‚    â”‚   Breaker    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                  \
    \    â†“\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    \
    \                â”‚        GRACEFUL DEGRADATION             â”‚\n               \
    \     â”‚    (fallback when models fail)          â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Pattern 1: Circuit Breakers (stop calling failing\
    \ models)**\n      \n      ```yaml\n      circuit_breaker_config:\n        model_a:\n\
    \          failure_threshold: 5        # Open after 5 failures\n          success_threshold:\
    \ 3        # Close after 3 successes\n          timeout_seconds: 30         #\
    \ Half-open test interval\n          \n          states:\n            closed:\
    \ \"Normal operation, requests pass through\"\n            open: \"Failures detected,\
    \ requests fail fast (don't call model)\"\n            half_open: \"Testing if\
    \ model recovered\"\n      ```\n      \n      **AWS Implementation (Lambda + DynamoDB):**\n\
    \      ```python\n      # Lambda extension checks circuit status before calling\
    \ model\n      def check_circuit(model_name):\n          status = dynamodb.get_item(Key={'model':\
    \ model_name})\n          if status['state'] == 'OPEN':\n              if time.now()\
    \ > status['retry_after']:\n                  return 'HALF_OPEN'  # Try one request\n\
    \              raise CircuitOpenError(\"Model unavailable\")\n          return\
    \ 'CLOSED'\n      \n      def call_model_with_circuit_breaker(model_name, input):\n\
    \          state = check_circuit(model_name)\n          try:\n              result\
    \ = invoke_model(model_name, input)\n              if state == 'HALF_OPEN':\n\
    \                  record_success(model_name)  # Close circuit\n             \
    \ return result\n          except Exception:\n              record_failure(model_name)\n\
    \              raise\n      ```\n      \n      **Pattern 2: Bulkheads (isolate\
    \ resources per model)**\n      \n      ```yaml\n      bulkhead_config:\n    \
    \    model_a:\n          max_concurrent_requests: 50\n          connection_pool_size:\
    \ 20\n          queue_size: 100\n          \n        model_b:\n          max_concurrent_requests:\
    \ 30\n          connection_pool_size: 15\n          queue_size: 50\n         \
    \ \n        # Model A exhausting resources won't affect Model B\n      ```\n \
    \     \n      **AWS Implementation:**\n      - Separate Lambda functions per model\
    \ (isolated concurrency)\n      - Separate SQS queues per pipeline stage\n   \
    \   - App Mesh for EKS workloads with per-model resource limits\n      \n    \
    \  **Pattern 3: Timeouts (fail fast, don't wait forever)**\n      \n      ```yaml\n\
    \      timeout_strategy:\n        # Cascading timeouts: each stage shorter than\
    \ previous\n        api_gateway: 29s    # API Gateway max\n        orchestrator:\
    \ 25s   # Total pipeline budget\n        model_a: 10s        # Individual model\
    \ budgets\n        model_b: 8s         # Leave headroom for retry\n        model_c:\
    \ 5s\n        \n        # If Model A times out at 10s, we still have 15s for fallback\n\
    \      ```\n      \n      **Pattern 4: Load Shedding (reject excess early)**\n\
    \      \n      ```yaml\n      load_shedding:\n        triggers:\n          - queue_depth\
    \ > 1000\n          - error_rate > 10%\n          - latency_p99 > 5s\n       \
    \   \n        actions:\n          - reject_new_requests: true\n          - return_code:\
    \ 503\n          - message: \"System overloaded, retry in 30 seconds\"\n     \
    \     - preserve_capacity_for: \"in-flight requests\"\n      ```\n      \n   \
    \   **Pattern 5: Graceful Degradation (fallback chain)**\n      \n      ```yaml\n\
    \      degradation_strategy:\n        model_a_failure:\n          fallback_1:\
    \ \"Use smaller/faster model (reduced quality)\"\n          fallback_2: \"Return\
    \ cached response if fresh enough\"\n          fallback_3: \"Route to human review\"\
    \n          fallback_4: \"Return error with retry guidance\"\n          \n   \
    \     rag_failure:\n          fallback_1: \"Answer without retrieval (warn user)\"\
    \n          fallback_2: \"Return 'I don't have enough context'\"\n          \n\
    \        full_pipeline_failure:\n          action: \"Queue request for later processing\"\
    \n          notify_user: \"Your request is queued, ETA: 30 minutes\"\n      ```\n\
    \      \n      **Step Functions orchestration with resilience:**\n      ```yaml\n\
    \      # Step Functions state machine with circuit breakers\n      States:\n \
    \       CheckModelACircuit:\n          Type: Choice\n          Choices:\n    \
    \        - Variable: \"$.circuitStatus\"\n              StringEquals: \"OPEN\"\
    \n              Next: ModelAFallback\n          Default: InvokeModelA\n      \
    \    \n        InvokeModelA:\n          Type: Task\n          Resource: \"arn:aws:lambda:...:invoke-model-a\"\
    \n          Retry:\n            - ErrorEquals: [\"TransientError\"]\n        \
    \      IntervalSeconds: 2\n              MaxAttempts: 3\n              BackoffRate:\
    \ 2\n          Catch:\n            - ErrorEquals: [\"States.ALL\"]\n         \
    \     Next: RecordModelAFailure\n              \n        RecordModelAFailure:\n\
    \          Type: Task\n          Resource: \"arn:aws:lambda:...:update-circuit-breaker\"\
    \n          Next: ModelAFallback\n      ```\n      \n      **Testing cascading\
    \ failure resilience:**\n      - Use AWS Fault Injection Simulator to inject failures\n\
    \      - Test each circuit breaker opens correctly\n      - Verify bulkheads isolate\
    \ failures\n      - Confirm fallbacks activate appropriately\n      - Load test\
    \ to find actual breaking points\n      \n      **PALETTE integration:**\n   \
    \   - Document failure modes in RIU-101 (Failure Mode Catalog)\n      - Define\
    \ circuit breaker configs in RIU-063 (Performance Baselines)\n      - Include\
    \ fallback procedures in RIU-069 (Runbook)\n      - Track cascade incidents in\
    \ RIU-100 (Incident Log)\n      \n      Key insight: The goal isn't preventing\
    \ all failures â€” it's containing blast radius. A well-designed pipeline degrades\
    \ gracefully: one model failing shouldn't take down the whole system."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-063
  - RIU-069
  - RIU-100
  - RIU-101
  difficulty: critical
  industries:
  - AI/ML
  - All
  tags:
  - cascading-failures
  - pipeline-reliability
  - fault-isolation
  - architecture
  sources:
  - title: Build resilient generative AI agents
    url: https://aws.amazon.com/blogs/architecture/build-resilient-generative-ai-agents/
  - title: Using the circuit-breaker pattern with AWS Lambda extensions and Amazon
      DynamoDB
    url: https://aws.amazon.com/blogs/compute/using-the-circuit-breaker-pattern-with-aws-lambda-extensions-and-amazon-dynamodb/
  - title: Using the circuit breaker pattern with AWS Step Functions and Amazon DynamoDB
    url: https://aws.amazon.com/blogs/compute/using-the-circuit-breaker-pattern-with-aws-step-functions-and-amazon-dynamodb/
  - title: Building a fault tolerant architecture with a Bulkhead Pattern on AWS App
      Mesh
    url: https://aws.amazon.com/blogs/containers/building-a-fault-tolerant-architecture-with-a-bulkhead-pattern-on-aws-app-mesh/
  - title: 'Planning for failure: How to make generative AI workloads more resilient'
    url: https://aws.amazon.com/blogs/publicsector/planning-for-failure-how-to-make-generative-ai-workloads-more-resilient/
- id: LIB-050
  question: What's the best fallback strategy when AI confidence drops below threshold?
  answer: "Low confidence means the AI doesn't know if it's right. The fallback strategy\
    \ depends on the stakes: low-stakes can fail gracefully, high-stakes need human\
    \ review.\n      \n      **Confidence threshold framework:**\n      \n      ```\n\
    \      Confidence Score\n      â”‚\n      â”œâ”€â”€ HIGH (>0.85): Proceed automatically\n\
    \      â”‚\n      â”œâ”€â”€ MEDIUM (0.65-0.85): Proceed with caveats\n      â”‚   â””â”€â”€ Flag\
    \ for async review, include uncertainty indicator\n      â”‚\n      â”œâ”€â”€ LOW (0.40-0.65):\
    \ Fallback required\n      â”‚   â””â”€â”€ Human review, alternative model, or graceful\
    \ decline\n      â”‚\n      â””â”€â”€ VERY LOW (<0.40): Decline to answer\n          â””â”€â”€\
    \ \"I'm not confident enough to answer this\"\n      ```\n      \n      **Fallback\
    \ decision tree:**\n      \n      ```\n      AI confidence < threshold\n     \
    \ â”‚\n      â”œâ”€ Is this a high-stakes decision?\n      â”‚  â”œâ”€ YES â†’ Route to human\
    \ review (HITL)\n      â”‚  â”‚        â”œâ”€ Approval-based: Human approves/rejects\n\
    \      â”‚  â”‚        â””â”€ Review-and-edit: Human modifies output\n      â”‚  â”‚\n   \
    \   â”‚  â””â”€ NO â†’ Try fallback chain\n      â”‚           â”œâ”€ Step 1: Retry with different\
    \ prompt\n      â”‚           â”œâ”€ Step 2: Try alternative model\n      â”‚        \
    \   â”œâ”€ Step 3: Return partial answer with caveat\n      â”‚           â””â”€ Step 4:\
    \ Graceful decline\n      â”‚\n      â””â”€ Has user asked for confirmation?\n     \
    \    â”œâ”€ YES â†’ Provide answer with explicit uncertainty\n         â””â”€ NO â†’ Follow\
    \ decision tree above\n      ```\n      \n      **Fallback options (ordered by\
    \ preference):**\n      \n      | Priority | Fallback | When to Use | User Impact\
    \ |\n      |----------|----------|-------------|-------------|\n      | 1 | Retry\
    \ with refined prompt | Confidence borderline | Minimal delay |\n      | 2 | Alternative\
    \ model | Primary model uncertain | Slightly different output |\n      | 3 | Cached/similar\
    \ response | Similar query answered before | Fast, may be stale |\n      | 4 |\
    \ Partial answer + caveat | Can answer partially | Useful but incomplete |\n \
    \     | 5 | Human review (async) | Non-urgent, quality critical | Delayed response\
    \ |\n      | 6 | Human review (sync) | Urgent, high-stakes | Wait for human |\n\
    \      | 7 | Graceful decline | Cannot help | Clear \"I don't know\" |\n     \
    \ \n      **HITL patterns for low confidence (Amazon A2I):**\n      \n      ```yaml\n\
    \      hitl_patterns:\n        approval_based:\n          trigger: \"confidence\
    \ < 0.65 AND action.is_irreversible\"\n          flow: \"AI generates â†’ Human\
    \ approves/rejects â†’ Execute or discard\"\n          use_case: \"Financial decisions,\
    \ compliance actions\"\n          \n        review_and_edit:\n          trigger:\
    \ \"confidence < 0.75 AND output.is_customer_facing\"\n          flow: \"AI generates\
    \ â†’ Human edits â†’ Publish modified\"\n          use_case: \"Content creation,\
    \ customer communications\"\n          \n        escalation_based:\n         \
    \ trigger: \"confidence < 0.50 OR user.requests_human\"\n          flow: \"AI\
    \ attempts â†’ Fails threshold â†’ Handoff to human agent\"\n          use_case: \"\
    Customer support, complex queries\"\n          \n        feedback_loop:\n    \
    \      trigger: \"all outputs\" (background)\n          flow: \"AI generates â†’\
    \ User interacts â†’ Feedback captured â†’ Model improves\"\n          use_case: \"\
    Continuous improvement, collaborative workflows\"\n      ```\n      \n      **Implementation\
    \ with confidence thresholds:**\n      \n      ```python\n      def handle_ai_response(response,\
    \ context):\n          confidence = response.confidence_score\n          \n  \
    \        # High confidence: proceed automatically\n          if confidence >=\
    \ 0.85:\n              return AIResult(response.output, status=\"auto_approved\"\
    )\n          \n          # Medium confidence: proceed with caveat\n          elif\
    \ confidence >= 0.65:\n              return AIResult(\n                  response.output,\n\
    \                  status=\"uncertain\",\n                  caveat=\"This response\
    \ may need verification\",\n                  flag_for_review=True\n         \
    \     )\n          \n          # Low confidence: fallback chain\n          elif\
    \ confidence >= 0.40:\n              # Try alternative model\n              alt_response\
    \ = try_alternative_model(context)\n              if alt_response.confidence >=\
    \ 0.65:\n                  return handle_ai_response(alt_response, context)\n\
    \              \n              # Route to human if high-stakes\n             \
    \ if context.is_high_stakes:\n                  return route_to_human_review(context,\
    \ response)\n              \n              # Return partial with strong caveat\n\
    \              return AIResult(\n                  response.output,\n        \
    \          status=\"low_confidence\",\n                  caveat=\"I'm not very\
    \ confident in this answer\"\n              )\n          \n          # Very low\
    \ confidence: decline\n          else:\n              return AIResult(\n     \
    \             output=None,\n                  status=\"declined\",\n         \
    \         message=\"I don't have enough information to answer this confidently\"\
    \n              )\n      ```\n      \n      **User communication during fallback:**\n\
    \      \n      | Confidence Level | What to Tell User |\n      |------------------|-------------------|\n\
    \      | Medium | \"Here's my answer, but you may want to verify...\" |\n    \
    \  | Low | \"I'm not very confident. Here's my best guess...\" |\n      | Very\
    \ Low | \"I don't have enough information to answer this.\" |\n      | Human Review\
    \ | \"This needs human review. Expected response time: X\" |\n      | Fallback\
    \ Model | No indication needed (transparent to user) |\n      \n      **Learning\
    \ from low-confidence cases:**\n      \n      ```yaml\n      low_confidence_logging:\n\
    \        capture:\n          - input_query\n          - confidence_score\n   \
    \       - fallback_path_taken\n          - human_feedback (if HITL)\n        \
    \  - final_outcome\n          \n        analysis:\n          - \"Which query types\
    \ trigger low confidence?\"\n          - \"Does alternative model perform better?\"\
    \n          - \"What did humans do differently?\"\n          \n        improvement:\n\
    \          - Add low-confidence queries to evaluation set\n          - Fine-tune\
    \ on human-corrected examples\n          - Update prompts to handle common low-confidence\
    \ patterns\n      ```\n      \n      **Setting confidence thresholds:**\n    \
    \  \n      | Factor | Lower Threshold | Higher Threshold |\n      |--------|-----------------|------------------|\n\
    \      | High stakes | | âœ… |\n      | Customer-facing | | âœ… |\n      | Reversible\
    \ action | âœ… | |\n      | Internal only | âœ… | |\n      | Time-sensitive | âœ… (prefer\
    \ speed) | |\n      | Compliance-related | | âœ… |\n      \n      **PALETTE integration:**\n\
    \      - Document threshold settings in RIU-500 (Prompt/Model Config)\n      -\
    \ Configure HITL workflows in RIU-513 (Human Approval for ONE-WAY DOORs)\n   \
    \   - Track low-confidence patterns in RIU-101 (Failure Mode Catalog)\n      -\
    \ Log fallback events in RIU-100 (Incident Log)\n      \n      Key insight: \"\
    I don't know\" is a valid and valuable AI output. A system that confidently gives\
    \ wrong answers is worse than one that admits uncertainty. Design fallbacks that\
    \ preserve user trust."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-100
  - RIU-101
  - RIU-500
  - RIU-513
  difficulty: high
  industries:
  - All
  tags:
  - fallback-strategies
  - confidence-thresholds
  - graceful-degradation
  - reliability
  sources:
  - title: Human-in-the-Loop for GenAI Systems
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_1_system_and_application_design_patterns_for_genai/3_1_1_foundation_architecture_components/3_1_1_8_additional_components/3_1_1_8_1_human_in_the_loop/3_1_1_8_1_human_in_the_loop.html
  - title: Building serverless architectures for agentic AI on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/agentic-ai-serverless/introduction.html
- id: LIB-051
  question: How do I design escalation paths for AI failures with unclear ownership?
  answer: "Unclear ownership is the #1 cause of slow incident response. Design escalation\
    \ paths with default owners, clear handoff criteria, and a decision tree for ambiguous\
    \ cases.\n      \n      **The ownership ambiguity problem in AI:**\n      ```\n\
    \      AI system fails\n      â”‚\n      â””â”€ Who owns this?\n         â”œâ”€ Data Team:\
    \ \"It's a model problem\"\n         â”œâ”€ ML Team: \"It's a data problem\"\n   \
    \      â”œâ”€ Platform Team: \"It's an application problem\"\n         â”œâ”€ Application\
    \ Team: \"It's an infrastructure problem\"\n         â””â”€ Everyone: \"Not my job\"\
    \n         \n      â†’ Incident unowned â†’ MTTR explodes\n      ```\n      \n   \
    \   **Ownership model for AI systems:**\n      \n      | Component | Primary Owner\
    \ | Secondary Owner | Escalation To |\n      |-----------|---------------|-----------------|---------------|\n\
    \      | Prompts/Templates | ML/AI Team | Application Team | AI Governance Lead\
    \ |\n      | Model Performance | ML/AI Team | Data Team | AI Governance Lead |\n\
    \      | Training Data | Data Team | ML/AI Team | Data Governance Lead |\n   \
    \   | RAG/Knowledge Base | Data Team | ML/AI Team | Data Governance Lead |\n \
    \     | Infrastructure | Platform Team | Application Team | Engineering Lead |\n\
    \      | API/Integration | Application Team | Platform Team | Engineering Lead\
    \ |\n      | Business Logic | Application Team | Product Team | Product Lead |\n\
    \      | Compliance/Safety | AI Governance | Legal/Compliance | Executive Sponsor\
    \ |\n      \n      **Default owner rule (when unclear):**\n      ```yaml\n   \
    \   default_ownership:\n        rule: \"The team that receives the first alert\
    \ owns initial triage\"\n        timeout: \"If no root cause identified in 30\
    \ minutes, escalate to AI Governance Lead\"\n        exception: \"Customer-facing\
    \ issues default to Application Team\"\n      ```\n      \n      **Escalation\
    \ matrix (RIU-102):**\n      \n      ```yaml\n      escalation_matrix:\n     \
    \   tier_1_initial_response:\n          who: \"On-call engineer (receiving team)\"\
    \n          actions:\n            - Acknowledge alert within 15 minutes\n    \
    \        - Initial triage: model vs. system failure\n            - Engage relevant\
    \ team if ownership clear\n          escalate_if:\n            - \"Cannot determine\
    \ ownership in 15 minutes\"\n            - \"Multiple teams potentially responsible\"\
    \n            - \"Customer impact confirmed\"\n            \n        tier_2_cross_functional:\n\
    \          who: \"AI Governance Lead + relevant team leads\"\n          actions:\n\
    \            - Convene war room (Slack channel, video call)\n            - Assign\
    \ incident commander\n            - Parallel investigation by suspected teams\n\
    \          escalate_if:\n            - \"No root cause in 1 hour\"\n         \
    \   - \"Business impact exceeds threshold\"\n            - \"Regulatory/compliance\
    \ concern\"\n            \n        tier_3_executive:\n          who: \"Executive\
    \ Sponsor + Department Heads\"\n          actions:\n            - Resource allocation\
    \ decisions\n            - External communication approval\n            - Business\
    \ continuity decisions\n          trigger:\n            - \"Major customer impact\
    \ > 2 hours\"\n            - \"Compliance/legal exposure\"\n            - \"Cross-departmental\
    \ conflict\"\n      ```\n      \n      **Decision tree for unclear ownership:**\n\
    \      \n      ```\n      AI failure detected\n      â”‚\n      â”œâ”€ Is it returning\
    \ errors/not responding?\n      â”‚  â””â”€ YES â†’ Platform/Infrastructure Team first\n\
    \      â”‚\n      â”œâ”€ Is it returning wrong/poor quality outputs?\n      â”‚  â”œâ”€ Did\
    \ prompt/template change recently?\n      â”‚  â”‚  â””â”€ YES â†’ ML/AI Team\n      â”‚ \
    \ â”œâ”€ Did training data change recently?\n      â”‚  â”‚  â””â”€ YES â†’ Data Team\n    \
    \  â”‚  â”œâ”€ Did RAG knowledge base change?\n      â”‚  â”‚  â””â”€ YES â†’ Data Team\n    \
    \  â”‚  â””â”€ No recent changes?\n      â”‚     â””â”€ ML/AI Team (model drift suspected)\n\
    \      â”‚\n      â”œâ”€ Is it affecting specific users/use cases?\n      â”‚  â””â”€ YES\
    \ â†’ Application Team first (then ML if needed)\n      â”‚\n      â””â”€ Still unclear?\n\
    \         â””â”€ Invoke cross-functional triage (Tier 2)\n      ```\n      \n    \
    \  **Incident commander model:**\n      \n      ```yaml\n      incident_commander:\n\
    \        role: \"Single point of accountability during incident\"\n        selection:\n\
    \          - default: \"Most senior on-call from most likely owning team\"\n \
    \         - unclear: \"AI Governance Lead assigns commander\"\n          \n  \
    \      responsibilities:\n          - Coordinate investigation across teams\n\
    \          - Make ownership decisions\n          - Communicate status to stakeholders\n\
    \          - Document actions and decisions\n          - Declare incident resolved\n\
    \          \n        authority:\n          - Can assign tasks to any team\n  \
    \        - Can escalate without permission\n          - Can request additional\
    \ resources\n      ```\n      \n      **Cross-functional war room protocol:**\n\
    \      \n      ```yaml\n      war_room_protocol:\n        activation: \"Any Tier\
    \ 2 escalation\"\n        \n        setup:\n          - Create Slack channel:\
    \ #incident-YYYY-MM-DD-description\n          - Start video bridge (optional but\
    \ recommended)\n          - Add representatives from: ML, Data, Platform, Application\n\
    \          \n        structure:\n          - Incident Commander leads\n      \
    \    - Each team reports findings every 15 minutes\n          - Shared document\
    \ for timeline and actions\n          - Clear handoff when ownership determined\n\
    \          \n        closure:\n          - Root cause owner identified\n     \
    \     - Remediation owner assigned\n          - Post-incident review scheduled\n\
    \      ```\n      \n      **Preventing unclear ownership (proactive):**\n    \
    \  \n      1. **Document ownership in advance**\n         - Component â†’ Team mapping\
    \ in runbook\n         - Review and update quarterly\n      \n      2. **Blameless\
    \ post-mortems**\n         - Assign ownership for future similar incidents\n \
    \        - Update escalation matrix based on learnings\n      \n      3. **Joint\
    \ on-call rotations**\n         - AI-specific on-call that spans teams\n     \
    \    - Train on cross-component triage\n      \n      4. **Shared dashboards**\n\
    \         - Single view of model + data + infra health\n         - Reduces \"\
    not my problem\" responses\n      \n      **PALETTE integration:**\n      - Define\
    \ ownership matrix in RIU-102 (Escalation Matrix)\n      - Document roles in RIU-042\
    \ (RACI/Stakeholder Map)\n      - Track incidents by owner in RIU-100 (Incident\
    \ Log)\n      - Update based on post-mortems\n      \n      Key insight: In AI\
    \ systems, most failures cross team boundaries. Design for collaboration, not\
    \ blame. The escalation path should answer \"who coordinates?\" not just \"who\
    \ fixes?\""
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  
  - RIU-069
  - RIU-100
  - RIU-102
  difficulty: high
  industries:
  - All
  tags:
  - escalation
  - ownership
  - incident-management
  - operations
  sources:
  - title: Organizational Design and Team Structure for AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_2_organizational_design_team_structure.html
  - title: Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/4_0_systematic_path_to_production_framework/4_4_governance/index.html
  - title: AWS DevOps Agent helps you accelerate incident response
    url: https://aws.amazon.com/blogs/aws/aws-devops-agent-helps-you-accelerate-incident-response-and-improve-system-reliability-preview/
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
- id: LIB-052
  question: What post-mortem format captures AI failure root causes effectively?
  answer: "AI post-mortems need standard incident structure PLUS AI-specific sections\
    \ for model, data, and prompt analysis. Use AWS COE (Correction of Errors) format\
    \ as base, extend for AI.\n      \n      **AI-enhanced post-mortem template:**\n\
    \      \n      ```yaml\n      post_mortem:\n        # SECTION 1: HEADER\n    \
    \    metadata:\n          incident_id: \"AI-INC-2024-042\"\n          title: \"\
    RAG system returning outdated information\"\n          severity: \"High\"\n  \
    \        date_occurred: \"2024-06-15\"\n          date_resolved: \"2024-06-15\"\
    \n          date_post_mortem: \"2024-06-18\"\n          author: \"Jane Smith\"\
    \n          reviewers: [\"ML Lead\", \"Data Lead\", \"On-call Engineer\"]\n  \
    \        status: \"Complete\"\n          \n        # SECTION 2: EXECUTIVE SUMMARY\n\
    \        summary: |\n          The RAG-based customer support AI returned outdated\
    \ product pricing\n          for 3 hours, affecting approximately 500 customer\
    \ interactions.\n          Root cause: Knowledge base sync pipeline failed silently\
    \ 2 days prior.\n          \n        # SECTION 3: IMPACT ASSESSMENT\n        impact:\n\
    \          duration: \"3 hours 15 minutes\"\n          users_affected: 500\n \
    \         customer_facing: true\n          financial_impact: \"~$2,500 in incorrect\
    \ quotes\"\n          reputational_impact: \"12 customer complaints\"\n      \
    \    compliance_impact: \"None\"\n          \n        # SECTION 4: TIMELINE\n\
    \        timeline:\n          - time: \"2024-06-13 02:00\"\n            event:\
    \ \"KB sync pipeline fails (silent failure)\"\n            who: \"System\"\n \
    \           \n          - time: \"2024-06-15 09:00\"\n            event: \"Customer\
    \ reports incorrect pricing\"\n            who: \"Support team\"\n           \
    \ \n          - time: \"2024-06-15 09:15\"\n            event: \"Alert acknowledged,\
    \ investigation started\"\n            who: \"On-call engineer\"\n           \
    \ \n          - time: \"2024-06-15 09:45\"\n            event: \"Root cause identified:\
    \ stale KB data\"\n            who: \"Data team\"\n            \n          - time:\
    \ \"2024-06-15 10:30\"\n            event: \"KB manually refreshed\"\n       \
    \     who: \"Data team\"\n            \n          - time: \"2024-06-15 12:15\"\
    \n            event: \"Incident resolved, monitoring confirmed\"\n           \
    \ who: \"On-call engineer\"\n            \n        # SECTION 5: AI-SPECIFIC ROOT\
    \ CAUSE ANALYSIS\n        ai_root_cause_analysis:\n          # Check each AI failure\
    \ category\n          prompt_orchestration:\n            investigated: true\n\
    \            findings: \"No prompt changes in past 7 days\"\n            was_cause:\
    \ false\n            \n          knowledge_retrieval:\n            investigated:\
    \ true\n            findings: |\n              - KB sync pipeline failed on 2024-06-13\n\
    \              - No alert configured for sync failures\n              - RAG was\
    \ retrieving 2-day-old pricing data\n              - Retrieval quality scores\
    \ were normal (misleading)\n            was_cause: true\n            \n      \
    \    model_behavior:\n            investigated: true\n            findings: \"\
    Model performed correctly with available context\"\n            was_cause: false\n\
    \            \n          data_quality:\n            investigated: true\n     \
    \       findings: \"Source data was correct; pipeline failure prevented update\"\
    \n            was_cause: false\n            \n          infrastructure:\n    \
    \        investigated: true\n            findings: \"All services healthy; no\
    \ latency or error spikes\"\n            was_cause: false\n            \n    \
    \    # SECTION 6: FIVE WHYS ANALYSIS\n        five_whys:\n          why_1:\n \
    \           question: \"Why did customers receive incorrect pricing?\"\n     \
    \       answer: \"RAG retrieved outdated pricing information\"\n            \n\
    \          why_2:\n            question: \"Why was the pricing information outdated?\"\
    \n            answer: \"Knowledge base hadn't been updated in 2 days\"\n     \
    \       \n          why_3:\n            question: \"Why hadn't the KB been updated?\"\
    \n            answer: \"Sync pipeline failed on June 13\"\n            \n    \
    \      why_4:\n            question: \"Why didn't we know the pipeline failed?\"\
    \n            answer: \"No alerting configured for sync pipeline failures\"\n\
    \            \n          why_5:\n            question: \"Why wasn't alerting configured?\"\
    \n            answer: \"Pipeline was added quickly without full observability\"\
    \n            \n          root_cause: |\n            Missing observability for\
    \ KB sync pipeline allowed silent failure.\n            Retrieval quality metrics\
    \ didn't detect staleness because the \n            retrieved content was still\
    \ \"relevant\" â€” just outdated.\n            \n        # SECTION 7: CONTRIBUTING\
    \ FACTORS\n        contributing_factors:\n          - factor: \"No freshness check\
    \ on retrieved content\"\n            type: \"System design\"\n            \n\
    \          - factor: \"Quality metrics didn't catch staleness\"\n            type:\
    \ \"Monitoring gap\"\n            \n          - factor: \"Quick deployment without\
    \ full observability\"\n            type: \"Process gap\"\n            \n    \
    \    # SECTION 8: WHAT WENT WELL\n        what_went_well:\n          - \"Fast\
    \ identification of RAG vs. model issue\"\n          - \"Data team responded quickly\
    \ once engaged\"\n          - \"Manual KB refresh was straightforward\"\n    \
    \      - \"Customer communication was prompt\"\n          \n        # SECTION\
    \ 9: WHAT COULD BE IMPROVED\n        what_could_be_improved:\n          - \"Should\
    \ have detected sync failure automatically\"\n          - \"Retrieval metrics\
    \ should include freshness\"\n          - \"Need runbook for KB staleness scenarios\"\
    \n          \n        # SECTION 10: ACTION ITEMS\n        action_items:\n    \
    \      - id: \"AI-042-001\"\n            action: \"Add CloudWatch alarm for KB\
    \ sync pipeline failures\"\n            owner: \"Data Team\"\n            priority:\
    \ \"P1\"\n            due_date: \"2024-06-22\"\n            status: \"In Progress\"\
    \n            \n          - id: \"AI-042-002\"\n            action: \"Add document\
    \ freshness check to retrieval pipeline\"\n            owner: \"ML Team\"\n  \
    \          priority: \"P1\"\n            due_date: \"2024-06-25\"\n          \
    \  status: \"Not Started\"\n            \n          - id: \"AI-042-003\"\n   \
    \         action: \"Create runbook for KB staleness incidents\"\n            owner:\
    \ \"On-call rotation\"\n            priority: \"P2\"\n            due_date: \"\
    2024-06-20\"\n            status: \"Complete\"\n            \n          - id:\
    \ \"AI-042-004\"\n            action: \"Add KB sync scenario to golden set evaluation\"\
    \n            owner: \"ML Team\"\n            priority: \"P2\"\n            due_date:\
    \ \"2024-06-29\"\n            status: \"Not Started\"\n            \n        #\
    \ SECTION 11: LESSONS LEARNED\n        lessons_learned:\n          - lesson: \"\
    Retrieval 'quality' â‰  retrieval 'correctness' or 'freshness'\"\n            applies_to:\
    \ \"All RAG systems\"\n            \n          - lesson: \"Silent pipeline failures\
    \ are worse than loud failures\"\n            applies_to: \"All data pipelines\"\
    \n            \n        # SECTION 12: RELATED INCIDENTS\n        related_incidents:\n\
    \          - \"AI-INC-2024-028: Similar KB sync issue in staging\"\n      ```\n\
    \      \n      **AI-specific sections explained:**\n      \n      | Section |\
    \ Why Needed for AI |\n      |---------|-------------------|\n      | AI Root\
    \ Cause Analysis | Standard RCA misses prompt/model/data causes |\n      | Knowledge\
    \ Retrieval Check | RAG failures look like model failures |\n      | Prompt Orchestration\
    \ Check | Template changes cause subtle bugs |\n      | Data Quality Check | Training/inference\
    \ data issues |\n      | Five Whys (AI-adapted) | Traces through AI pipeline layers\
    \ |\n      \n      **Blameless post-mortem principles:**\n      - Focus on systems,\
    \ not people\n      - \"How did our systems allow this?\" not \"Who caused this?\"\
    \n      - Share widely to maximize learning\n      - Celebrate finding and fixing\
    \ issues\n      \n      **Action item tracking:**\n      ```yaml\n      action_tracking:\n\
    \        review_cadence: \"Weekly until all P1 complete\"\n        escalation:\
    \ \"Unstarted P1 after 7 days â†’ escalate to lead\"\n        verification: \"Each\
    \ action requires proof of completion\"\n        metrics:\n          - \"Time\
    \ to complete P1 actions\"\n          - \"% of actions completed on time\"\n \
    \         - \"Recurrence rate of similar incidents\"\n      ```\n      \n    \
    \  **PALETTE integration:**\n      - Store post-mortems in RIU-100 (Incident Log)\n\
    \      - Update RIU-101 (Failure Mode Catalog) with new patterns\n      - Add\
    \ to RIU-014 (Edge-Case Catalog) for testing\n      - Track actions in RIU-102\
    \ (Escalation Matrix review)\n      \n      Key insight: AI post-mortems must\
    \ answer \"Which part of the AI pipeline failed?\" â€” not just \"What failed?\"\
    \ The 3-way check (prompt/knowledge/model) prevents misattribution and ensures\
    \ the right fix."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-004
  - RIU-014
  - RIU-100
  - RIU-101
  - RIU-102
  difficulty: medium
  industries:
  - All
  tags:
  - post-mortems
  - root-cause-analysis
  - documentation
  - learning
  sources:
  - title: Creating a correction of errors document
    url: https://aws.amazon.com/blogs/mt/creating-a-correction-of-errors-document/
  - title: Why you should develop a correction of error (COE)
    url: https://aws.amazon.com/blogs/mt/why-you-should-develop-a-correction-of-error-coe/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
- id: LIB-053
  question: How do I test failure scenarios that only happen in production?
  answer: "Production-only failures (scale, timing, real data patterns) require controlled\
    \ chaos engineering. Use fault injection to create failures safely, shadow testing\
    \ to observe without impact, and traffic replay to reproduce issues.\n      \n\
    \      **Why some failures only happen in production:**\n      - Scale effects\
    \ (concurrency, rate limits, resource exhaustion)\n      - Real data patterns\
    \ (edge cases not in test data)\n      - Timing issues (race conditions, timeouts\
    \ under load)\n      - Integration failures (third-party services, network)\n\
    \      - User behavior (unexpected inputs, usage patterns)\n      \n      **Testing\
    \ strategy pyramid:**\n      ```\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \                    â”‚  PRODUCTION CHAOS                   â”‚\n               \
    \     â”‚  (Fault injection in prod)          â”‚\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \                    â”‚  SHADOW TESTING                     â”‚\n               \
    \     â”‚  (Observe prod traffic, no impact)  â”‚\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \                    â”‚  STAGING CHAOS                      â”‚\n               \
    \     â”‚  (Fault injection in staging)       â”‚\n                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \                    â”‚  LOAD TESTING                       â”‚\n               \
    \     â”‚  (Production-like scale)            â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \                    â”‚  INTEGRATION TESTING                â”‚\n               \
    \     â”‚  (Component interactions)           â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Tool 1: AWS Fault Injection Simulator (FIS)**\n  \
    \    \n      Inject controlled failures into AWS resources:\n      ```yaml\n \
    \     fis_experiment:\n        name: \"AI-Pipeline-Latency-Test\"\n        description:\
    \ \"Test AI pipeline resilience to model latency\"\n        \n        targets:\n\
    \          - name: \"ai-inference-lambda\"\n            resource_type: \"aws:lambda:function\"\
    \n            selection_mode: \"ALL\"\n            \n        actions:\n      \
    \    - name: \"inject-latency\"\n            action_id: \"aws:lambda:function:invocation-add-delay\"\
    \n            parameters:\n              duration: \"PT5M\"  # 5 minutes\n   \
    \           delay_millis: \"3000\"  # 3 second delay\n            targets: [\"\
    ai-inference-lambda\"]\n            \n        stop_conditions:\n          - source:\
    \ \"aws:cloudwatch:alarm\"\n            value: \"arn:aws:cloudwatch:...:alarm:AI-Error-Rate-Critical\"\
    \n            \n        role_arn: \"arn:aws:iam::...:role/FISRole\"\n      ```\n\
    \      \n      **AI-specific failure scenarios to test:**\n      \n      | Scenario\
    \ | FIS Action | What You Learn |\n      |----------|------------|----------------|\n\
    \      | Model latency spike | Lambda delay injection | Timeout handling, fallbacks\
    \ |\n      | Model unavailable | Lambda invocation error | Circuit breaker activation\
    \ |\n      | Rate limiting | Throttle API calls | Queue behavior, retry logic\
    \ |\n      | RAG retrieval failure | Network disruption | Fallback to non-RAG\
    \ response |\n      | Vector DB latency | EBS I/O pause | Timeout configuration\
    \ |\n      | Memory pressure | Resource constraints | Graceful degradation |\n\
    \      | Region failover | AZ/region disruption | Cross-region resilience |\n\
    \      \n      **Tool 2: Shadow Testing (SageMaker)**\n      \n      Compare new\
    \ model against production without user impact:\n      ```yaml\n      shadow_test:\n\
    \        production_variant: \"model-v1.2.3\"\n        shadow_variant: \"model-v1.3.0\"\
    \n        traffic_to_shadow: \"100%\"  # All traffic mirrored\n        duration:\
    \ \"7 days\"\n        \n        comparison_metrics:\n          - latency_p99\n\
    \          - error_rate\n          - output_quality_score\n          \n      \
    \  promotion_criteria:\n          - \"shadow.latency_p99 <= production.latency_p99\
    \ * 1.1\"\n          - \"shadow.error_rate <= production.error_rate\"\n      \
    \    - \"shadow.quality_score >= production.quality_score * 0.95\"\n      ```\n\
    \      \n      **Tool 3: Traffic Replay**\n      \n      Reproduce production\
    \ issues in staging:\n      ```yaml\n      traffic_replay:\n        source: \"\
    s3://logs/api-requests/2024-06-15/\"\n        target: \"staging-endpoint\"\n \
    \       \n        filters:\n          - \"status_code >= 500\"  # Replay only\
    \ errors\n          - \"latency > 5000\"      # Replay slow requests\n       \
    \   \n        transformation:\n          - anonymize_pii: true\n          - sample_rate:\
    \ 0.1  # 10% of matching requests\n      ```\n      \n      **5-step chaos experiment\
    \ process:**\n      \n      ```\n      1. DEFINE STEADY STATE\n         â””â”€ \"\
    Error rate < 1%, latency p99 < 500ms, quality score > 85%\"\n         \n     \
    \ 2. FORM HYPOTHESIS\n         â””â”€ \"If model latency increases 3x, circuit breaker\
    \ activates\n             and fallback model serves requests within 1 minute\"\
    \n             \n      3. INJECT FAILURE\n         â””â”€ Run FIS experiment with\
    \ Lambda delay injection\n         \n      4. OBSERVE BEHAVIOR\n         â””â”€ Monitor\
    \ dashboards, verify hypothesis\n         â””â”€ Did circuit breaker open? Did fallback\
    \ activate?\n         \n      5. IMPROVE & DOCUMENT\n         â””â”€ If hypothesis\
    \ failed: fix the gap\n         â””â”€ If passed: document as validated resilience\n\
    \      ```\n      \n      **Safe production chaos (guardrails):**\n      \n  \
    \    ```yaml\n      safety_guardrails:\n        stop_conditions:\n          -\
    \ \"Error rate > 5%\"\n          - \"Customer complaints received\"\n        \
    \  - \"On-call manually stops experiment\"\n          \n        blast_radius_limits:\n\
    \          - \"Affect max 10% of traffic\"\n          - \"Duration max 15 minutes\"\
    \n          - \"Single AZ only (not region-wide)\"\n          \n        timing:\n\
    \          - \"Run during low-traffic hours\"\n          - \"Avoid during deployments\"\
    \n          - \"Have rollback ready\"\n          \n        communication:\n  \
    \        - \"Notify on-call before experiment\"\n          - \"Post in #ops channel\"\
    \n          - \"Have incident commander available\"\n      ```\n      \n     \
    \ **CI/CD integration (automate chaos):**\n      \n      ```yaml\n      # CodePipeline\
    \ with FIS\n      pipeline:\n        stages:\n          - name: \"Deploy\"\n \
    \           actions: [deploy_to_staging]\n            \n          - name: \"ChaosTest\"\
    \n            actions:\n              - run_fis_experiment: \"latency-test\"\n\
    \              - run_fis_experiment: \"failure-test\"\n              - validate_recovery\n\
    \              \n          - name: \"PromoteOrRollback\"\n            actions:\n\
    \              - if_chaos_passed: promote_to_prod\n              - else: rollback_and_alert\n\
    \      ```\n      \n      **Reproducing production-only bugs:**\n      \n    \
    \  | Bug Type | Reproduction Strategy |\n      |----------|----------------------|\n\
    \      | Scale issues | Load test with production traffic volume |\n      | Edge\
    \ case inputs | Replay production requests that caused errors |\n      | Timing\
    \ bugs | FIS delay injection at various points |\n      | Integration failures\
    \ | Mock third-party with FIS network disruption |\n      | Data patterns | Shadow\
    \ test with production data |\n      \n      **PALETTE integration:**\n      -\
    \ Document failure scenarios in RIU-101 (Failure Mode Catalog)\n      - Track\
    \ chaos experiments in RIU-540 (Evaluation Harness)\n      - Update runbooks based\
    \ on findings (RIU-069)\n      - Log results in RIU-100 (Incident Log) as \"proactive\
    \ tests\"\n      \n      Key insight: If you haven't tested a failure mode, you\
    \ haven't proven resilience â€” you're just hoping. Chaos engineering converts unknown-unknowns\
    \ into known-knowns before they become incidents."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-069
  - RIU-100
  - RIU-101
  - RIU-540
  difficulty: critical
  industries:
  - All
  tags:
  - chaos-engineering
  - production-testing
  - failure-injection
  - reliability
  sources:
  - title: Verify the resilience of your workloads using Chaos Engineering
    url: https://aws.amazon.com/blogs/architecture/verify-the-resilience-of-your-workloads-using-chaos-engineering/
  - title: 'Generative AI Resilience: Chaos Engineering with AWS Fault Injection Service
      Workshop'
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/d56fd754-5e56-43c5-addc-d69ac130a099
  - title: Introducing AWS Fault Injection Service Actions to Inject Chaos in Lambda
      functions
    url: https://aws.amazon.com/blogs/mt/introducing-aws-fault-injection-service-actions-to-inject-chaos-in-lambda-functions/
  - title: Minimize the production impact of ML model updates with Amazon SageMaker
      shadow testing
    url: https://aws.amazon.com/blogs/machine-learning/minimize-the-production-impact-of-ml-model-updates-with-amazon-sagemaker-shadow-testing/
  - title: GitHub - awslabs/chaos-machine
    url: https://github.com/awslabs/chaos-machine
- id: LIB-054
  question: What's the checklist for 'production-ready' AI reliability?
  answer: "Use this checklist before declaring an AI system production-ready. All\
    \ sections must PASS or have documented exceptions approved by the AI Governance\
    \ Lead.\n      \n      **SECTION 1: ARCHITECTURE RELIABILITY (Must pass all)**\n\
    \      \n      - [ ] **High availability configured**\n        - Multi-AZ deployment\
    \ for stateful components\n        - Cross-region capability for critical workloads\n\
    \        - No single points of failure identified\n        \n      - [ ] **Redundancy\
    \ implemented**\n        - Fallback model configured (alternative provider/model)\n\
    \        - Cross-region inference profiles (Bedrock) or multi-endpoint (SageMaker)\n\
    \        - RAG fallback to non-retrieval response\n        \n      - [ ] **Scaling\
    \ configured**\n        - Auto-scaling policies defined and tested\n        -\
    \ Quota headroom validated (>50% buffer recommended)\n        - Load tested at\
    \ 2x expected peak\n        \n      - [ ] **State management**\n        - Conversation/session\
    \ state persisted (DynamoDB)\n        - Cache layer for performance (ElastiCache)\n\
    \        - State recovery tested after restart\n        \n      **SECTION 2: FAILURE\
    \ HANDLING (Must pass all)**\n      \n      - [ ] **Circuit breakers implemented**\n\
    \        - Per-model circuit breaker configured\n        - Failure thresholds\
    \ defined\n        - Fallback behavior tested\n        \n      - [ ] **Retry logic**\n\
    \        - Exponential backoff with jitter\n        - Max retry limits set\n \
    \       - Idempotency implemented for state-changing operations\n        \n  \
    \    - [ ] **Timeout configuration**\n        - Explicit timeouts at every integration\
    \ point\n        - Cascading timeout budget (each stage < total)\n        - Timeout\
    \ handling tested\n        \n      - [ ] **Graceful degradation**\n        - Fallback\
    \ chain defined (LIB-050)\n        - Human escalation path configured\n      \
    \  - \"I don't know\" responses enabled for low confidence\n        \n      **SECTION\
    \ 3: OBSERVABILITY (Must pass all)**\n      \n      - [ ] **Metrics configured**\n\
    \        ```\n        Required metrics:\n        - Latency: p50, p95, p99, TTFT,\
    \ TPOT\n        - Throughput: RPM, TPM\n        - Errors: Error rate, error types\n\
    \        - Quality: Confidence scores, guardrail triggers\n        - Cost: Per-request\
    \ cost, daily spend\n        - Resources: CPU, memory, GPU utilization\n     \
    \   ```\n        \n      - [ ] **Logging implemented**\n        - Structured logs\
    \ with consistent schema\n        - Trace IDs for request correlation\n      \
    \  - PII redaction in logs\n        - Log retention policy defined\n        \n\
    \      - [ ] **Distributed tracing**\n        - End-to-end trace through AI pipeline\n\
    \        - X-Ray or OpenTelemetry configured\n        - Trace sampling rate appropriate\n\
    \        \n      - [ ] **Dashboards created**\n        - Real-time operations\
    \ dashboard\n        - Quality metrics dashboard\n        - Cost dashboard\n \
    \       - Alert status visible\n        \n      - [ ] **Alerting configured**\n\
    \        - Leading indicator alerts (LIB-048)\n        - Severity tiers defined\
    \ (Critical/High/Warning/Info)\n        - Escalation paths configured\n      \
    \  - On-call rotation documented\n        \n      **SECTION 4: OPERATIONAL READINESS\
    \ (Must pass all)**\n      \n      - [ ] **Runbooks created**\n        - Incident\
    \ response runbook (LIB-045)\n        - Common failure scenarios documented\n\
    \        - Escalation matrix defined (LIB-051)\n        - Rollback procedures\
    \ tested\n        \n      - [ ] **On-call established**\n        - Primary and\
    \ secondary on-call assigned\n        - Escalation contacts documented\n     \
    \   - Paging configured and tested\n        - Handoff procedures defined\n   \
    \     \n      - [ ] **Deployment process**\n        - CI/CD pipeline implemented\n\
    \        - Canary/blue-green deployment configured\n        - Rollback automation\
    \ tested\n        - Change approval process defined\n        \n      - [ ] **Documentation\
    \ complete**\n        - Architecture diagram current\n        - API documentation\
    \ published\n        - Dependency map maintained\n        - Contact information\
    \ current\n        \n      **SECTION 5: TESTING COMPLETED (Must pass all)**\n\
    \      \n      - [ ] **Functional testing**\n        - Golden set evaluation passing\
    \ (>baseline)\n        - Edge cases tested\n        - Negative tests (bad inputs)\
    \ passing\n        \n      - [ ] **Integration testing**\n        - All integrations\
    \ verified\n        - Contract tests passing\n        - Error handling tested\n\
    \        \n      - [ ] **Performance testing**\n        - Load test at 2x peak\
    \ completed\n        - Latency SLOs met under load\n        - No resource exhaustion\n\
    \        \n      - [ ] **Chaos testing**\n        - Failure injection completed\
    \ (LIB-053)\n        - Recovery validated\n        - Fallbacks verified\n    \
    \    \n      - [ ] **Shadow/canary completed**\n        - Shadow test with production\
    \ traffic\n        - Metrics compared to baseline\n        - No regressions identified\n\
    \        \n      **SECTION 6: SAFETY & COMPLIANCE (Must pass all)**\n      \n\
    \      - [ ] **Guardrails configured**\n        - Content filters enabled\n  \
    \      - Sensitive information filters active\n        - Denied topics configured\n\
    \        - Guardrail effectiveness tested\n        \n      - [ ] **Security validated**\n\
    \        - IAM least privilege verified\n        - Network security configured\n\
    \        - Secrets management implemented\n        - Security review completed\n\
    \        \n      - [ ] **Compliance verified**\n        - Regulatory requirements\
    \ documented\n        - PII handling compliant\n        - Audit logging enabled\n\
    \        - Data retention compliant\n        \n      **SECTION 7: SLOs DEFINED\
    \ (Must pass all)**\n      \n      - [ ] **SLOs documented**\n        ```yaml\n\
    \        slos:\n          availability: 99.9%\n          latency_p99: 2000ms\n\
    \          error_rate: <0.1%\n          quality_score: >85%\n          cost_per_request:\
    \ <$0.05\n        ```\n        \n      - [ ] **SLO monitoring configured**\n \
    \       - Burn rate alerts set\n        - Error budget tracking enabled\n    \
    \    - SLO dashboard created\n        \n      - [ ] **SLO review process**\n \
    \       - Weekly SLO review scheduled\n        - Escalation for SLO breach defined\n\
    \        \n      **SCORING:**\n      ```\n      Each section: PASS = All items\
    \ checked or N/A with approval\n      \n      Production readiness:\n      - All\
    \ 7 sections PASS â†’ Ready for production\n      - Any section FAIL â†’ Not production-ready\n\
    \      \n      Approval required:\n      - Engineering Lead: Sections 1-5\n  \
    \    - AI Governance Lead: Section 6\n      - Product Lead: Section 7 (SLOs)\n\
    \      ```\n      \n      **Quick reference thresholds:**\n      \n      | Requirement\
    \ | Minimum for Production |\n      |-------------|------------------------|\n\
    \      | Availability | 99.9% (or per SLO) |\n      | Latency p99 | < 3x baseline\
    \ |\n      | Error rate | < 1% |\n      | Quality score | > 80% |\n      | Load\
    \ test | 2x peak traffic |\n      | Chaos tests | 3+ scenarios |\n      | Runbook\
    \ coverage | All critical paths |\n      | On-call coverage | 24/7 |\n      \n\
    \      **PALETTE integration:**\n      - Use as Deployment Readiness gate (RIU-060)\n\
    \      - Track in RIU-100 (Incident preparedness)\n      - Reference in RIU-102\
    \ (Escalation Matrix)\n      - Update post-incident as needed\n      \n      Key\
    \ insight: \"Production-ready\" is a bar, not a feeling. Every checkbox should\
    \ have evidence. If you can't prove it, you haven't done it."
  problem_type: Reliability_and_Failure_Handling
  related_rius:
  - RIU-060
  - RIU-100
  - RIU-101
  - RIU-102
  difficulty: high
  industries:
  - All
  tags:
  - production-readiness
  - reliability-criteria
  - checklist
  - standards
  sources:
  - title: Reliability for GenerativeAI applications - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_7_resilience_high_availability/resilience.html
  - title: Deploying generative AI applications - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_deployment.html
- id: LIB-055
  question: How do I turn a successful AI pilot into a repeatable deployment process?
  answer: "Pilots prove value; operationalization proves sustainability. The gap is\
    \ documentation, automation, and handoff. Without these, you'll rebuild from scratch\
    \ every time.\n      \n      **The pilot-to-production gap:**\n      ```\n   \
    \   Pilot Success                    Production Reality\n      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \
    \                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      Hero developer                 \
    \  Team rotation\n      Manual processes                 Automated pipelines\n\
    \      Single use case                  Multiple deployments\n      Ad-hoc monitoring\
    \                SLO-driven operations\n      \"It works on my machine\"     \
    \    \"It works everywhere\"\n      ```\n      \n      **Five V's Framework for\
    \ operationalization:**\n      \n      | Phase | Pilot Focus | Production Focus\
    \ |\n      |-------|-------------|------------------|\n      | **Value** | Prove\
    \ ROI | Document ROI calculation method |\n      | **Visualize** | Define metrics\
    \ | Create metric templates |\n      | **Validate** | Test solution | Create test\
    \ automation |\n      | **Verify** | Deploy once | Create deployment pipeline\
    \ |\n      | **Venture** | Get resources | Create resource estimation model |\n\
    \      \n      **Step 1: Document everything from the pilot**\n      \n      ```yaml\n\
    \      pilot_documentation:\n        # What was built\n        architecture:\n\
    \          - System diagram with all components\n          - Data flows and integrations\n\
    \          - Model/prompt versions used\n          - Infrastructure specifications\n\
    \          \n        # How it was built\n        process:\n          - Decision\
    \ log (why choices were made)\n          - Challenges encountered and solutions\n\
    \          - What would you do differently?\n          - Time estimates by phase\n\
    \          \n        # How to know it works\n        validation:\n          -\
    \ Success metrics and how measured\n          - Test cases and golden set\n  \
    \        - Edge cases discovered\n          - Failure modes observed\n       \
    \   \n        # What's needed to run it\n        operations:\n          - Monitoring\
    \ requirements\n          - On-call procedures\n          - Common issues and\
    \ fixes\n          - Escalation paths\n      ```\n      \n      **Step 2: Create\
    \ reusable components**\n      \n      ```yaml\n      reusable_components:\n \
    \       # Infrastructure as Code\n        iac_templates:\n          - Terraform/CDK\
    \ modules for AI infrastructure\n          - Parameterized for different use cases\n\
    \          - Environment-specific configurations\n          \n        # Code templates\n\
    \        code_templates:\n          - Prompt management patterns\n          -\
    \ RAG implementation patterns\n          - Agent orchestration patterns\n    \
    \      - Error handling patterns\n          \n        # Pipeline templates\n \
    \       pipeline_templates:\n          - CI/CD pipeline for AI deployments\n \
    \         - Evaluation pipeline\n          - Monitoring setup\n          - Rollback\
    \ procedures\n          \n        # Documentation templates\n        doc_templates:\n\
    \          - Architecture decision record (ADR)\n          - Runbook template\n\
    \          - Post-mortem template\n          - Success metrics template\n    \
    \  ```\n      \n      **Step 3: Build GenAIOps pipeline**\n      \n      ```\n\
    \      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     \
    \ â”‚                     GenAIOps Pipeline                        â”‚\n      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \      â”‚                                                             â”‚\n     \
    \ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n      â”‚  â”‚ Develop\
    \ â”‚â”€â”€â”€â–¶â”‚  Test   â”‚â”€â”€â”€â–¶â”‚ Deploy  â”‚â”€â”€â”€â–¶â”‚ Monitor â”‚  â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n      â”‚       â”‚              â”‚           \
    \   â”‚              â”‚        â”‚\n      â”‚       â–¼              â–¼              â–¼ \
    \             â–¼        â”‚\n      â”‚  Prompt mgmt    Evaluation     Canary/Blue \
    \   Continuous   â”‚\n      â”‚  Version ctrl   Golden set     green deploy   evaluation\
    \   â”‚\n      â”‚  Code review    Quality gates  Rollback       Feedback     â”‚\n\
    \      â”‚                                                loop        â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **AWS implementation:**\n      ```yaml\n      genaiops_stack:\n\
    \        # Centralized AI Gateway\n        - service: \"Amazon Bedrock\"\n   \
    \       pattern: \"Centralized gateway for all LLM calls\"\n          benefits:\
    \ \"Unified monitoring, cost tracking, guardrails\"\n          \n        # Version\
    \ Control\n        - service: \"AWS CodeCommit / GitHub\"\n          pattern:\
    \ \"Prompts, configs, and IaC versioned together\"\n          \n        # CI/CD\n\
    \        - service: \"AWS CodePipeline + CodeBuild\"\n          pattern: \"Automated\
    \ test â†’ deploy â†’ validate\"\n          \n        # Evaluation\n        - service:\
    \ \"Amazon Bedrock Evaluations\"\n          pattern: \"Automated quality checks\
    \ in pipeline\"\n          \n        # Monitoring\n        - service: \"CloudWatch\
    \ + X-Ray\"\n          pattern: \"Metrics, logs, traces with AI-specific dashboards\"\
    \n          \n        # Governance\n        - service: \"Amazon Bedrock Guardrails\"\
    \n          pattern: \"Consistent safety controls across deployments\"\n     \
    \ ```\n      \n      **Step 4: Establish handoff process**\n      \n      ```yaml\n\
    \      handoff_process:\n        from_pilot_team:\n          - Complete documentation\
    \ package\n          - Recorded knowledge transfer sessions\n          - Paired\
    \ deployment with ops team\n          - 2-week shadow support period\n       \
    \   \n        to_operations_team:\n          - Runbook review and acceptance\n\
    \          - On-call training completed\n          - Access and permissions configured\n\
    \          - Escalation paths verified\n          \n        sign_off_criteria:\n\
    \          - Ops team can deploy independently\n          - Ops team can troubleshoot\
    \ common issues\n          - Monitoring and alerting verified\n          - Rollback\
    \ tested successfully\n      ```\n      \n      **Step 5: Create scaling playbook**\n\
    \      \n      ```yaml\n      scaling_playbook:\n        new_deployment_checklist:\n\
    \          phase_1_discovery:\n            - Use case assessment (Five V's)\n\
    \            - Stakeholder identification\n            - Success metrics definition\n\
    \            duration: \"1-2 weeks\"\n            \n          phase_2_development:\n\
    \            - Clone template repository\n            - Customize prompts/configuration\n\
    \            - Integrate with use-case data\n            duration: \"2-4 weeks\"\
    \n            \n          phase_3_validation:\n            - Run evaluation pipeline\n\
    \            - Shadow test with production data\n            - Stakeholder acceptance\n\
    \            duration: \"1-2 weeks\"\n            \n          phase_4_deployment:\n\
    \            - Production deployment\n            - Monitoring verification\n\
    \            - Handoff to operations\n            duration: \"1 week\"\n     \
    \       \n        estimated_time:\n          first_deployment: \"12-16 weeks\"\
    \n          subsequent_deployments: \"4-8 weeks\"  # 50%+ reduction\n      ```\n\
    \      \n      **Governance by design:**\n      - Embed guardrails in templates\
    \ (not added later)\n      - Automate compliance checks in pipeline\n      - Include\
    \ security review in deployment gates\n      - Use AIRI (AI Risk Intelligence)\
    \ for automated governance\n      \n      **PALETTE integration:**\n      - Document\
    \ process in RIU-120 (Integration Mode Selection)\n      - Create templates in\
    \ RIU-121 (Deployment Template)\n      - Track deployments in RIU-122 (Deployment\
    \ Registry)\n      - Reference LIB-003 (pilot scoping) for intake process\n  \
    \    \n      Key insight: The pilot team's job isn't done when the pilot succeeds\
    \ â€” it's done when someone else can deploy the next one without them. Measure\
    \ success by \"time to deploy next use case,\" not just \"pilot worked.\""
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-120
  - RIU-121
  - RIU-122
  difficulty: critical
  industries:
  - All
  tags:
  - pilot-to-production
  - repeatability
  - process-design
  - scaling
  sources:
  - title: 'Beyond pilots: A proven framework for scaling AI to production'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-pilots-a-proven-framework-for-scaling-ai-to-production/
  - title: 'Operationalize generative AI workloads and scale to hundreds of use cases
      with Amazon Bedrock â€“ Part 1: GenAIOps'
    url: https://aws.amazon.com/blogs/machine-learning/operationalize-generative-ai-workloads-and-scale-to-hundreds-of-use-cases-with-amazon-bedrock-part-1-genaiops/
  - title: 'Governance by design: The essential guide for successful AI scaling'
    url: https://aws.amazon.com/blogs/machine-learning/governance-by-design-the-essential-guide-for-successful-ai-scaling/
- id: LIB-056
  question: What's the minimum viable SOP for AI system operations?
  answer: "A minimum viable SOP ensures anyone can operate the system without the\
    \ original builders. Cover: daily operations, incident response, change management,\
    \ and access control.\n      \n      **Minimum viable SOP structure:**\n     \
    \ \n      ```yaml\n      ai_operations_sop:\n        metadata:\n          system_name:\
    \ \"Customer Support AI Assistant\"\n          version: \"1.0.0\"\n          owner:\
    \ \"AI Platform Team\"\n          last_updated: \"2024-06-15\"\n          review_cadence:\
    \ \"Quarterly\"\n          \n        # SECTION 1: SYSTEM OVERVIEW\n        overview:\n\
    \          description: \"RAG-based AI assistant for customer support queries\"\
    \n          architecture_diagram: \"link/to/diagram\"\n          dependencies:\n\
    \            - \"Amazon Bedrock (Claude)\"\n            - \"OpenSearch (vector\
    \ store)\"\n            - \"DynamoDB (conversation state)\"\n          contacts:\n\
    \            primary_owner: \"jane.smith@company.com\"\n            on_call_rotation:\
    \ \"#ai-oncall\"\n            escalation: \"AI Governance Lead\"\n           \
    \ \n        # SECTION 2: DAILY OPERATIONS\n        daily_operations:\n       \
    \   health_checks:\n            - task: \"Review dashboard for anomalies\"\n \
    \             frequency: \"Start of shift\"\n              dashboard: \"link/to/cloudwatch/dashboard\"\
    \n              \n            - task: \"Check error rate and latency\"\n     \
    \         threshold: \"Error >1% or p99 >2s = investigate\"\n              \n\
    \            - task: \"Verify knowledge base sync status\"\n              check:\
    \ \"Last sync < 24 hours ago\"\n              \n          routine_tasks:\n   \
    \         - task: \"Review low-confidence outputs\"\n              frequency:\
    \ \"Daily\"\n              queue: \"link/to/review/queue\"\n              \n \
    \           - task: \"Clear DLQ if items present\"\n              frequency: \"\
    Daily\"\n              procedure: \"See DLQ handling section\"\n             \
    \ \n          monitoring:\n            dashboards:\n              - name: \"AI\
    \ Operations\"\n                url: \"cloudwatch/dashboard/ai-ops\"\n       \
    \       - name: \"Quality Metrics\"\n                url: \"cloudwatch/dashboard/ai-quality\"\
    \n            alerts:\n              - name: \"Error Rate High\"\n           \
    \     action: \"Page on-call\"\n              - name: \"Quality Score Low\"\n\
    \                action: \"Review queue + investigate\"\n                \n  \
    \      # SECTION 3: INCIDENT RESPONSE\n        incident_response:\n          severity_levels:\n\
    \            critical: \"Customer-facing outage\"\n            high: \"Significant\
    \ quality degradation\"\n            medium: \"Non-critical feature impacted\"\
    \n            low: \"Minor issue, workaround exists\"\n            \n        \
    \  response_procedures:\n            critical:\n              - \"Acknowledge\
    \ within 15 minutes\"\n              - \"Engage incident commander\"\n       \
    \       - \"Consider rollback\"\n              - \"Communicate to stakeholders\"\
    \n              \n            high:\n              - \"Acknowledge within 30 minutes\"\
    \n              - \"Begin investigation\"\n              - \"Escalate if no progress\
    \ in 1 hour\"\n              \n          runbook_links:\n            - \"Quality\
    \ Degradation: link/to/runbook\"\n            - \"Model Failure: link/to/runbook\"\
    \n            - \"RAG Retrieval Issues: link/to/runbook\"\n            \n    \
    \      escalation:\n            tier_1: \"On-call engineer\"\n            tier_2:\
    \ \"AI Platform Lead\"\n            tier_3: \"AI Governance Lead\"\n         \
    \   executive: \"VP Engineering\"\n            \n        # SECTION 4: CHANGE MANAGEMENT\n\
    \        change_management:\n          change_types:\n            prompt_update:\n\
    \              approval: \"ML Engineer + QA\"\n              testing: \"Golden\
    \ set evaluation\"\n              deployment: \"Canary (10% for 1 hour)\"\n  \
    \            rollback: \"Automatic on error spike\"\n              \n        \
    \    model_update:\n              approval: \"ML Lead + AI Governance\"\n    \
    \          testing: \"Full evaluation suite + shadow test\"\n              deployment:\
    \ \"Blue-green with manual promotion\"\n              rollback: \"Revert to previous\
    \ endpoint\"\n              \n            knowledge_base_update:\n           \
    \   approval: \"Content owner + ML Engineer\"\n              testing: \"Retrieval\
    \ quality check\"\n              deployment: \"Incremental re-index\"\n      \
    \        rollback: \"Restore from backup\"\n              \n            infrastructure_change:\n\
    \              approval: \"Platform Lead + Security\"\n              testing:\
    \ \"Staging environment validation\"\n              deployment: \"IaC through\
    \ CI/CD\"\n              rollback: \"Previous IaC version\"\n              \n\
    \          deployment_gates:\n            - gate: \"Code review approved\"\n \
    \           - gate: \"Automated tests passing\"\n            - gate: \"Evaluation\
    \ score >= baseline\"\n            - gate: \"Security scan clean\"\n         \
    \   - gate: \"Manual approval (if required)\"\n            \n        # SECTION\
    \ 5: ACCESS MANAGEMENT\n        access_management:\n          access_levels:\n\
    \            read_only: \"View dashboards, logs\"\n            operator: \"Execute\
    \ runbooks, restart services\"\n            developer: \"Deploy changes, modify\
    \ configs\"\n            admin: \"Full access including IAM changes\"\n      \
    \      \n          access_request:\n            process: \"Submit ticket to #access-requests\"\
    \n            approval: \"Team lead + system owner\"\n            review: \"Quarterly\
    \ access review\"\n            \n          emergency_access:\n            process:\
    \ \"Break-glass procedure\"\n            approval: \"Post-hoc, must document within\
    \ 24h\"\n            audit: \"All emergency access logged and reviewed\"\n   \
    \         \n        # SECTION 6: BACKUP AND RECOVERY\n        backup_recovery:\n\
    \          what_is_backed_up:\n            - component: \"Knowledge base content\"\
    \n              frequency: \"Daily\"\n              retention: \"30 days\"\n \
    \             location: \"S3 with versioning\"\n              \n            -\
    \ component: \"Conversation history\"\n              frequency: \"Continuous (DynamoDB)\"\
    \n              retention: \"90 days\"\n              recovery: \"Point-in-time\
    \ recovery\"\n              \n            - component: \"Model artifacts\"\n \
    \             frequency: \"On change\"\n              retention: \"All versions\"\
    \n              location: \"S3 + Model Registry\"\n              \n          recovery_procedures:\n\
    \            knowledge_base: \"link/to/kb/restore/procedure\"\n            conversation_state:\
    \ \"link/to/dynamodb/pitr/procedure\"\n            model_rollback: \"link/to/model/rollback/procedure\"\
    \n            \n          rto_rpo:\n            rto: \"4 hours\"\n           \
    \ rpo: \"1 hour\"\n            \n        # SECTION 7: KEY METRICS\n        key_metrics:\n\
    \          slos:\n            availability: \"99.9%\"\n            latency_p99:\
    \ \"2000ms\"\n            error_rate: \"<0.1%\"\n            quality_score: \"\
    >85%\"\n            \n          operational_metrics:\n            mttr: \"Target:\
    \ <1 hour\"\n            change_failure_rate: \"Target: <5%\"\n            deployment_frequency:\
    \ \"Target: Weekly\"\n      ```\n      \n      **\"Minimum viable\" criteria:**\n\
    \      - [ ] New team member can operate system after reading SOP\n      - [ ]\
    \ All critical procedures have documented steps\n      - [ ] Escalation paths\
    \ are clear\n      - [ ] Recovery procedures are tested\n      - [ ] Change approval\
    \ process is defined\n      - [ ] Access management is documented\n      \n  \
    \    **SOP maintenance:**\n      - Review quarterly or after significant incidents\n\
    \      - Update immediately when procedures change\n      - Version control with\
    \ change history\n      - Validate with tabletop exercises\n      \n      **PALETTE\
    \ integration:**\n      - Store in RIU-069 (Runbook)\n      - Link from RIU-060\
    \ (Deployment Readiness)\n      - Reference in RIU-102 (Escalation Matrix)\n \
    \     - Update based on RIU-100 (Incident Log) learnings\n      \n      Key insight:\
    \ An SOP you don't update is an SOP that will fail you. Schedule quarterly reviews\
    \ and update after every incident that revealed a gap."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-004
  - RIU-060
  - RIU-069
  - RIU-102
  - RIU-120
  - RIU-121
  difficulty: medium
  industries:
  - All
  tags:
  - sops
  - operations
  - documentation
  - process
  sources:
  - title: Introducing Strands Agent SOPs â€“ Natural Language Workflows for AI Agents
    url: https://aws.amazon.com/blogs/opensource/introducing-strands-agent-sops-natural-language-workflows-for-ai-agents/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: 'Operationalize generative AI workloads with Amazon Bedrock â€“ Part 1: GenAIOps'
    url: https://aws.amazon.com/blogs/machine-learning/operationalize-generative-ai-workloads-and-scale-to-hundreds-of-use-cases-with-amazon-bedrock-part-1-genaiops/
- id: LIB-057
  question: How do I eliminate manual steps from AI deployment pipelines?
  answer: "Manual steps are deployment bottlenecks and error sources. Automate in\
    \ priority order: triggers, testing, deployment, rollback. Keep human approval\
    \ only for ONE-WAY DOORs.\n      \n      **Manual steps to automate (priority\
    \ order):**\n      \n      | Priority | Manual Step | Automation Approach | AWS\
    \ Service |\n      |----------|-------------|---------------------|-------------|\n\
    \      | 1 | Triggering deployments | Git commit/merge triggers | CodePipeline\
    \ |\n      | 2 | Running tests | Automated test suites | CodeBuild |\n      |\
    \ 3 | Evaluation scoring | Automated eval pipeline | Bedrock Evaluations |\n \
    \     | 4 | Building artifacts | Container/Lambda packaging | CodeBuild |\n  \
    \    | 5 | Deploying to staging | IaC deployment | CloudFormation/CDK |\n    \
    \  | 6 | Promoting to production | Automated gates with criteria | CodePipeline\
    \ |\n      | 7 | Rollback on failure | Automatic rollback triggers | CodeDeploy\
    \ |\n      | 8 | Post-deploy validation | Smoke tests + monitoring | Lambda +\
    \ CloudWatch |\n      \n      **Fully automated GenAI pipeline:**\n      \n  \
    \    ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                    AUTOMATED CI/CD PIPELINE                      â”‚\n\
    \      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n \
    \     â”‚                                                                 â”‚\n  \
    \    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n     \
    \ â”‚  â”‚ TRIGGER â”‚â”€â”€â–¶â”‚  BUILD  â”‚â”€â”€â–¶â”‚  TEST   â”‚â”€â”€â–¶â”‚  EVAL   â”‚        â”‚\n      â”‚ \
    \ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n      â”‚    \
    \   â”‚             â”‚             â”‚             â”‚              â”‚\n      â”‚   Git\
    \ push     Container      Unit tests   Golden set         â”‚\n      â”‚   Prompt\
    \ reg   Lambda pkg     Integration  LLM-as-judge       â”‚\n      â”‚   Schedule \
    \    Config         Contract     Quality score      â”‚\n      â”‚               \
    \                                                 â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n      â”‚  â”‚ STAGING â”‚â”€â”€â–¶â”‚ APPROVE â”‚â”€â”€â–¶â”‚\
    \  PROD   â”‚â”€â”€â–¶â”‚ VERIFY  â”‚        â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n      â”‚       â”‚             â”‚             â”‚        \
    \     â”‚              â”‚\n      â”‚   Deploy IaC   Auto-gate     Canary/BG    Smoke\
    \ tests        â”‚\n      â”‚   Shadow test  (criteria)    Traffic      Metrics check\
    \       â”‚\n      â”‚   Load test    Manual (1WD)  shift        Auto-rollback   \
    \   â”‚\n      â”‚                                                               \
    \ â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Automation by component:**\n      \n      **1. Trigger\
    \ automation:**\n      ```yaml\n      triggers:\n        code_change:\n      \
    \    source: \"CodeCommit/GitHub\"\n          events: [\"push to main\", \"PR\
    \ merge\"]\n          action: \"Start pipeline\"\n          \n        prompt_change:\n\
    \          source: \"Prompt Registry (S3/DynamoDB)\"\n          events: [\"New\
    \ version published\"]\n          action: \"Start evaluation + deploy pipeline\"\
    \n          \n        scheduled:\n          source: \"EventBridge\"\n        \
    \  schedule: \"Weekly model evaluation\"\n          action: \"Run full evaluation\
    \ suite\"\n          \n        manual:\n          source: \"Console/CLI\"\n  \
    \        use_case: \"Emergency hotfix\"\n          action: \"Start pipeline with\
    \ expedited gates\"\n      ```\n      \n      **2. Evaluation automation:**\n\
    \      ```yaml\n      automated_evaluation:\n        # Run automatically on every\
    \ change\n        stages:\n          - name: \"Unit tests\"\n            type:\
    \ \"Fast, deterministic\"\n            duration: \"<5 min\"\n            gate:\
    \ \"100% pass\"\n            \n          - name: \"Golden set evaluation\"\n \
    \           type: \"Quality scoring\"\n            tool: \"Bedrock Evaluations\
    \ / FMEval\"\n            metrics: [\"accuracy\", \"relevance\", \"safety\"]\n\
    \            gate: \"Score >= baseline - 5%\"\n            \n          - name:\
    \ \"LLM-as-judge\"\n            type: \"Quality assessment\"\n            tool:\
    \ \"Amazon Nova as evaluator\"\n            gate: \"Score >= 4.0/5.0\"\n     \
    \       \n          - name: \"Cost estimation\"\n            type: \"Token usage\
    \ projection\"\n            gate: \"Cost increase < 20%\"\n      ```\n      \n\
    \      **3. Deployment automation:**\n      ```yaml\n      deployment_automation:\n\
    \        staging:\n          trigger: \"All tests pass\"\n          method: \"\
    CloudFormation/CDK\"\n          validation: \"Automated smoke tests\"\n      \
    \    duration: \"~10 minutes\"\n          \n        production:\n          trigger:\
    \ \"Staging validation pass + approval gate\"\n          method: \"Canary deployment\"\
    \n          phases:\n            - traffic: \"10%\"\n              duration: \"\
    15 min\"\n              validation: \"Error rate < 1%, latency < baseline\"\n\
    \              \n            - traffic: \"50%\"\n              duration: \"30\
    \ min\"\n              validation: \"Quality score stable\"\n              \n\
    \            - traffic: \"100%\"\n              duration: \"Complete\"\n     \
    \         validation: \"All metrics nominal\"\n      ```\n      \n      **4. Automated\
    \ rollback:**\n      ```yaml\n      auto_rollback:\n        triggers:\n      \
    \    - condition: \"Error rate > 5%\"\n            window: \"5 minutes\"\n   \
    \         action: \"Immediate rollback\"\n            \n          - condition:\
    \ \"Latency p99 > 3x baseline\"\n            window: \"10 minutes\"\n        \
    \    action: \"Rollback + alert\"\n            \n          - condition: \"Quality\
    \ score < 70%\"\n            window: \"30 minutes\"\n            action: \"Rollback\
    \ + review queue\"\n            \n        rollback_procedure:\n          - \"\
    Shift traffic to previous version\"\n          - \"Alert on-call\"\n         \
    \ - \"Preserve failed version for analysis\"\n          - \"Log rollback reason\"\
    \n      ```\n      \n      **What to keep manual (ONE-WAY DOORs):**\n      \n\
    \      | Change Type | Automation Level | Why |\n      |-------------|------------------|-----|\n\
    \      | Prompt tweaks | Fully automated | TWO-WAY DOOR, easy rollback |\n   \
    \   | Model version update | Auto-test, manual approve | Higher risk |\n     \
    \ | New capability | Auto-test, manual approve | Business decision |\n      |\
    \ Production data access | Manual approval required | Compliance |\n      | Cost\
    \ increase >50% | Manual approval required | Budget impact |\n      | Breaking\
    \ API change | Manual approval required | ONE-WAY DOOR |\n      \n      **CodePipeline\
    \ example:**\n      ```yaml\n      # AWS CDK Pipeline with automated stages\n\
    \      pipeline = CodePipeline(\n          synth=ShellStep(\"Synth\", commands=[\"\
    npm ci\", \"npm run build\"]),\n          \n          # Automated test stage\n\
    \          pre_production_steps=[\n              ShellStep(\"UnitTests\", commands=[\"\
    npm test\"]),\n              ShellStep(\"Evaluation\", commands=[\"python run_eval.py\"\
    ]),\n          ],\n          \n          # Automated deployment to staging\n \
    \         stages=[\n              DeployStage(self, \"Staging\", env=staging_env),\n\
    \          ],\n          \n          # Manual approval only for production\n \
    \         post_production_steps=[\n              ManualApprovalStep(\"ProductionApproval\"\
    ,\n                  comment=\"Approve production deployment?\"),\n          ]\n\
    \      )\n      ```\n      \n      **Metrics for automation success:**\n     \
    \ - Deployment frequency: Target weekly â†’ daily\n      - Lead time (commit â†’ production):\
    \ Target <1 day\n      - Change failure rate: Target <5%\n      - MTTR: Target\
    \ <1 hour\n      - Manual steps per deployment: Target 0-1\n      \n      **PALETTE\
    \ integration:**\n      - Automate RIU-060 (Deployment Readiness) checks\n   \
    \   - Version prompts in RIU-520 (Prompt Version Control)\n      - Track deployments\
    \ in RIU-121 (Deployment Template)\n      - Auto-populate RIU-122 (Deployment\
    \ Registry)\n      \n      Key insight: Every manual step is a delay, an error\
    \ opportunity, and a scaling bottleneck. Automate everything except decisions\
    \ that require human judgment â€” and even those should have automated gates that\
    \ only escalate when criteria aren't met."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-060
  - RIU-120
  - RIU-121
  - RIU-122
  - RIU-520
  difficulty: high
  industries:
  - All
  tags:
  - automation
  - ci-cd
  - deployment
  - efficiency
  sources:
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: Build an automated generative AI solution evaluation pipeline with Amazon
      Nova
    url: https://aws.amazon.com/blogs/machine-learning/build-an-automated-generative-ai-solution-evaluation-pipeline-with-amazon-nova/
  - title: Automate the machine learning model approval process with Amazon SageMaker
    url: https://aws.amazon.com/blogs/machine-learning/automate-the-machine-learning-model-approval-process-with-amazon-sagemaker-model-registry-and-amazon-sagemaker-pipelines/
  - title: How to add notifications and manual approval to an AWS CDK Pipeline
    url: https://aws.amazon.com/blogs/devops/how-to-add-notifications-and-manual-approval-to-an-aws-cdk-pipeline/
- id: LIB-058
  question: What training materials enable non-experts to operate AI systems?
  answer: "Non-experts need role-specific training, not generic AI courses. Focus\
    \ on \"how to operate THIS system\" not \"how AI works.\" Combine documentation,\
    \ hands-on practice, and ongoing support.\n      \n      **Training distribution\
    \ model (70-20-10):**\n      \n      | Audience | % of Effort | Training Focus\
    \ |\n      |----------|-------------|----------------|\n      | End Users (70%)\
    \ | Day-to-day operation | How to use the AI system effectively |\n      | Business\
    \ Leaders (20%) | Decision making | When to use AI, interpreting outputs |\n \
    \     | Technical Teams (10%) | Deep expertise | Troubleshooting, customization\
    \ |\n      \n      **Role-specific training materials:**\n      \n      ```yaml\n\
    \      training_materials:\n        # OPERATORS (Day-to-day system operation)\n\
    \        operators:\n          prerequisite: \"None - designed for non-technical\
    \ staff\"\n          \n          materials:\n            - type: \"Quick Start\
    \ Guide\"\n              format: \"PDF/Wiki, 5 pages max\"\n              content:\n\
    \                - System purpose and capabilities\n                - How to access\
    \ and authenticate\n                - Basic usage workflow\n                -\
    \ What to do when it doesn't work\n                - Who to contact for help\n\
    \                \n            - type: \"Video Walkthrough\"\n              format:\
    \ \"Screen recording, 10-15 min\"\n              content:\n                - End-to-end\
    \ happy path demonstration\n                - Common variations\n            \
    \    - Tips for best results\n                \n            - type: \"FAQ Document\"\
    \n              format: \"Searchable wiki\"\n              content:\n        \
    \        - \"Why did it give me this answer?\"\n                - \"What if the\
    \ output seems wrong?\"\n                - \"How do I report issues?\"\n     \
    \           - \"What shouldn't I ask it?\"\n                \n            - type:\
    \ \"Hands-on Exercise\"\n              format: \"Sandbox environment\"\n     \
    \         duration: \"30 minutes\"\n              content:\n                -\
    \ Guided exercises with expected outputs\n                - Practice with realistic\
    \ scenarios\n                - Self-assessment quiz\n                \n      \
    \    competency_validation:\n            - \"Can complete standard workflow independently\"\
    \n            - \"Knows when to escalate vs. retry\"\n            - \"Understands\
    \ system limitations\"\n            \n        # POWER USERS (Advanced usage, first-line\
    \ support)\n        power_users:\n          prerequisite: \"Completed Operator\
    \ training\"\n          \n          materials:\n            - type: \"Advanced\
    \ Usage Guide\"\n              content:\n                - Prompt engineering\
    \ best practices\n                - Complex use case patterns\n              \
    \  - Integration with other tools\n                - Performance optimization\
    \ tips\n                \n            - type: \"Troubleshooting Guide\"\n    \
    \          content:\n                - Common issues and solutions\n         \
    \       - How to interpret error messages\n                - When to escalate\
    \ to technical team\n                - How to gather diagnostic info\n       \
    \         \n            - type: \"Workshop\"\n              format: \"Live or\
    \ recorded, 2 hours\"\n              content:\n                - Deep dive on\
    \ system architecture (high level)\n                - Hands-on troubleshooting\
    \ exercises\n                - Q&A with technical team\n                \n   \
    \       competency_validation:\n            - \"Can handle 80% of user questions\"\
    \n            - \"Can diagnose common issues\"\n            - \"Can effectively\
    \ escalate complex issues\"\n            \n        # TECHNICAL OPERATORS (On-call,\
    \ system administration)\n        technical_operators:\n          prerequisite:\
    \ \"Technical background + Power User training\"\n          \n          materials:\n\
    \            - type: \"System Architecture Overview\"\n              content:\n\
    \                - Component diagram and data flows\n                - Dependencies\
    \ and integration points\n                - Failure modes and recovery\n     \
    \           \n            - type: \"Operational Runbook\"\n              content:\n\
    \                - Health check procedures\n                - Incident response\
    \ procedures\n                - Deployment procedures\n                - Backup\
    \ and recovery procedures\n                \n            - type: \"Monitoring\
    \ Guide\"\n              content:\n                - Dashboard walkthrough\n \
    \               - Alert interpretation\n                - Metric thresholds and\
    \ meaning\n                - Log analysis techniques\n                \n     \
    \       - type: \"Hands-on Lab\"\n              format: \"Guided exercises, 4\
    \ hours\"\n              content:\n                - Deploy to test environment\n\
    \                - Simulate and recover from failures\n                - Execute\
    \ runbook procedures\n                - On-call handoff simulation\n         \
    \       \n          competency_validation:\n            - \"Can deploy system\
    \ independently\"\n            - \"Can respond to alerts\"\n            - \"Can\
    \ execute runbooks\"\n            - \"Can perform on-call duties\"\n         \
    \   \n        # BUSINESS STAKEHOLDERS (Decision makers)\n        business_stakeholders:\n\
    \          prerequisite: \"None\"\n          \n          materials:\n        \
    \    - type: \"Executive Briefing\"\n              format: \"Presentation, 30\
    \ min\"\n              content:\n                - What the system does and why\n\
    \                - Business value and metrics\n                - Limitations and\
    \ risks\n                - Governance and compliance\n                \n     \
    \       - type: \"Decision Guide\"\n              content:\n                -\
    \ When to use AI vs. human judgment\n                - How to interpret AI outputs\n\
    \                - Escalation criteria\n                - Feedback mechanisms\n\
    \      ```\n      \n      **Training delivery methods:**\n      \n      | Method\
    \ | Best For | Effort to Create | Maintenance |\n      |--------|----------|------------------|-------------|\n\
    \      | Documentation (wiki) | Reference | Low | Easy |\n      | Video walkthroughs\
    \ | Visual learners | Medium | Hard to update |\n      | Hands-on labs | Skill\
    \ building | High | Medium |\n      | Live workshops | Complex topics | Medium\
    \ | None (one-time) |\n      | Office hours | Ongoing questions | Low | Ongoing\
    \ time |\n      | AI Ambassadors | Peer support | Low | Training ambassadors |\n\
    \      \n      **Minimum viable training kit:**\n      \n      - [ ] **Quick Start\
    \ Guide** (1-2 pages): Get started in 10 minutes\n      - [ ] **FAQ** (living\
    \ document): Top 20 questions answered\n      - [ ] **Video demo** (10 min): Watch\
    \ before first use\n      - [ ] **Runbook** (for operators): How to keep it running\n\
    \      - [ ] **Escalation contact**: Who to ask when stuck\n      \n      **AI\
    \ Ambassador Program:**\n      \n      ```yaml\n      ai_ambassador_program:\n\
    \        purpose: \"Bridge between technical team and end users\"\n        \n\
    \        selection:\n          - \"1 ambassador per 20-50 users\"\n          -\
    \ \"Enthusiastic early adopters\"\n          - \"Good communicators\"\n      \
    \    - \"Respected by peers\"\n          \n        training:\n          - \"Power\
    \ User certification\"\n          - \"Monthly sync with technical team\"\n   \
    \       - \"Early access to new features\"\n          \n        responsibilities:\n\
    \          - \"First point of contact for questions\"\n          - \"Collect and\
    \ relay feedback\"\n          - \"Identify training gaps\"\n          - \"Champion\
    \ adoption in their team\"\n          \n        support:\n          - \"Dedicated\
    \ Slack channel\"\n          - \"Office hours with technical team\"\n        \
    \  - \"Recognition program\"\n      ```\n      \n      **External training resources\
    \ (AWS):**\n      \n      | Role | AWS Training | Certification |\n      |------|--------------|---------------|\n\
    \      | Anyone | \"Introduction to Generative AI\" | AWS Certified AI Practitioner\
    \ |\n      | Developers | \"Amazon Bedrock Getting Started\" | - |\n      | ML\
    \ Engineers | \"Amazon SageMaker JumpStart\" | AWS Certified ML Engineer |\n \
    \     | All Technical | \"Amazon Q Developer\" | - |\n      \n      **Training\
    \ effectiveness metrics:**\n      \n      - Time to productivity (first successful\
    \ use)\n      - Support ticket volume from trained users\n      - User satisfaction\
    \ scores\n      - Error rate by training completion status\n      - Ambassador\
    \ utilization rate\n      \n      **PALETTE integration:**\n      - Store materials\
    \ in RIU-140 (Training Materials)\n      - Track competencies in RIU-004 (Workstream\
    \ planning)\n      - Link from RIU-122 (Deployment Registry) to relevant training\n\
    \      - Update based on support ticket patterns\n      \n      Key insight: The\
    \ best training material is the one that prevents support tickets. Track what\
    \ users struggle with and build training that addresses those specific gaps."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-004
  - RIU-122
  - RIU-140
  difficulty: high
  industries:
  - All
  tags:
  - training
  - enablement
  - documentation
  - knowledge-transfer
  sources:
  - title: Training and Upskilling - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/4_0_systematic_path_to_production_framework/4_3_training_upskilling/index.html
  - title: Change Management and Adoption for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_3_implementation_and_execution/5_3_2_change_management_and_adoption.html
  - title: Unlock the power of generative AI with AWS Training and Certification
    url: https://aws.amazon.com/blogs/training-and-certification/unlock-the-power-of-generative-ai-with-aws-training-and-certification/
- id: LIB-059
  question: How do I scale AI operations from 1 customer to 100 without 100x team
    growth?
  answer: "Scaling 100x with <10x team growth requires leverage: multi-tenancy, self-service,\
    \ automation, and shared infrastructure. Every manual, per-customer task becomes\
    \ a scaling bottleneck.\n      \n      **The scaling math:**\n      ```\n    \
    \  1 Customer:   1 FTE dedicated = 1:1 ratio\n      10 Customers: 3 FTE with automation\
    \ = 1:3.3 ratio\n      100 Customers: 8 FTE with platform = 1:12.5 ratio\n   \
    \   \n      Goal: Sublinear team growth through leverage\n      ```\n      \n\
    \      **Four pillars of operational leverage:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                    OPERATIONAL LEVERAGE                      â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n      â”‚  â”‚ MULTI-TENANCY â”‚      \
    \  â”‚  SELF-SERVICE â”‚                â”‚\n      â”‚  â”‚ Share infra,  â”‚        â”‚ Users\
    \ help    â”‚                â”‚\n      â”‚  â”‚ isolate data  â”‚        â”‚ themselves \
    \   â”‚                â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  \
    \              â”‚\n      â”‚                                                    \
    \         â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             \
    \   â”‚\n      â”‚  â”‚  AUTOMATION   â”‚        â”‚ STANDARDIZED  â”‚                â”‚\n\
    \      â”‚  â”‚ Eliminate     â”‚        â”‚  PLATFORM     â”‚                â”‚\n      â”‚\
    \  â”‚ manual tasks  â”‚        â”‚ Reusable      â”‚                â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n      â”‚                         \
    \                                    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Pillar 1: Multi-tenancy architecture**\n      \n \
    \     | Model | Description | Best For | Team Efficiency |\n      |-------|-------------|----------|-----------------|\n\
    \      | Siloed | Dedicated infrastructure per customer | Enterprise, regulated\
    \ | Lower (more to manage) |\n      | Pooled | Shared infrastructure, logical\
    \ isolation | Standard customers | Higher (one system) |\n      | Hybrid | Mix\
    \ of siloed and pooled | Tiered offerings | Medium |\n      \n      ```yaml\n\
    \      multi_tenant_architecture:\n        shared_components:\n          - \"\
    Centralized AI Gateway (Amazon Bedrock)\"\n          - \"Shared model endpoints\"\
    \n          - \"Common monitoring infrastructure\"\n          - \"Unified deployment\
    \ pipeline\"\n          \n        tenant_isolated:\n          - \"Data storage\
    \ (separate S3 prefixes/buckets)\"\n          - \"Knowledge bases (per-tenant\
    \ RAG)\"\n          - \"Conversation history\"\n          - \"Custom prompts/configurations\"\
    \n          \n        isolation_mechanisms:\n          - \"IAM policies with tenant\
    \ context\"\n          - \"Row-level security in databases\"\n          - \"Tenant\
    \ ID in all requests\"\n          - \"Separate CloudWatch log groups\"\n     \
    \ ```\n      \n      **Pillar 2: Self-service enablement**\n      \n      ```yaml\n\
    \      self_service_capabilities:\n        # What customers can do without support\
    \ ticket\n        tier_1_self_service:\n          - \"Access dashboards and usage\
    \ reports\"\n          - \"Adjust prompt templates (within guardrails)\"\n   \
    \       - \"Update knowledge base content\"\n          - \"View logs and traces\"\
    \n          - \"Basic troubleshooting via FAQ\"\n          \n        tier_2_light_touch:\n\
    \          - \"Request quota increases\"\n          - \"Add new users\"\n    \
    \      - \"Export data\"\n          - \"Feature flag changes\"\n          \n \
    \       tier_3_supported:\n          - \"Custom integrations\"\n          - \"\
    New capability requests\"\n          - \"Complex troubleshooting\"\n         \
    \ - \"Architecture changes\"\n          \n        self_service_tools:\n      \
    \    - \"Admin portal per tenant\"\n          - \"API for common operations\"\n\
    \          - \"Documentation wiki\"\n          - \"AI-powered support bot (eating\
    \ our own cooking)\"\n      ```\n      \n      **Pillar 3: Automation**\n    \
    \  \n      | Task | Manual Time | Automated Time | Savings |\n      |------|-------------|----------------|---------|\n\
    \      | Customer onboarding | 4-8 hours | 15 minutes | 95%+ |\n      | Deployment\
    \ | 2 hours | 5 minutes | 95%+ |\n      | Monitoring setup | 1 hour | Automatic\
    \ | 100% |\n      | Incident triage | 30 min | 5 min (AI-assisted) | 80%+ |\n\
    \      | Usage reporting | 2 hours/month | Automatic | 100% |\n      \n      ```yaml\n\
    \      automation_priorities:\n        # Automate in this order\n        1_onboarding:\n\
    \          - \"Tenant provisioning script\"\n          - \"Configuration templating\"\
    \n          - \"Automatic monitoring setup\"\n          - \"Welcome email with\
    \ credentials\"\n          \n        2_operations:\n          - \"Auto-scaling\
    \ based on usage\"\n          - \"Automated backup and recovery\"\n          -\
    \ \"Self-healing for common issues\"\n          - \"Automated cost reporting\"\
    \n          \n        3_support:\n          - \"AI chatbot for tier-1 questions\"\
    \n          - \"Automated ticket routing\"\n          - \"Runbook automation\"\
    \n          - \"Proactive issue detection\"\n      ```\n      \n      **Pillar\
    \ 4: Standardized platform**\n      \n      ```yaml\n      platform_components:\n\
    \        # Build once, use for all customers\n        infrastructure:\n      \
    \    - \"Terraform/CDK modules for tenant provisioning\"\n          - \"Centralized\
    \ AI gateway with routing\"\n          - \"Shared monitoring and alerting\"\n\
    \          - \"Common CI/CD pipeline\"\n          \n        application:\n   \
    \       - \"Prompt library (configurable per tenant)\"\n          - \"Standard\
    \ integration patterns\"\n          - \"Reusable UI components\"\n          -\
    \ \"Common API design\"\n          \n        operations:\n          - \"Unified\
    \ admin console\"\n          - \"Centralized logging and tracing\"\n         \
    \ - \"Shared runbooks with tenant context\"\n          - \"Standard SLAs and SLOs\"\
    \n      ```\n      \n      **Organizational model for scale:**\n      \n     \
    \ ```yaml\n      hybrid_coe_model:\n        central_platform_team:\n         \
    \ size: \"5-8 engineers\"\n          responsibilities:\n            - \"Core platform\
    \ development\"\n            - \"Infrastructure management\"\n            - \"\
    Security and compliance\"\n            - \"Tooling and automation\"\n        \
    \    - \"Tier-3 escalations\"\n            \n        customer_success_team:\n\
    \          size: \"2-3 per 50 customers\"\n          responsibilities:\n     \
    \       - \"Customer onboarding\"\n            - \"Tier-1/2 support\"\n      \
    \      - \"Usage optimization\"\n            - \"Feedback collection\"\n     \
    \       \n        ratio_targets:\n          \"10 customers\": \"3 FTE platform\
    \ + 1 FTE success\"\n          \"50 customers\": \"5 FTE platform + 3 FTE success\"\
    \n          \"100 customers\": \"6 FTE platform + 5 FTE success\"\n      ```\n\
    \      \n      **Cost allocation for multi-tenant:**\n      \n      ```yaml\n\
    \      cost_tracking:\n        # Application Inference Profiles per tenant\n \
    \       per_tenant_tracking:\n          - \"Token usage (input + output)\"\n \
    \         - \"Compute time\"\n          - \"Storage\"\n          - \"API calls\"\
    \n          \n        implementation:\n          - \"Inference profiles per tenant/team\"\
    \n          - \"Cost allocation tags\"\n          - \"Usage-based alarms\"\n \
    \         - \"Consumption limits/quotas\"\n          \n        reporting:\n  \
    \        - \"Automated monthly cost reports\"\n          - \"Usage dashboards\
    \ per tenant\"\n          - \"Anomaly detection for cost spikes\"\n      ```\n\
    \      \n      **Scaling metrics to track:**\n      \n      | Metric | 1 Customer\
    \ | 10 Customers | 100 Customers |\n      |--------|------------|--------------|---------------|\n\
    \      | Team size | 3 | 6 | 12 |\n      | Customers per FTE | 0.33 | 1.7 | 8.3\
    \ |\n      | Onboarding time | 2 weeks | 2 days | 2 hours |\n      | Support tickets/customer\
    \ | 10/month | 5/month | 2/month |\n      | Automated tasks % | 20% | 60% | 90%\
    \ |\n      \n      **PALETTE integration:**\n      - Design multi-tenancy in RIU-120\
    \ (Integration Mode Selection)\n      - Standardize deployments with RIU-121 (Deployment\
    \ Template)\n      - Track all tenants in RIU-122 (Deployment Registry)\n    \
    \  - Automate onboarding per RIU-055 guidance\n      \n      Key insight: Every\
    \ customer-specific task you do manually becomes a scaling constraint. The question\
    \ for every operational task: \"How do we do this once and apply to 100 customers?\""
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-120
  - RIU-121
  - RIU-122
  difficulty: critical
  industries:
  - Enterprise SaaS
  - Marketplaces
  - Operations
  tags:
  - scaling
  - efficiency
  - automation
  - leverage
  sources:
  - title: Building multi-tenant architectures for agentic AI on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/agentic-ai-multitenant/introduction.html
  - title: Build a multi-tenant generative AI environment for your enterprise on AWS
    url: https://aws.amazon.com/blogs/machine-learning/build-a-multi-tenant-generative-ai-environment-for-your-enterprise-on-aws/
  - title: 'Scaling AI Operations and Costs: Mastering Application Inference Profiles'
    url: https://catalog.us-east-1.prod.workshops.aws/workshops/59f16109-2e4a-424e-8f51-dfda4ecdb83e
  - title: Operationalize generative AI workloads with Amazon Bedrock â€“ GenAIOps
    url: https://aws.amazon.com/blogs/machine-learning/operationalize-generative-ai-workloads-and-scale-to-hundreds-of-use-cases-with-amazon-bedrock-part-1-genaiops/
- id: LIB-060
  question: What metrics indicate an AI system is ready to scale beyond pilot?
  answer: "Scaling readiness isn't just \"it works\" â€” it's \"it works reliably, economically,\
    \ and we can support it at scale.\" Evaluate across four dimensions: technical,\
    \ operational, business, and organizational.\n      \n      **Scaling readiness\
    \ scorecard:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚              SCALING READINESS ASSESSMENT                    â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  TECHNICAL\
    \        OPERATIONAL      BUSINESS     ORGANIZATIONALâ”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  \
    \     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n      â”‚  Performance âœ“  \
    \  Monitoring âœ“    ROI proven âœ“  Team ready âœ“  â”‚\n      â”‚  Reliability âœ“    Runbooks\
    \ âœ“      Demand âœ“      Process âœ“     â”‚\n      â”‚  Cost viable âœ“    On-call âœ“  \
    \     Stakeholder âœ“ Governance âœ“  â”‚\n      â”‚                                 \
    \                             â”‚\n      â”‚  ALL FOUR DIMENSIONS MUST PASS TO SCALE\
    \                      â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **DIMENSION 1: Technical Readiness**\n      \n     \
    \ | Metric | Pilot Threshold | Scale Threshold | How to Measure |\n      |--------|-----------------|-----------------|----------------|\n\
    \      | **Latency p99** | <5s | <2s | CloudWatch percentiles |\n      | **TTFT**\
    \ | <1s | <500ms | Custom metric |\n      | **Error rate** | <5% | <1% | CloudWatch\
    \ errors/total |\n      | **Availability** | 95% | 99.9% | Uptime calculation\
    \ |\n      | **Quality score** | >70% | >85% | Evaluation pipeline |\n      |\
    \ **Throughput** | Handles pilot load | 2x projected scale | Load testing |\n\
    \      | **Cost per request** | Understood | Within budget | Cost allocation |\n\
    \      \n      ```yaml\n      technical_checklist:\n        performance:\n   \
    \       - metric: \"latency_p99\"\n            current: \"1.2s\"\n           \
    \ threshold: \"<2s\"\n            status: \"PASS\"\n            \n          -\
    \ metric: \"error_rate\"\n            current: \"0.8%\"\n            threshold:\
    \ \"<1%\"\n            status: \"PASS\"\n            \n          - metric: \"\
    quality_score\"\n            current: \"87%\"\n            threshold: \">85%\"\
    \n            status: \"PASS\"\n            \n        scalability:\n         \
    \ - test: \"Load test at 2x projected scale\"\n            result: \"Passed, latency\
    \ stable\"\n            status: \"PASS\"\n            \n          - test: \"Auto-scaling\
    \ validation\"\n            result: \"Scales within 60 seconds\"\n           \
    \ status: \"PASS\"\n            \n        cost:\n          - metric: \"cost_per_request\"\
    \n            current: \"$0.03\"\n            budget: \"$0.05\"\n            status:\
    \ \"PASS\"\n            \n          - metric: \"projected_monthly_cost\"\n   \
    \         current: \"$15,000\"\n            budget: \"$20,000\"\n            status:\
    \ \"PASS\"\n      ```\n      \n      **DIMENSION 2: Operational Readiness**\n\
    \      \n      | Requirement | Pilot | Scale | Status |\n      |-------------|-------|-------|--------|\n\
    \      | **Monitoring dashboards** | Basic | Comprehensive | Required |\n    \
    \  | **Alerting** | Manual checks | Automated alerts | Required |\n      | **Runbooks**\
    \ | Notes | Formal documentation | Required |\n      | **On-call rotation** |\
    \ Ad-hoc | Formal rotation | Required |\n      | **Incident response** | Reactive\
    \ | Defined process | Required |\n      | **Deployment automation** | Manual/semi\
    \ | Fully automated | Required |\n      | **Rollback tested** | Not tested | Tested\
    \ & documented | Required |\n      \n      ```yaml\n      operational_checklist:\n\
    \        observability:\n          - \"CloudWatch dashboards configured\" # PASS/FAIL\n\
    \          - \"Alerts for critical metrics\" # PASS/FAIL\n          - \"Distributed\
    \ tracing enabled\" # PASS/FAIL\n          - \"Log retention configured\" # PASS/FAIL\n\
    \          \n        documentation:\n          - \"SOP documented (LIB-056)\"\
    \ # PASS/FAIL\n          - \"Runbooks for common issues\" # PASS/FAIL\n      \
    \    - \"Architecture diagram current\" # PASS/FAIL\n          - \"Escalation\
    \ paths defined\" # PASS/FAIL\n          \n        team_readiness:\n         \
    \ - \"On-call rotation established\" # PASS/FAIL\n          - \"Team trained on\
    \ operations\" # PASS/FAIL\n          - \"Handoff from pilot team complete\" #\
    \ PASS/FAIL\n      ```\n      \n      **DIMENSION 3: Business Readiness**\n  \
    \    \n      | Metric | Evidence Required | Threshold |\n      |--------|-------------------|-----------|\n\
    \      | **ROI demonstrated** | Before/after comparison | Positive ROI |\n   \
    \   | **User satisfaction** | Survey or feedback | >4.0/5.0 |\n      | **Adoption\
    \ rate** | % of target users active | >70% of pilot users |\n      | **Business\
    \ KPI impact** | Measurable improvement | Meeting targets |\n      | **Stakeholder\
    \ approval** | Sign-off documented | Approved |\n      | **Demand validated**\
    \ | Pipeline of additional users | Demand exists |\n      \n      ```yaml\n  \
    \    business_checklist:\n        value_proven:\n          - metric: \"ROI\"\n\
    \            baseline: \"Manual process: $50/task\"\n            current: \"AI-assisted:\
    \ $15/task\"\n            improvement: \"70% cost reduction\"\n            status:\
    \ \"PASS\"\n            \n          - metric: \"user_satisfaction\"\n        \
    \    score: \"4.3/5.0\"\n            threshold: \">4.0\"\n            status:\
    \ \"PASS\"\n            \n          - metric: \"pilot_adoption\"\n           \
    \ active_users: \"85 of 100\"\n            threshold: \">70%\"\n            status:\
    \ \"PASS\"\n            \n        demand_validated:\n          - \"Waitlist for\
    \ access: 500 users\"\n          - \"Business units requesting: 5\"\n        \
    \  - \"Executive sponsor committed: Yes\"\n          \n        stakeholder_approval:\n\
    \          - approver: \"Product Lead\"\n            status: \"Approved\"\n  \
    \        - approver: \"Finance\"\n            status: \"Approved\"\n         \
    \ - approver: \"Legal/Compliance\"\n            status: \"Approved\"\n      ```\n\
    \      \n      **DIMENSION 4: Organizational Readiness**\n      \n      | Requirement\
    \ | Description | Status |\n      |-------------|-------------|--------|\n   \
    \   | **Ownership assigned** | Clear team owns production | Required |\n     \
    \ | **Governance in place** | AI governance review passed | Required |\n     \
    \ | **Support model defined** | Who handles what issues | Required |\n      |\
    \ **Training materials** | Operators and users trained | Required |\n      | **Change\
    \ management** | Process for updates defined | Required |\n      | **Budget approved**\
    \ | Funding for scale operation | Required |\n      \n      ```yaml\n      organizational_checklist:\n\
    \        ownership:\n          - \"Production owner identified: AI Platform Team\"\
    \n          - \"On-call rotation staffed\"\n          - \"Escalation matrix documented\"\
    \n          \n        governance:\n          - \"AI Governance review: Passed\"\
    \n          - \"Security review: Passed\"\n          - \"Compliance review: Passed\"\
    \n          \n        enablement:\n          - \"User training materials complete\"\
    \n          - \"Operator training complete\"\n          - \"AI Ambassadors identified\"\
    \n          \n        resources:\n          - \"Scaling budget approved\"\n  \
    \        - \"Team capacity available\"\n          - \"Infrastructure provisioned\"\
    \n      ```\n      \n      **Go/No-Go decision framework:**\n      \n      ```yaml\n\
    \      scaling_decision:\n        gate_criteria:\n          technical: \"All metrics\
    \ within threshold\"\n          operational: \"All checklist items PASS\"\n  \
    \        business: \"ROI positive + stakeholder approval\"\n          organizational:\
    \ \"Team + governance + budget ready\"\n          \n        decision_matrix:\n\
    \          all_pass: \"GO - Proceed with scaling\"\n          one_fail: \"CONDITIONAL\
    \ - Address gaps, re-evaluate in 2 weeks\"\n          multiple_fail: \"NO-GO -\
    \ Not ready, create remediation plan\"\n          \n        escalation:\n    \
    \      decision_maker: \"AI Governance Lead + Product Lead\"\n          meeting:\
    \ \"Scaling Readiness Review\"\n          artifacts: \"This scorecard with evidence\"\
    \n      ```\n      \n      **Red flags (not ready to scale):**\n      - Quality\
    \ score unstable or declining\n      - Support tickets per user increasing\n \
    \     - Cost per request higher than projected\n      - Pilot users not adopting\n\
    \      - No formal on-call rotation\n      - Runbooks don't exist or untested\n\
    \      - ROI not demonstrated with data\n      \n      **PALETTE integration:**\n\
    \      - Use as Deployment Readiness gate (RIU-060)\n      - Track metrics in\
    \ RIU-540 (Evaluation Harness)\n      - Document approval in decisions.md (RIU-003)\n\
    \      - Update RIU-532 (Model Registry) with scale status\n      \n      Key\
    \ insight: \"It works\" is pilot criteria. \"It works, we can afford it, we can\
    \ support it, and users want more\" is scaling criteria. Don't skip dimensions\
    \ â€” technical success with organizational unreadiness still fails."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-003
  - RIU-060
  - RIU-120
  - RIU-532
  - RIU-540
  difficulty: high
  industries:
  - All
  tags:
  - scaling-readiness
  - metrics
  - validation
  - criteria
  sources:
  - title: 'Beyond pilots: A proven framework for scaling AI to production'
    url: https://aws.amazon.com/blogs/machine-learning/beyond-pilots-a-proven-framework-for-scaling-ai-to-production/
  - title: Business Value and use cases - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/1_0_generative_ai_fundamentals/1_2_business_value_and_use_cases/1_2_business_value_and_use_cases.html
  - title: Enabling customers to deliver production-ready AI agents at scale
    url: https://aws.amazon.com/blogs/machine-learning/enabling-customers-to-deliver-production-ready-ai-agents-at-scale/
- id: LIB-061
  question: How do I design AI systems that work across regions with different regulations?
  answer: "Multi-region AI compliance requires architecture that respects data boundaries\
    \ while enabling global operations. Design for the strictest regulation, then\
    \ relax where permitted.\n      \n      **Key regulatory landscape:**\n      \n\
    \      | Region | Key Regulation | Key Requirements |\n      |--------|----------------|------------------|\n\
    \      | **EU** | EU AI Act (Aug 2024), GDPR | Risk classification, data residency,\
    \ transparency |\n      | **US** | State laws (CA, CO), sector rules | Varies\
    \ by state and sector |\n      | **UK** | UK GDPR, AI framework | Similar to EU\
    \ but diverging |\n      | **APAC** | Country-specific (PDPA, etc.) | Data localization\
    \ requirements vary |\n      | **Global** | ISO IEC 42001 | AI management system\
    \ standard |\n      \n      **Architecture decision framework:**\n      \n   \
    \   ```\n      For each region, determine:\n      \n      1. CAN data leave this\
    \ region?\n         â”œâ”€â”€ YES â†’ Can use cross-region inference\n         â””â”€â”€ NO\
    \ â†’ Need local processing\n         \n      2. WHAT data is restricted?\n    \
    \     â”œâ”€â”€ All data â†’ Fully local architecture\n         â”œâ”€â”€ PII only â†’ Anonymize\
    \ before cross-region\n         â””â”€â”€ Specific categories â†’ Selective routing\n\
    \         \n      3. WHAT AI risk level applies?\n         â”œâ”€â”€ High-risk (EU AI\
    \ Act) â†’ Additional requirements\n         â””â”€â”€ Limited/minimal risk â†’ Standard\
    \ controls\n      ```\n      \n      **Architecture patterns by compliance requirement:**\n\
    \      \n      **Pattern 1: Cross-Region Inference (CRIS)**\n      ```\n     \
    \ Best for: Performance optimization with compliance\n      \n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                    GLOBAL USERS                         â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \                              â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \                    â”‚   CRIS Profile    â”‚\n                    â”‚  (Geographic/EU)\
    \  â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            \
    \  â”‚\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n          â–¼       \
    \            â–¼                   â–¼\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      \
    \ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚ EU-West â”‚       â”‚EU-Centralâ”‚       â”‚  EU-N   â”‚\n      â”‚\
    \ Region  â”‚       â”‚  Region  â”‚       â”‚ Region  â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      \n      Data stays within EU, inference distributed\
    \ for availability\n      ```\n      \n      ```yaml\n      cris_configuration:\n\
    \        profile_type: \"geographic\"  # or \"global\"\n        \n        geographic_eu:\n\
    \          regions: [\"eu-west-1\", \"eu-central-1\", \"eu-north-1\"]\n      \
    \    use_case: \"EU data residency required\"\n          data_flow: \"Data stays\
    \ in EU regions only\"\n          \n        global:\n          regions: [\"all\
    \ available\"]\n          use_case: \"No data residency restrictions\"\n     \
    \     data_flow: \"Routed to optimal region\"\n          \n        security:\n\
    \          - \"Data encrypted in transit\"\n          - \"Temporary processing\
    \ only\"\n          - \"No persistent storage in destination\"\n      ```\n  \
    \    \n      **Pattern 2: Fully Local RAG (Outposts)**\n      ```\n      Best\
    \ for: Strictest data residency requirements\n      \n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                  CUSTOMER DATACENTER                     â”‚\n      â”‚ \
    \ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n      â”‚  â”‚       \
    \       AWS OUTPOSTS                        â”‚    â”‚\n      â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” \
    \ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”‚\n      â”‚  â”‚  â”‚ Bedrock  â”‚  â”‚ Knowledgeâ”‚\
    \  â”‚  Vector  â”‚      â”‚    â”‚\n      â”‚  â”‚  â”‚  Agent   â”‚  â”‚   Base   â”‚  â”‚   DB  \
    \   â”‚      â”‚    â”‚\n      â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\
    \    â”‚\n      â”‚  â”‚                                                  â”‚    â”‚\n \
    \     â”‚  â”‚  All AI processing on-premises                   â”‚    â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n  \
    \    \n      Data never leaves customer premises\n      ```\n      \n      **Pattern\
    \ 3: Hybrid RAG (Regional + Edge)**\n      ```\n      Best for: Balance of capability\
    \ and compliance\n      \n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                    AWS CLOUD (EU)                        â”‚\n      â”‚ \
    \ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n      â”‚  â”‚ Bedrock\
    \  â”‚  â”‚ Non-PII  â”‚  â† General knowledge       â”‚\n      â”‚  â”‚ Agents   â”‚  â”‚   KB\
    \     â”‚                            â”‚\n      â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    \
    \                        â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \              â”‚\n              â”‚ Orchestration (no PII)\n              â”‚\n  \
    \    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚    \
    \   â–¼           CUSTOMER EDGE                         â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n      â”‚  â”‚ Local    â”‚  â”‚  PII  \
    \   â”‚  â† Sensitive data local    â”‚\n      â”‚  â”‚ LLM      â”‚  â”‚   KB     â”‚      \
    \                      â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                \
    \            â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      \n      PII stays local, non-sensitive leverages cloud\n      ```\n   \
    \   \n      **Per-region configuration:**\n      \n      ```yaml\n      regional_configuration:\n\
    \        eu:\n          data_residency: \"strict\"\n          inference_profile:\
    \ \"geographic-eu\"\n          knowledge_base_location: \"eu-west-1\"\n      \
    \    logging_region: \"eu-west-1\"\n          pii_handling: \"process locally,\
    \ anonymize before cross-region\"\n          ai_act_risk_level: \"determine per\
    \ use case\"\n          required_controls:\n            - \"Human oversight for\
    \ high-risk\"\n            - \"Transparency documentation\"\n            - \"\
    Bias monitoring\"\n            \n        us:\n          data_residency: \"sector-dependent\"\
    \n          inference_profile: \"geographic-us\" # or global\n          knowledge_base_location:\
    \ \"us-east-1\"\n          logging_region: \"us-east-1\"\n          pii_handling:\
    \ \"varies by state (CCPA, etc.)\"\n          \n        apac_singapore:\n    \
    \      data_residency: \"strict (PDPA)\"\n          inference_profile: \"geographic-apac\"\
    \n          knowledge_base_location: \"ap-southeast-1\"\n          required_controls:\n\
    \            - \"Data transfer agreements\"\n            - \"Local representative\"\
    \n      ```\n      \n      **Data flow controls:**\n      \n      ```yaml\n  \
    \    data_flow_controls:\n        # IAM policies\n        iam_policies:\n    \
    \      - name: \"RestrictCRISToEU\"\n            effect: \"Deny\"\n          \
    \  action: \"bedrock:InvokeModel\"\n            condition:\n              StringNotEquals:\n\
    \                \"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:*:*:inference-profile/eu.*\"\
    \n                \n        # Service Control Policies (SCPs)\n        scps:\n\
    \          - name: \"EnforceEUDataResidency\"\n            target: \"EU Organization\
    \ Units\"\n            policy:\n              - deny_regions_outside: [\"eu-west-1\"\
    , \"eu-central-1\", \"eu-north-1\"]\n              - deny_global_inference: true\n\
    \              \n        # Network controls\n        network:\n          - vpc_endpoints:\
    \ \"Use PrivateLink, no internet\"\n          - nacls: \"Restrict outbound to\
    \ approved regions\"\n      ```\n      \n      **EU AI Act compliance checklist:**\n\
    \      \n      ```yaml\n      eu_ai_act_compliance:\n        risk_classification:\n\
    \          - task: \"Classify AI system risk level\"\n            categories:\
    \ [\"Unacceptable\", \"High-risk\", \"Limited\", \"Minimal\"]\n            action:\
    \ \"Document classification rationale\"\n            \n        high_risk_requirements:\n\
    \          - \"Risk management system\"\n          - \"Data governance\"\n   \
    \       - \"Technical documentation\"\n          - \"Record-keeping\"\n      \
    \    - \"Transparency\"\n          - \"Human oversight\"\n          - \"Accuracy,\
    \ robustness, cybersecurity\"\n          \n        transparency_requirements:\n\
    \          - \"Inform users they're interacting with AI\"\n          - \"Label\
    \ AI-generated content\"\n          - \"Provide AI Service Cards\"\n         \
    \ \n        aws_support:\n          - \"ISO IEC 42001 certification\"\n      \
    \    - \"AI Service Cards\"\n          - \"Frontier model safety framework\"\n\
    \          - \"EU AI Pact signatory\"\n      ```\n      \n      **Compliance monitoring:**\n\
    \      \n      ```yaml\n      compliance_monitoring:\n        automated_checks:\n\
    \          - \"Data flow logging (CloudTrail)\"\n          - \"Region compliance\
    \ (AWS Config rules)\"\n          - \"Model usage tracking (inference profiles)\"\
    \n          - \"Guardrail effectiveness\"\n          \n        regular_audits:\n\
    \          - \"Quarterly compliance review\"\n          - \"Annual third-party\
    \ audit\"\n          - \"Regulatory update tracking\"\n          \n        alerts:\n\
    \          - \"Data flow outside approved regions\"\n          - \"Unapproved\
    \ model access\"\n          - \"Guardrail bypass attempts\"\n      ```\n     \
    \ \n      **PALETTE integration:**\n      - Document regional requirements in\
    \ RIU-530 (AI Governance Config)\n      - Configure guardrails per region in RIU-531\
    \ (Guardrail Selection)\n      - Track compliance in RIU-120 (Integration Mode\
    \ Selection)\n      - Include in Deployment Readiness (RIU-060) for each region\n\
    \      \n      Key insight: Design for the strictest regulation first (usually\
    \ EU), then relax controls where other regions permit. It's easier to loosen restrictions\
    \ than to retrofit compliance into a permissive architecture."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-060
  - RIU-120
  - RIU-530
  - RIU-531
  difficulty: critical
  industries:
  - Enterprise SaaS
  - Finance
  - Healthcare
  tags:
  - multi-region
  - compliance
  - localization
  - architecture
  sources:
  - title: 'Building trust in AI: The AWS approach to the EU AI Act'
    url: https://aws.amazon.com/blogs/machine-learning/building-trust-in-ai-the-aws-approach-to-the-eu-ai-act/
  - title: 'Unlocking AI flexibility in Switzerland: Cross-region inference for EU
      data processing'
    url: https://aws.amazon.com/blogs/alps/unlocking-ai-flexibility-in-switzerland-a-guide-to-cross-region-inference-for-eu-data-processing-and-model-access/
  - title: 'Securing Amazon Bedrock cross-Region inference: Geographic and global'
    url: https://aws.amazon.com/blogs/machine-learning/securing-amazon-bedrock-cross-region-inference-geographic-and-global/
  - title: Implement RAG while meeting data residency requirements using AWS hybrid
      and edge services
    url: https://aws.amazon.com/blogs/machine-learning/implement-rag-while-meeting-data-residency-requirements-using-aws-hybrid-and-edge-services/
  - title: Regulatory Compliance and Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_5_security_privacy/3_5_3_compliance_data_protection/3_5_3-2_regulatory_governance/regulatory_governance.html
- id: LIB-062
  question: What's the best pattern for handling customer-specific AI customizations
    at scale?
  answer: "Customer customizations at scale require a layered configuration approach:\
    \ shared platform â†’ customer overrides â†’ environment-specific settings. Don't\
    \ fork code â€” parameterize everything.\n      \n      **The customization challenge:**\n\
    \      ```\n      1 customer:   Hand-crafted configuration\n      10 customers:\
    \ Copy-paste configurations, drift begins\n      100 customers: Unmaintainable\
    \ mess\n      \n      Solution: Configuration inheritance with overrides\n   \
    \   ```\n      \n      **Customization layer architecture:**\n      \n      ```\n\
    \      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     \
    \ â”‚                    CONFIGURATION LAYERS                      â”‚\n      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \      â”‚                                                             â”‚\n     \
    \ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n      â”‚  â”‚ LAYER\
    \ 4: ENVIRONMENT OVERRIDES                        â”‚ â”‚\n      â”‚  â”‚ (dev/staging/prod\
    \ settings)                           â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚                          â–²                                  â”‚\n \
    \     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n      â”‚\
    \  â”‚ LAYER 3: CUSTOMER-SPECIFIC                            â”‚ â”‚\n      â”‚  â”‚ (customer\
    \ prompts, guardrails, KB, branding)          â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚                          â–²                                  â”‚\n \
    \     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n      â”‚\
    \  â”‚ LAYER 2: INDUSTRY/VERTICAL DEFAULTS                   â”‚ â”‚\n      â”‚  â”‚ (healthcare,\
    \ finance, retail templates)               â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚                          â–²                                  â”‚\n \
    \     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n      â”‚\
    \  â”‚ LAYER 1: PLATFORM DEFAULTS                            â”‚ â”‚\n      â”‚  â”‚ (base\
    \ prompts, guardrails, models)                    â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚                                                             â”‚\n \
    \     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      \n\
    \      Resolution: Layer 4 â†’ Layer 3 â†’ Layer 2 â†’ Layer 1 (first defined wins)\n\
    \      ```\n      \n      **What can be customized:**\n      \n      | Layer |\
    \ What's Customized | Example | Stored In |\n      |-------|-------------------|---------|-----------|\n\
    \      | **Prompts** | System prompts, templates | \"You are a {company} assistant...\"\
    \ | Prompt Registry |\n      | **Guardrails** | Safety filters, blocked topics\
    \ | Industry-specific compliance | Bedrock Guardrails |\n      | **Knowledge Base**\
    \ | Customer-specific content | Product docs, policies | Per-tenant S3/OpenSearch\
    \ |\n      | **Model Selection** | Model preferences | Claude vs. Nova | Config\
    \ DB |\n      | **Branding** | Tone, terminology | Company voice guide | Config\
    \ DB |\n      | **Integrations** | External systems | CRM, ticketing connections\
    \ | Secrets Manager |\n      | **Thresholds** | Confidence, escalation | When\
    \ to involve humans | Config DB |\n      \n      **Configuration storage pattern:**\n\
    \      \n      ```yaml\n      customer_configuration:\n        # Stored in DynamoDB\
    \ / Parameter Store\n        customer_id: \"acme-corp\"\n        \n        # Inheritance\n\
    \        inherits_from: \"healthcare\"  # Industry template\n        \n      \
    \  # Prompt customizations\n        prompts:\n          system_prompt_override:\
    \ |\n            You are ACME Corp's healthcare assistant. \n            Always\
    \ recommend consulting a physician for medical advice.\n            Use these\
    \ approved product names: {product_list}\n          \n          greeting_override:\
    \ \"Welcome to ACME Health Support!\"\n          \n        # Guardrail customizations\n\
    \        guardrails:\n          guardrail_id: \"acme-healthcare-guardrail\"\n\
    \          blocked_topics:\n            - \"competitor products\"\n          \
    \  - \"off-label drug use\"\n          required_disclaimers:\n            - \"\
    This is not medical advice\"\n            \n        # Knowledge base\n       \
    \ knowledge_base:\n          kb_id: \"acme-kb-prod\"\n          s3_prefix: \"\
    s3://kb-bucket/acme-corp/\"\n          \n        # Model preferences\n       \
    \ model:\n          primary: \"anthropic.claude-3-sonnet\"\n          fallback:\
    \ \"amazon.nova-pro\"\n          \n        # Thresholds\n        thresholds:\n\
    \          confidence_for_auto_response: 0.85\n          escalation_after_turns:\
    \ 5\n          \n        # Feature flags\n        features:\n          enable_product_recommendations:\
    \ true\n          enable_appointment_scheduling: false\n      ```\n      \n  \
    \    **Runtime configuration resolution:**\n      \n      ```python\n      def\
    \ get_config(customer_id: str, key: str) -> Any:\n          \"\"\"\n         \
    \ Resolve configuration with inheritance.\n          Customer â†’ Industry â†’ Platform\
    \ defaults\n          \"\"\"\n          # Layer 3: Customer-specific\n       \
    \   customer_config = config_store.get(f\"customer/{customer_id}\")\n        \
    \  if key in customer_config:\n              return customer_config[key]\n   \
    \       \n          # Layer 2: Industry defaults\n          industry = customer_config.get(\"\
    inherits_from\")\n          if industry:\n              industry_config = config_store.get(f\"\
    industry/{industry}\")\n              if key in industry_config:\n           \
    \       return industry_config[key]\n          \n          # Layer 1: Platform\
    \ defaults\n          platform_config = config_store.get(\"platform/defaults\"\
    )\n          return platform_config.get(key)\n      \n      # Usage\n      system_prompt\
    \ = get_config(\"acme-corp\", \"prompts.system_prompt\")\n      guardrail_id =\
    \ get_config(\"acme-corp\", \"guardrails.guardrail_id\")\n      ```\n      \n\
    \      **Multi-tenant isolation patterns:**\n      \n      | Pattern | Customization\
    \ Isolation | Resource Efficiency | Use When |\n      |---------|------------------------|---------------------|----------|\n\
    \      | **Siloed** | Separate infrastructure per customer | Low | Regulated,\
    \ enterprise |\n      | **Pooled** | Shared infra, config-based customization\
    \ | High | Standard customers |\n      | **Hybrid** | Shared compute, isolated\
    \ data/KB | Medium | Most common |\n      \n      ```yaml\n      hybrid_pattern:\n\
    \        shared:\n          - \"Model endpoints (Bedrock)\"\n          - \"Inference\
    \ compute\"\n          - \"Deployment pipeline\"\n          - \"Monitoring infrastructure\"\
    \n          \n        per_customer:\n          - \"Configuration (DynamoDB)\"\n\
    \          - \"Knowledge base content (S3)\"\n          - \"Conversation history\"\
    \n          - \"Guardrail rules\"\n          - \"Usage tracking (inference profiles)\"\
    \n      ```\n      \n      **Deploying customizations:**\n      \n      ```yaml\n\
    \      customization_deployment:\n        # Separate from application deployment\n\
    \        pipeline:\n          trigger: \"Config change in repo\"\n          \n\
    \          stages:\n            - name: \"Validate\"\n              actions:\n\
    \                - \"Schema validation\"\n                - \"Prompt syntax check\"\
    \n                - \"Guardrail compatibility\"\n                \n          \
    \  - name: \"Test\"\n              actions:\n                - \"Run against customer's\
    \ golden set\"\n                - \"Verify guardrails activate appropriately\"\
    \n                - \"Check for regressions\"\n                \n            -\
    \ name: \"Deploy\"\n              actions:\n                - \"Update config\
    \ store (DynamoDB)\"\n                - \"Invalidate caches\"\n              \
    \  - \"Update guardrail if changed\"\n                \n            - name: \"\
    Verify\"\n              actions:\n                - \"Smoke test with customer\
    \ context\"\n                - \"Monitor for errors\"\n      ```\n      \n   \
    \   **Testing customizations at scale:**\n      \n      ```yaml\n      customization_testing:\n\
    \        per_customer_tests:\n          - \"Golden set specific to customer\"\n\
    \          - \"Brand voice validation\"\n          - \"Guardrail effectiveness\"\
    \n          - \"Integration connectivity\"\n          \n        cross_customer_tests:\n\
    \          - \"Platform regression suite\"\n          - \"Performance benchmarks\"\
    \n          - \"Cost projections\"\n          \n        automation:\n        \
    \  - \"Run customer tests on config change\"\n          - \"Weekly full regression\
    \ across all customers\"\n          - \"A/B testing infrastructure for prompt\
    \ experiments\"\n      ```\n      \n      **Self-service customization portal:**\n\
    \      \n      ```yaml\n      self_service_capabilities:\n        # What customers\
    \ can customize themselves\n        tier_1_self_service:\n          - \"Greeting\
    \ and sign-off messages\"\n          - \"Product/service list updates\"\n    \
    \      - \"FAQ content in knowledge base\"\n          - \"Basic tone adjustments\"\
    \n          \n        tier_2_assisted:\n          - \"System prompt modifications\"\
    \n          - \"Custom guardrail rules\"\n          - \"Integration configurations\"\
    \n          \n        tier_3_platform_team:\n          - \"Model selection changes\"\
    \n          - \"New capability enablement\"\n          - \"Custom fine-tuning\"\
    \n      ```\n      \n      **PALETTE integration:**\n      - Document customization\
    \ options in RIU-044 (Business Rules Documentation)\n      - Store templates in\
    \ RIU-121 (Deployment Template)\n      - Track per-customer configs in RIU-120\
    \ (Integration Mode Selection)\n      - Version prompts per customer in RIU-520\
    \ (Prompt Version Control)\n      \n      Key insight: Every customer-specific\
    \ code path is technical debt. Instead: one codebase, many configurations. The\
    \ platform team maintains the engine; customers configure the behavior."
  problem_type: Operationalization_and_Scaling
  related_rius:
  
  - RIU-120
  - RIU-121
  - RIU-520
  difficulty: critical
  industries:
  - Enterprise SaaS
  - All
  tags:
  - customization
  - multi-tenancy
  - configuration
  - scaling
  sources:
  - title: Building multi-tenant architectures for agentic AI on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/agentic-ai-multitenant/introduction.html
  - title: Building generative AI applications - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_applicationbuilding.html
  - title: 'Tailored support at scale: Turning a unified Salesforce KB into LOB-focused
      AI agents'
    url: https://aws.amazon.com/blogs/contact-center/tailored-support-at-scale-turning-a-unified-salesforce-kb-into-lob-focused-ai-agents/
  - title: 'Advanced fine-tuning techniques for multi-agent orchestration: Patterns
      from Amazon at scale'
    url: https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-techniques-for-multi-agent-orchestration-patterns-from-amazon-at-scale/
- id: LIB-063
  question: How do I prevent hero-driven AI operations from becoming a bottleneck?
  answer: "Hero-driven operations feel efficient until the hero is unavailable. Prevent\
    \ bottlenecks by distributing knowledge, automating tribal knowledge, and designing\
    \ for team resilience.\n      \n      **The hero problem:**\n      ```\n     \
    \ Hero available:     Everything works smoothly\n      Hero on vacation:   Issues\
    \ pile up, anxiety rises\n      Hero leaves:        Knowledge walks out the door\n\
    \      \n      Bus factor = 1 is a scaling and resilience failure\n      ```\n\
    \      \n      **Diagnose hero dependencies:**\n      \n      | Warning Sign |\
    \ What It Means | Action |\n      |--------------|---------------|--------|\n\
    \      | \"Only X knows how to...\" | Single point of knowledge | Document + cross-train\
    \ |\n      | Tickets wait for specific person | Bottleneck dependency | Distribute\
    \ expertise |\n      | Hero works nights/weekends | Unsustainable workload | Add\
    \ capacity + automate |\n      | No one else volunteers for on-call | Skill/confidence\
    \ gap | Training + pairing |\n      | Documentation says \"ask X\" | Missing documentation\
    \ | Hero writes docs |\n      | Hero gets paged for everything | Alert routing\
    \ issue | Tier alerts, train others |\n      \n      **Hero assessment checklist:**\n\
    \      ```yaml\n      hero_assessment:\n        questions:\n          - \"Can\
    \ someone else resolve this if hero is unavailable?\"\n          - \"Is there\
    \ documentation sufficient for someone else to follow?\"\n          - \"Has anyone\
    \ else successfully performed this task?\"\n          - \"Would hero's departure\
    \ cause significant disruption?\"\n          \n        score:\n          - 0-1\
    \ \"yes\": \"Critical hero dependency - immediate action\"\n          - 2 \"yes\"\
    : \"Moderate dependency - plan mitigation\"\n          - 3-4 \"yes\": \"Healthy\
    \ distribution - maintain\"\n      ```\n      \n      **Strategy 1: Document tribal\
    \ knowledge**\n      \n      ```yaml\n      documentation_requirements:\n    \
    \    for_every_system:\n          - \"Architecture overview (what it does, how\
    \ it works)\"\n          - \"Operational runbook (LIB-045, LIB-056)\"\n      \
    \    - \"Troubleshooting guide (common issues + solutions)\"\n          - \"Escalation\
    \ contacts (not just the hero)\"\n          \n        for_every_process:\n   \
    \       - \"Step-by-step instructions (anyone can follow)\"\n          - \"Decision\
    \ criteria (when to do what)\"\n          - \"Common variations and exceptions\"\
    \n          \n        validation:\n          - \"Someone other than author follows\
    \ the doc successfully\"\n          - \"Doc reviewed and updated quarterly\"\n\
    \      ```\n      \n      **Strategy 2: Cross-training program**\n      \n   \
    \   ```yaml\n      cross_training:\n        pairing_rotations:\n          - \"\
    Hero pairs with different team member each sprint\"\n          - \"Pair handles\
    \ incidents together\"\n          - \"Pair takes turns leading\"\n          \n\
    \        shadow_on_call:\n          - \"Non-hero shadows hero during on-call\"\
    \n          - \"Hero explains thinking during incidents\"\n          - \"Shadow\
    \ handles next similar incident\"\n          \n        teaching_assignments:\n\
    \          - \"Hero creates training materials\"\n          - \"Hero runs lunch-and-learn\
    \ sessions\"\n          - \"Hero mentors backup designates\"\n          \n   \
    \     validation:\n          - \"Backup successfully handles incident without\
    \ hero\"\n          - \"Backup can deploy changes independently\"\n          -\
    \ \"Backup passes competency assessment\"\n      ```\n      \n      **Strategy\
    \ 3: On-call rotation design**\n      \n      ```yaml\n      on_call_design:\n\
    \        rotation_principles:\n          - \"Minimum 3 people in rotation (bus\
    \ factor > 1)\"\n          - \"Equal distribution of load\"\n          - \"Clear\
    \ escalation paths\"\n          - \"No perpetual on-call for anyone\"\n      \
    \    \n        tiered_response:\n          tier_1:\n            who: \"Rotating\
    \ generalist\"\n            handles: \"Common issues with runbook\"\n        \
    \    escalates_to: \"Tier 2 if unresolved in 30 min\"\n            \n        \
    \  tier_2:\n            who: \"Subject matter experts (rotating)\"\n         \
    \   handles: \"Complex issues, novel problems\"\n            escalates_to: \"\
    Tier 3 for critical/prolonged\"\n            \n          tier_3:\n           \
    \ who: \"Team leads / architects\"\n            handles: \"Major incidents, escalations\"\
    \n            \n        training_requirements:\n          before_joining_rotation:\n\
    \            - \"Complete system training module\"\n            - \"Shadow 2 on-call\
    \ shifts\"\n            - \"Handle 3 supervised incidents\"\n            - \"\
    Demonstrate runbook competency\"\n      ```\n      \n      **Strategy 4: Automate\
    \ hero tasks**\n      \n      | Hero Task | Automation Approach | Tool |\n   \
    \   |-----------|---------------------|------|\n      | \"Manual deployment\"\
    \ | CI/CD pipeline | CodePipeline |\n      | \"Check system health\" | Automated\
    \ monitoring | CloudWatch |\n      | \"Diagnose issues\" | AI-assisted triage\
    \ | DevOps Guru, Q |\n      | \"Scale resources\" | Auto-scaling | EKS/Lambda\
    \ auto-scale |\n      | \"Generate reports\" | Scheduled automation | EventBridge\
    \ + Lambda |\n      | \"Answer common questions\" | Self-service docs | Wiki,\
    \ chatbot |\n      \n      ```yaml\n      automation_priorities:\n        # Automate\
    \ in order of hero time consumed\n        1_deployment: \"Hero shouldn't be required\
    \ for deploys\"\n        2_monitoring: \"Alerts should be actionable by anyone\"\
    \n        3_common_fixes: \"Runbook automation for recurring issues\"\n      \
    \  4_reporting: \"Scheduled, not manual\"\n        5_triage: \"AI-assisted to\
    \ reduce expertise required\"\n      ```\n      \n      **Strategy 5: Organizational\
    \ structure**\n      \n      ```yaml\n      hybrid_coe_model:\n        central_platform_team:\n\
    \          role: \"Set standards, build tools, handle escalations\"\n        \
    \  staffing: \"Multiple people per specialty\"\n          \n        business_unit_specialists:\n\
    \          role: \"Day-to-day operations, customer-specific\"\n          relationship:\
    \ \"Trained by central, escalate to central\"\n          \n        knowledge_flow:\n\
    \          - \"Central â†’ BU: Standards, training, tools\"\n          - \"BU â†’\
    \ Central: Feedback, patterns, escalations\"\n          \n        benefit: \"\
    No single team member is critical path\"\n      ```\n      \n      **Building\
    \ a learning culture (PostNL 5 tips):**\n      \n      1. **Create momentum**:\
    \ Kick-off events, dedicated brand for learning\n      2. **Make it relevant**:\
    \ Training tied to daily work, not abstract\n      3. **Recognize and empower**:\
    \ Celebrate knowledge sharing\n      4. **Encourage collaboration**: Cross-team\
    \ learning sessions\n      5. **Gamify**: Badges, leaderboards, friendly competition\n\
    \      \n      **Metrics to track hero reduction:**\n      \n      | Metric |\
    \ Hero State | Healthy State |\n      |--------|------------|---------------|\n\
    \      | On-call escalations to specific person | >50% | <20% |\n      | Docs\
    \ marked \"ask X\" | Many | Zero |\n      | People who can deploy | 1-2 | All\
    \ team |\n      | People who handled incident this quarter | 1-2 | All rotation\
    \ |\n      | Single points of failure documented | Unknown | Zero |\n      \n\
    \      **PALETTE integration:**\n      - Document knowledge in RIU-069 (Runbook)\n\
    \      - Track competencies in RIU-004 (Workstream planning)\n      - Design rotation\
    \ in RIU-102 (Escalation Matrix)\n      - Store templates in RIU-121 (Deployment\
    \ Template)\n      \n      Key insight: Heroes aren't the problem â€” undocumented\
    \ heroes are. The goal isn't to eliminate expertise; it's to ensure expertise\
    \ is shared, documented, and backed up. A great hero builds systems that don't\
    \ need them."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-004
  - RIU-069
  - RIU-102
  - RIU-120
  - RIU-121
  - RIU-122
  difficulty: high
  industries:
  - All
  tags:
  - knowledge-transfer
  - bus-factor
  - documentation
  - process
  sources:
  - title: 'PostNL: 5 tips to help drive a culture of cloud learning and knowledge
      sharing'
    url: https://aws.amazon.com/blogs/training-and-certification/postnl-5-tips-to-help-drive-a-culture-of-cloud-learning-and-knowledge-sharing/
  - title: Delivering operational insights directly to your on-call team with DevOps
      Guru and Opsgenie
    url: https://aws.amazon.com/blogs/machine-learning/delivering-operational-insights-directly-to-your-on-call-team-by-integrating-amazon-devops-guru-with-atlassian-opsgenie/
  - title: 'Letting Go: Enabling Autonomy in Teams'
    url: https://aws.amazon.com/blogs/enterprise-strategy/letting-go-enabling-autonomy-in-teams/
  - title: How BMW Group breaks down knowledge silos with Amazon QuickSight
    url: https://aws.amazon.com/blogs/business-intelligence/how-bmw-group-breaks-down-knowledge-silos-with-amazon-quick-sight/
- id: LIB-064
  question: What automation reduces AI operational costs without sacrificing quality?
  answer: "Cost optimization without quality loss requires automation at multiple\
    \ layers: prompt efficiency, smart routing, caching, model selection, and infrastructure.\
    \ Measure quality continuously â€” cost savings mean nothing if outputs degrade.\n\
    \      \n      **Cost optimization layers:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                  COST OPTIMIZATION STACK                     â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \ â”‚\n      â”‚  â”‚ LAYER 5: INFRASTRUCTURE                               â”‚ â”‚\n  \
    \    â”‚  â”‚ GPU sharing, auto-scaling, spot instances             â”‚ â”‚\n      â”‚ \
    \ â”‚ Potential savings: 50-90%                             â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n  \
    \    â”‚  â”‚ LAYER 4: MODEL SELECTION                              â”‚ â”‚\n      â”‚ \
    \ â”‚ Right-size models, smaller for simple tasks           â”‚ â”‚\n      â”‚  â”‚ Potential\
    \ savings: 30-70%                             â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n  \
    \    â”‚  â”‚ LAYER 3: CACHING                                      â”‚ â”‚\n      â”‚ \
    \ â”‚ Prompt caching, semantic cache                        â”‚ â”‚\n      â”‚  â”‚ Potential\
    \ savings: 50-90%                             â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n  \
    \    â”‚  â”‚ LAYER 2: SMART ROUTING                                â”‚ â”‚\n      â”‚ \
    \ â”‚ Route by complexity, batch similar requests           â”‚ â”‚\n      â”‚  â”‚ Potential\
    \ savings: 20-30%                             â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n  \
    \    â”‚  â”‚ LAYER 1: PROMPT EFFICIENCY                            â”‚ â”‚\n      â”‚ \
    \ â”‚ Concise prompts, decomposition, compression           â”‚ â”‚\n      â”‚  â”‚ Potential\
    \ savings: 20-40%                             â”‚ â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \ â”‚\n      â”‚                                                             â”‚\n \
    \     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      ```\n\
    \      \n      **Layer 1: Prompt efficiency automation**\n      \n      | Technique\
    \ | Savings | Quality Impact | Implementation |\n      |-----------|---------|----------------|----------------|\n\
    \      | Prompt compression | 20-40% tokens | Monitor closely | Automated prompt\
    \ optimizer |\n      | Remove redundancy | 10-20% tokens | None | Template review\
    \ |\n      | Structured output | 15-25% output tokens | Improved | JSON/XML mode\
    \ |\n      | Prompt decomposition | 20-30% | May improve | Multi-step pipelines\
    \ |\n      \n      ```yaml\n      prompt_optimization:\n        automated_compression:\n\
    \          tool: \"Prompt optimizer in CI/CD\"\n          action: \"Flag prompts\
    \ with >1000 tokens for review\"\n          \n        template_standards:\n  \
    \        - \"No filler phrases ('I want you to...', 'Please...')\"\n         \
    \ - \"Use structured output formats\"\n          - \"Reuse system prompts across\
    \ requests\"\n          \n        decomposition:\n          pattern: \"Break complex\
    \ tasks into simpler subtasks\"\n          benefit: \"Use smaller/cheaper models\
    \ for simple parts\"\n      ```\n      \n      **Layer 2: Smart routing automation**\n\
    \      \n      ```yaml\n      intelligent_routing:\n        # Amazon Bedrock Intelligent\
    \ Prompt Routing\n        configuration:\n          model_family: \"anthropic.claude\"\
    \n          routing_criteria: \"complexity\"\n          potential_savings: \"\
    up to 30%\"\n          \n        routing_rules:\n          simple_queries:\n \
    \           criteria: \"Short input, factual answer\"\n            route_to: \"\
    claude-haiku / nova-lite\"\n            cost: \"$0.25/M tokens\"\n           \
    \ \n          complex_queries:\n            criteria: \"Long context, reasoning\
    \ required\"\n            route_to: \"claude-sonnet / nova-pro\"\n           \
    \ cost: \"$3/M tokens\"\n            \n          critical_queries:\n         \
    \   criteria: \"High-stakes, maximum quality needed\"\n            route_to: \"\
    claude-opus\"\n            cost: \"$15/M tokens\"\n      ```\n      \n      **Layer\
    \ 3: Caching automation**\n      \n      ```yaml\n      caching_strategies:\n\
    \        prompt_caching:\n          # Amazon Bedrock prompt caching\n        \
    \  benefit: \"Up to 90% cost reduction, 85% latency reduction\"\n          use_cases:\n\
    \            - \"Chatbots with uploaded documents\"\n            - \"Repeated\
    \ system prompts\"\n            - \"Long context windows\"\n          cache_duration:\
    \ \"5 minutes\"\n          implementation: \"Automatic for supported models\"\n\
    \          \n        semantic_caching:\n          # MemoryDB with vector search\n\
    \          benefit: \"Millisecond responses for similar queries\"\n          mechanism:\n\
    \            1: \"Embed incoming query\"\n            2: \"Search cache for similar\
    \ (cosine > 0.95)\"\n            3: \"Return cached response if match\"\n    \
    \        4: \"Otherwise, invoke model and cache\"\n          storage: \"Amazon\
    \ MemoryDB\"\n          \n        response_caching:\n          # For deterministic\
    \ queries\n          use_cases:\n            - \"FAQ-style questions\"\n     \
    \       - \"Data lookups\"\n            - \"Classification tasks\"\n         \
    \ ttl: \"Based on data freshness requirements\"\n      ```\n      \n      **Layer\
    \ 4: Model selection automation**\n      \n      | Use Case | Recommended Model\
    \ | Cost Level | Quality |\n      |----------|-------------------|------------|---------|\n\
    \      | Simple Q&A | Nova Lite / Haiku | $ | Good |\n      | General tasks |\
    \ Nova Pro / Sonnet | $$ | Very Good |\n      | Complex reasoning | Claude Opus\
    \ | $$$$ | Excellent |\n      | Embeddings | Titan Embed | $ | Good |\n      |\
    \ Image generation | Nova Canvas | $$ | Good |\n      \n      ```yaml\n      model_selection_automation:\n\
    \        function_calling_vs_agents:\n          function_calling:\n          \
    \  use_when: \"Structured, repetitive tasks\"\n            benefit: \"Single API\
    \ call, less tokens\"\n            cost: \"Lower\"\n            \n          agents:\n\
    \            use_when: \"Complex reasoning, multi-step\"\n            benefit:\
    \ \"Autonomous problem solving\"\n            cost: \"Higher (multiple calls)\"\
    \n            \n        hosting_decision:\n          per_token_pricing:\n    \
    \        use_when: \"Variable traffic, low-medium volume\"\n            \n   \
    \       self_hosted:\n            use_when: \"High volume, consistent traffic\"\
    \n            options: [\"EKS\", \"EC2\", \"SageMaker\"]\n            benefit:\
    \ \"Predictable costs at scale\"\n      ```\n      \n      **Layer 5: Infrastructure\
    \ automation**\n      \n      ```yaml\n      infrastructure_optimization:\n  \
    \      gpu_time_slicing:\n          benefit: \"Up to 12x cost reduction\"\n  \
    \        implementation: \"EKS with NVIDIA time-slicing\"\n          use_case:\
    \ \"Multiple models sharing GPU\"\n          \n        inference_optimization:\n\
    \          tool: \"SageMaker Inference Optimization Toolkit\"\n          techniques:\n\
    \            - \"Speculative decoding\"\n            - \"Quantization (INT8, FP8)\"\
    \n            - \"Model compilation\"\n          benefit: \"2x throughput, 50%\
    \ cost reduction\"\n          \n        auto_scaling:\n          pattern: \"Scale\
    \ to zero when idle\"\n          implementation: \"SageMaker serverless inference\"\
    \n          benefit: \"Pay only for actual usage\"\n          \n        spot_instances:\n\
    \          use_case: \"Batch processing, training\"\n          savings: \"Up to\
    \ 90%\"\n          caveat: \"Not for real-time inference\"\n      ```\n      \n\
    \      **Automation for cost control:**\n      \n      ```yaml\n      cost_sentry_system:\n\
    \        # Proactive cost management\n        components:\n          - \"Token\
    \ usage tracking per tenant/team\"\n          - \"Usage-based alarms\"\n     \
    \     - \"Consumption limits/quotas\"\n          - \"Automated throttling when\
    \ limits approached\"\n          \n        implementation:\n          - service:\
    \ \"Step Functions\"\n            role: \"Orchestration\"\n          - service:\
    \ \"Lambda\"\n            role: \"Cost calculations\"\n          - service: \"\
    DynamoDB\"\n            role: \"Usage tracking\"\n          - service: \"CloudWatch\"\
    \n            role: \"Alarms and dashboards\"\n            \n        alerts:\n\
    \          warning: \"80% of budget consumed\"\n          critical: \"95% of budget\
    \ consumed\"\n          action: \"Throttle or switch to cheaper model\"\n    \
    \  ```\n      \n      **Quality guardrails during optimization:**\n      \n  \
    \    ```yaml\n      quality_monitoring:\n        # Never optimize without measuring\
    \ quality\n        metrics_to_track:\n          - \"Quality score (evaluation\
    \ pipeline)\"\n          - \"User satisfaction (feedback)\"\n          - \"Task\
    \ completion rate\"\n          - \"Error rate\"\n          \n        optimization_rules:\n\
    \          - \"Any optimization that drops quality >5% is rejected\"\n       \
    \   - \"A/B test before full rollout\"\n          - \"Rollback if quality degrades\
    \ post-optimization\"\n          \n        continuous_evaluation:\n          -\
    \ \"Daily golden set evaluation\"\n          - \"Weekly quality review\"\n   \
    \       - \"Compare cost-per-successful-task, not just cost-per-token\"\n    \
    \  ```\n      \n      **PALETTE integration:**\n      - Implement routing in RIU-520\
    \ (Prompt/Model Config)\n      - Track costs in RIU-121 (Deployment Template)\n\
    \      - Monitor quality in RIU-540 (Evaluation Harness)\n      - Document optimization\
    \ decisions in decisions.md\n      \n      Key insight: The metric that matters\
    \ is cost-per-successful-outcome, not cost-per-token. A cheaper model that fails\
    \ 20% more often is more expensive overall. Optimize for efficiency, not just\
    \ cost."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-120
  - RIU-121
  - RIU-520
  - RIU-540
  difficulty: high
  industries:
  - All
  tags:
  - cost-optimization
  - automation
  - efficiency
  - quality
  sources:
  - title: Cost Optimization Strategy and Techniques - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_6_cost_optimization/3_6_3_cost_optimization_strategy/readme.html
  - title: Reduce costs and latency with Amazon Bedrock Intelligent Prompt Routing
      and prompt caching
    url: https://aws.amazon.com/blogs/aws/reduce-costs-and-latency-with-amazon-bedrock-intelligent-prompt-routing-and-prompt-caching-preview/
  - title: Effectively use prompt caching on Amazon Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/effectively-use-prompt-caching-on-amazon-bedrock/
  - title: Achieve up to 2x higher throughput while reducing costs by 50% with SageMaker
      inference optimization
    url: https://aws.amazon.com/blogs/machine-learning/achieve-up-to-2x-higher-throughput-while-reducing-costs-by-50-for-generative-ai-inference-on-amazon-sagemaker-with-the-new-inference-optimization-toolkit-part-1/
  - title: Build a proactive AI cost management system for Amazon Bedrock
    url: https://aws.amazon.com/blogs/machine-learning/build-a-proactive-ai-cost-management-system-for-amazon-bedrock-part-1/
- id: LIB-065
  question: How do I version control operational procedures for AI systems?
  answer: "Treat operational procedures like code: version control, review, test,\
    \ and deploy. Unversioned procedures lead to drift, confusion, and incidents.\n\
    \      \n      **What to version control:**\n      \n      | Artifact Type | Example\
    \ | Version Control Method |\n      |---------------|---------|------------------------|\n\
    \      | Runbooks | Incident response procedures | Git (markdown) |\n      | SOPs\
    \ | Daily operations guide | Git (markdown) |\n      | Prompts | System prompts,\
    \ templates | Git + Prompt Registry |\n      | Configurations | Model settings,\
    \ thresholds | Git (YAML/JSON) |\n      | Infrastructure | IaC templates | Git\
    \ (Terraform/CDK) |\n      | Dashboards | CloudWatch configs | Git (JSON) |\n\
    \      | Alerts | Alarm definitions | Git (YAML) |\n      \n      **Repository\
    \ structure:**\n      \n      ```\n      ai-operations/\n      â”œâ”€â”€ README.md \
    \                   # Overview and quick links\n      â”œâ”€â”€ CHANGELOG.md       \
    \          # Change history\n      â”‚\n      â”œâ”€â”€ runbooks/\n      â”‚   â”œâ”€â”€ incident-response/\n\
    \      â”‚   â”‚   â”œâ”€â”€ quality-degradation.md\n      â”‚   â”‚   â”œâ”€â”€ model-failure.md\n\
    \      â”‚   â”‚   â””â”€â”€ rag-retrieval-issues.md\n      â”‚   â”œâ”€â”€ deployment/\n      â”‚\
    \   â”‚   â”œâ”€â”€ standard-deployment.md\n      â”‚   â”‚   â”œâ”€â”€ rollback-procedure.md\n\
    \      â”‚   â”‚   â””â”€â”€ emergency-hotfix.md\n      â”‚   â””â”€â”€ maintenance/\n      â”‚  \
    \     â”œâ”€â”€ knowledge-base-update.md\n      â”‚       â””â”€â”€ model-version-upgrade.md\n\
    \      â”‚\n      â”œâ”€â”€ sops/\n      â”‚   â”œâ”€â”€ daily-operations.md\n      â”‚   â”œâ”€â”€ on-call-handbook.md\n\
    \      â”‚   â””â”€â”€ change-management.md\n      â”‚\n      â”œâ”€â”€ prompts/\n      â”‚   â”œâ”€â”€\
    \ system-prompts/\n      â”‚   â”‚   â”œâ”€â”€ v1.0.0/\n      â”‚   â”‚   â”œâ”€â”€ v1.1.0/\n    \
    \  â”‚   â”‚   â””â”€â”€ current -> v1.1.0\n      â”‚   â””â”€â”€ templates/\n      â”‚\n      â”œâ”€â”€\
    \ configurations/\n      â”‚   â”œâ”€â”€ guardrails/\n      â”‚   â”œâ”€â”€ thresholds/\n    \
    \  â”‚   â””â”€â”€ model-configs/\n      â”‚\n      â””â”€â”€ infrastructure/\n          â”œâ”€â”€ terraform/\n\
    \          â””â”€â”€ cdk/\n      ```\n      \n      **Version control workflow:**\n\
    \      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚              OPERATIONAL PROCEDURE LIFECYCLE                 â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n      â”‚  â”‚ CREATE  â”‚â”€â”€â–¶â”‚ REVIEW\
    \  â”‚â”€â”€â–¶â”‚  TEST   â”‚â”€â”€â–¶â”‚ PUBLISH â”‚    â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n      â”‚       â”‚             â”‚             â”‚            \
    \ â”‚          â”‚\n      â”‚   Branch      Pull Request   Validation    Merge to  \
    \     â”‚\n      â”‚   from main   + Approval     (if appl.)   main + tag      â”‚\n\
    \      â”‚                                                            â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Change management process:**\n      \n      ```yaml\n\
    \      change_management:\n        minor_changes:\n          examples: [\"Typo\
    \ fixes\", \"Clarifications\", \"Contact updates\"]\n          process:\n    \
    \        - \"Create branch\"\n            - \"Make changes\"\n            - \"\
    Submit PR\"\n            - \"1 reviewer approval\"\n            - \"Merge\"\n\
    \          turnaround: \"Same day\"\n          \n        major_changes:\n    \
    \      examples: [\"New procedures\", \"Process changes\", \"Threshold updates\"\
    ]\n          process:\n            - \"Create branch\"\n            - \"Make changes\"\
    \n            - \"Submit PR with description of impact\"\n            - \"2 reviewer\
    \ approvals (including subject matter expert)\"\n            - \"Test in staging\
    \ (if applicable)\"\n            - \"Merge + announce to team\"\n          turnaround:\
    \ \"1-3 days\"\n          \n        critical_changes:\n          examples: [\"\
    Security procedures\", \"Compliance updates\", \"Escalation paths\"]\n       \
    \   process:\n            - \"Create branch\"\n            - \"Make changes\"\n\
    \            - \"Submit PR with impact assessment\"\n            - \"Review by:\
    \ Tech lead, Security, Compliance (as applicable)\"\n            - \"Approval\
    \ from AI Governance Lead\"\n            - \"Staged rollout with communication\
    \ plan\"\n          turnaround: \"3-7 days\"\n      ```\n      \n      **Pull\
    \ request template:**\n      \n      ```markdown\n      ## Change Type\n     \
    \ - [ ] Minor (typo, clarification)\n      - [ ] Major (new procedure, process\
    \ change)\n      - [ ] Critical (security, compliance, escalation)\n      \n \
    \     ## Description\n      [What is being changed and why]\n      \n      ##\
    \ Impact\n      [Who is affected, what changes for them]\n      \n      ## Testing\n\
    \      - [ ] Reviewed by someone who will use this procedure\n      - [ ] Tested\
    \ steps (if applicable)\n      - [ ] Links updated and working\n      \n     \
    \ ## Rollout\n      - [ ] Team notified (Slack/email)\n      - [ ] Training needed?\
    \ [Yes/No]\n      - [ ] CHANGELOG updated\n      \n      ## Reviewers\n      -\
    \ Technical: @[name]\n      - SME: @[name] (if applicable)\n      - Governance:\
    \ @[name] (if critical)\n      ```\n      \n      **CHANGELOG format:**\n    \
    \  \n      ```markdown\n      # Changelog\n      \n      ## [2024-06-15] - v1.2.0\n\
    \      ### Added\n      - New runbook: RAG retrieval troubleshooting (runbooks/incident-response/rag-retrieval-issues.md)\n\
    \      - Emergency hotfix procedure (runbooks/deployment/emergency-hotfix.md)\n\
    \      \n      ### Changed\n      - Updated escalation contacts in on-call-handbook.md\n\
    \      - Revised quality degradation thresholds (lowered warning from 85% to 80%)\n\
    \      \n      ### Deprecated\n      - Old deployment procedure (use standard-deployment.md\
    \ instead)\n      \n      ## [2024-06-01] - v1.1.0\n      ...\n      ```\n   \
    \   \n      **Linking procedures to deployments:**\n      \n      ```yaml\n  \
    \    deployment_metadata:\n        deployment_id: \"deploy-2024-06-15-001\"\n\
    \        git_commit: \"abc123def456\"\n        \n        procedures_version:\n\
    \          runbooks: \"v1.2.0\"\n          prompts: \"v1.1.0\"\n          configurations:\
    \ \"v2.3.1\"\n          \n        links:\n          deployment_runbook: \"runbooks/deployment/standard-deployment.md@v1.2.0\"\
    \n          rollback_procedure: \"runbooks/deployment/rollback-procedure.md@v1.2.0\"\
    \n          incident_response: \"runbooks/incident-response/@v1.2.0\"\n      ```\n\
    \      \n      **Automated validation (CI/CD):**\n      \n      ```yaml\n    \
    \  procedure_validation:\n        on_pull_request:\n          - check: \"Markdown\
    \ linting\"\n            tool: \"markdownlint\"\n            \n          - check:\
    \ \"Link validation\"\n            tool: \"markdown-link-check\"\n           \
    \ \n          - check: \"Required sections present\"\n            tool: \"Custom\
    \ script\"\n            sections: [\"Purpose\", \"Prerequisites\", \"Steps\",\
    \ \"Rollback\", \"Contacts\"]\n            \n          - check: \"CHANGELOG updated\"\
    \n            tool: \"Custom script\"\n            \n        on_merge:\n     \
    \     - action: \"Tag release\"\n          - action: \"Update 'current' symlink\"\
    \n          - action: \"Notify team via Slack\"\n          - action: \"Update\
    \ documentation portal\"\n      ```\n      \n      **AWS Systems Manager integration:**\n\
    \      \n      ```yaml\n      ssm_automation:\n        # Convert markdown runbooks\
    \ to executable automation\n        pattern:\n          - \"Store runbooks in\
    \ Git (source of truth)\"\n          - \"Sync to SSM Documents for automation\"\
    \n          - \"Manual steps remain in markdown\"\n          - \"Automated steps\
    \ execute via SSM Runbooks\"\n          \n        benefits:\n          - \"Executable\
    \ procedures reduce human error\"\n          - \"Audit trail of procedure execution\"\
    \n          - \"Approval gates in SSM\"\n      ```\n      \n      **PALETTE integration:**\n\
    \      - Store procedures in RIU-069 (Runbook)\n      - Version prompts in RIU-520\
    \ (Prompt Version Control)\n      - Track configurations in RIU-532 (Model Registry)\n\
    \      - Link from RIU-060 (Deployment Readiness)\n      \n      Key insight:\
    \ The question isn't whether to version control procedures â€” it's whether you\
    \ can answer \"what version of this runbook was in effect when this incident happened?\"\
    \ If not, you have a traceability gap."
  problem_type: Operationalization_and_Scaling
  related_rius:
  - RIU-004
  - RIU-060
  - RIU-069
  - RIU-121
  - RIU-520
  - RIU-532
  difficulty: medium
  industries:
  - All
  tags:
  - version-control
  - sops
  - change-management
  - governance
  sources:
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
  - title: AI Ops Overview - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/index.html
  - title: Achieving Operational Excellence using automated playbook and runbook
    url: https://aws.amazon.com/blogs/mt/achieving-operational-excellence-using-automated-playbook-and-runbook/
- id: LIB-066
  question: How do I design audit trails for AI systems that satisfy compliance requirements?
  answer: "AI audit trails must answer: \"What decision was made, by what model, with\
    \ what inputs, under what configuration, and who approved it?\" Design for both\
    \ real-time monitoring and historical reconstruction.\n      \n      **What auditors\
    \ ask (design to answer these):**\n      \n      | Auditor Question | Required\
    \ Data | Source |\n      |------------------|---------------|--------|\n     \
    \ | \"What model made this decision?\" | Model ID, version, endpoint | Request\
    \ metadata |\n      | \"What was the input?\" | Full request payload | Request\
    \ logs |\n      | \"What was the output?\" | Full response | Response logs |\n\
    \      | \"What guardrails were applied?\" | Guardrail ID, actions taken | Guardrail\
    \ logs |\n      | \"Who had access?\" | IAM principals, roles | CloudTrail |\n\
    \      | \"What configuration was active?\" | Prompts, thresholds, settings |\
    \ Config audit |\n      | \"Was human oversight applied?\" | Approval records\
    \ | Workflow logs |\n      | \"Can this decision be reproduced?\" | All inputs\
    \ + config | Lineage tracking |\n      \n      **Audit log schema:**\n      \n\
    \      ```yaml\n      ai_audit_log:\n        # Request identification\n      \
    \  request_id: \"uuid-v4\"\n        trace_id: \"correlation-id-for-full-trace\"\
    \n        timestamp: \"2024-06-15T10:30:00Z\"\n        \n        # Who\n     \
    \   principal:\n          type: \"IAMUser | IAMRole | ServiceAccount\"\n     \
    \     arn: \"arn:aws:iam::123456789012:user/jane\"\n          source_ip: \"10.0.1.50\"\
    \n          user_agent: \"MyApp/1.0\"\n          \n        # What model\n    \
    \    model:\n          model_id: \"anthropic.claude-3-sonnet\"\n          endpoint:\
    \ \"arn:aws:bedrock:us-east-1::foundation-model/...\"\n          inference_profile:\
    \ \"customer-a-profile\"\n          \n        # Configuration at time of request\n\
    \        configuration:\n          prompt_version: \"v1.2.3\"\n          guardrail_id:\
    \ \"guardrail-abc123\"\n          guardrail_version: \"1\"\n          system_prompt_hash:\
    \ \"sha256:abc123...\"\n          \n        # Input (with PII handling)\n    \
    \    input:\n          type: \"text | structured\"\n          content_hash: \"\
    sha256:...\"  # Hash if PII\n          content: \"...\"  # Full content if permitted\n\
    \          token_count: 150\n          \n        # Output\n        output:\n \
    \         content_hash: \"sha256:...\"\n          content: \"...\"\n         \
    \ token_count: 500\n          finish_reason: \"end_turn\"\n          \n      \
    \  # Safety and guardrails\n        guardrail_result:\n          action: \"NONE\
    \ | BLOCKED | MODIFIED\"\n          triggered_policies:\n            - policy:\
    \ \"content-filter\"\n              severity: \"MEDIUM\"\n              action:\
    \ \"allowed\"\n            - policy: \"pii-filter\"\n              severity: \"\
    HIGH\"\n              action: \"redacted\"\n              \n        # Performance\n\
    \        metrics:\n          latency_ms: 1250\n          time_to_first_token_ms:\
    \ 350\n          \n        # Business context\n        context:\n          tenant_id:\
    \ \"acme-corp\"\n          application: \"customer-support-bot\"\n          use_case:\
    \ \"product-inquiry\"\n          environment: \"production\"\n          \n   \
    \     # Human oversight (if applicable)\n        human_oversight:\n          required:\
    \ true\n          approval_status: \"approved | pending | rejected\"\n       \
    \   approver: \"arn:aws:iam::...\"\n          approval_timestamp: \"2024-06-15T10:31:00Z\"\
    \n      ```\n      \n      **Regulatory requirements by framework:**\n      \n\
    \      | Regulation | Key Audit Requirements | Retention |\n      |------------|------------------------|-----------|\n\
    \      | **EU AI Act** | Decision traceability, risk assessments, human oversight\
    \ records | 10 years (high-risk) |\n      | **GDPR** | Data processing records,\
    \ consent, right to explanation | Duration of processing + years |\n      | **HIPAA**\
    \ | Access logs, PHI handling, breach records | 6 years |\n      | **SOX** | Financial\
    \ decision audit, controls evidence | 7 years |\n      | **SOC 2** | Access controls,\
    \ change management, incident response | Per audit period |\n      | **CCPA**\
    \ | Data access, deletion requests | 24 months |\n      \n      **AWS implementation\
    \ architecture:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                    AI AUDIT ARCHITECTURE                     â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n      â”‚  â”‚ Bedrock â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚\
    \ CloudWatch Logs (Request/Response)â”‚   â”‚\n      â”‚  â”‚  API    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \    â”‚\n      â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      â”‚                        â”‚\n\
    \      â”‚       â”‚                           â–¼                        â”‚\n      â”‚\
    \       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚      \
    \ â”‚              â”‚   S3 (Long-term retention)       â”‚   â”‚\n      â”‚       â”‚   \
    \           â”‚   - Immutable (Object Lock)      â”‚   â”‚\n      â”‚       â”‚        \
    \      â”‚   - Lifecycle policies           â”‚   â”‚\n      â”‚       â”‚             \
    \ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n      â”‚       â”‚                   \
    \                                  â”‚\n      â”‚       â–¼                        \
    \                             â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”‚\n      â”‚  â”‚          CloudTrail (API Activity)                   â”‚   â”‚\n\
    \      â”‚  â”‚  - All Bedrock API calls                            â”‚   â”‚\n      â”‚\
    \  â”‚  - IAM actions                                       â”‚   â”‚\n      â”‚  â”‚  -\
    \ Config changes                                    â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚\
    \  â”‚          AWS Audit Manager                           â”‚   â”‚\n      â”‚  â”‚  -\
    \ GenAI best practices framework                    â”‚   â”‚\n      â”‚  â”‚  - Evidence\
    \ collection                               â”‚   â”‚\n      â”‚  â”‚  - Compliance reports\
    \                                â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     \
    \ ```\n      \n      **Immutability and tamper-proofing:**\n      \n      ```yaml\n\
    \      immutability_controls:\n        s3_object_lock:\n          mode: \"GOVERNANCE\"\
    \  # or COMPLIANCE for stricter\n          retention_period: \"7 years\"\n   \
    \       purpose: \"Prevent deletion/modification\"\n          \n        cloudtrail:\n\
    \          log_file_validation: true  # Digest files for integrity\n         \
    \ kms_encryption: true\n          multi_region: true\n          \n        access_controls:\n\
    \          write: \"Automated systems only (no human write access)\"\n       \
    \   read: \"Auditors + compliance team\"\n          delete: \"Requires compliance\
    \ approval + waiting period\"\n          \n        integrity_verification:\n \
    \         - \"CloudTrail digest validation\"\n          - \"S3 object lock prevents\
    \ modification\"\n          - \"Checksums on all log entries\"\n      ```\n  \
    \    \n      **Audit Manager GenAI framework (8 principles):**\n      \n     \
    \ ```yaml\n      audit_manager_framework:\n        framework_id: \"generative-ai-best-practices\"\
    \n        \n        control_sets:\n          accuracy:\n            - \"Model\
    \ evaluation results documented\"\n            - \"Golden set test scores tracked\"\
    \n            \n          fairness:\n            - \"Bias testing conducted\"\n\
    \            - \"Demographic parity monitored\"\n            \n          privacy:\n\
    \            - \"PII handling documented\"\n            - \"Data retention policies\
    \ enforced\"\n            \n          resilience:\n            - \"Failover tested\"\
    \n            - \"Recovery procedures documented\"\n            \n          explainability:\n\
    \            - \"Decision rationale logged\"\n            - \"Model cards available\"\
    \n            \n          safety:\n            - \"Guardrails configured\"\n \
    \           - \"Content filtering active\"\n            \n          security:\n\
    \            - \"Access controls implemented\"\n            - \"Encryption enabled\"\
    \n            \n          sustainability:\n            - \"Resource usage tracked\"\
    \n            - \"Efficiency optimizations documented\"\n      ```\n      \n \
    \     **Query and retrieval patterns:**\n      \n      ```yaml\n      audit_queries:\n\
    \        # Common audit queries\n        by_user:\n          query: \"SELECT *\
    \ FROM audit_logs WHERE principal.arn = ?\"\n          use_case: \"Investigate\
    \ user activity\"\n          \n        by_time_range:\n          query: \"SELECT\
    \ * FROM audit_logs WHERE timestamp BETWEEN ? AND ?\"\n          use_case: \"\
    Incident investigation\"\n          \n        by_decision_outcome:\n         \
    \ query: \"SELECT * FROM audit_logs WHERE output.content LIKE '%denied%'\"\n \
    \         use_case: \"Review negative decisions\"\n          \n        guardrail_triggers:\n\
    \          query: \"SELECT * FROM audit_logs WHERE guardrail_result.action !=\
    \ 'NONE'\"\n          use_case: \"Safety review\"\n          \n        tools:\n\
    \          - \"Amazon Athena (S3 queries)\"\n          - \"CloudWatch Logs Insights\"\
    \n          - \"OpenSearch (full-text search)\"\n      ```\n      \n      **High-risk\
    \ AI (EU AI Act) additional requirements:**\n      \n      ```yaml\n      high_risk_ai_audit:\n\
    \        required_documentation:\n          - \"Risk management system documentation\"\
    \n          - \"Data governance records\"\n          - \"Technical documentation\"\
    \n          - \"Conformity assessment\"\n          - \"Human oversight procedures\"\
    \n          \n        logging_requirements:\n          - \"All decisions affecting\
    \ individuals\"\n          - \"Modifications to the system\"\n          - \"Performance\
    \ monitoring data\"\n          \n        retention: \"10 years minimum\"\n   \
    \     \n        access_for_authorities:\n          - \"Must be provided on request\"\
    \n          - \"Readable format\"\n          - \"Complete traceability\"\n   \
    \   ```\n      \n      **PALETTE integration:**\n      - Define audit requirements\
    \ in RIU-530 (AI Governance Config)\n      - Configure logging in RIU-531 (Guardrail\
    \ Selection)\n      - Document in RIU-140 (Training Materials) for compliance\
    \ team\n      - Track in RIU-534 (Audit Trail Config)\n      \n      Key insight:\
    \ Design audit trails for reconstruction, not just recording. An auditor should\
    \ be able to take any AI decision and reconstruct exactly why it happened â€” model\
    \ version, configuration, inputs, and any human oversight. If you can't reproduce\
    \ it, you can't defend it."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-140
  - RIU-530
  - RIU-531
  - RIU-534
  difficulty: critical
  industries:
  - Finance
  - Healthcare
  - Government
  tags:
  - audit-trails
  - compliance
  - governance
  - transparency
  sources:
  - title: Regulatory Compliance and Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_5_security_privacy/3_5_3_compliance_data_protection/3_5_3-2_regulatory_governance/regulatory_governance.html
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
  - title: Align and monitor your Amazon Bedrock chatbot with AWS Audit Manager
    url: https://aws.amazon.com/blogs/machine-learning/align-and-monitor-your-amazon-bedrock-powered-insurance-assistance-chatbot-to-responsible-ai-principles-with-aws-audit-manager/
  - title: Safeguard generative AI applications with Amazon Bedrock Guardrails
    url: https://aws.amazon.com/blogs/machine-learning/safeguard-generative-ai-applications-with-amazon-bedrock-guardrails/
- id: LIB-067
  question: What explainability is actually required for EU AI Act compliance?
  answer: "EU AI Act explainability requirements depend on risk classification. \"\
    Explainability\" means different things at each level: from simple disclosure\
    \ (\"this is AI\") to comprehensive decision traceability for high-risk systems.\n\
    \      \n      **EU AI Act risk classification:**\n      \n      | Risk Level\
    \ | Examples | Status |\n      |------------|----------|--------|\n      | **Unacceptable**\
    \ | Social scoring, real-time biometric ID in public | PROHIBITED |\n      | **High-Risk**\
    \ | Employment, credit, healthcare, education, critical infrastructure | REGULATED\
    \ |\n      | **Limited Risk** | Chatbots, emotion recognition, deepfakes | TRANSPARENCY\
    \ |\n      | **Minimal Risk** | Spam filters, AI-enhanced games | LARGELY UNREGULATED\
    \ |\n      \n      **Explainability requirements by risk level:**\n      \n  \
    \    ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚              EXPLAINABILITY REQUIREMENTS                     â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  UNACCEPTABLE\
    \ RISK                                          â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                                          â”‚\n      â”‚  N/A - These systems are\
    \ prohibited                         â”‚\n      â”‚                              \
    \                               â”‚\n      â”‚  HIGH-RISK                        \
    \                           â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                            \
    \                       â”‚\n      â”‚  âœ“ Technical documentation of system design\
    \                 â”‚\n      â”‚  âœ“ Human oversight mechanisms                   \
    \            â”‚\n      â”‚  âœ“ Decision traceability and logging                 \
    \       â”‚\n      â”‚  âœ“ Explanation capability for affected persons            \
    \  â”‚\n      â”‚  âœ“ Conformity assessment                                    â”‚\n\
    \      â”‚                                                             â”‚\n     \
    \ â”‚  LIMITED RISK                                                â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                                                â”‚\n      â”‚  âœ“ Disclosure that\
    \ user is interacting with AI              â”‚\n      â”‚  âœ“ Label AI-generated content\
    \ (deepfakes)                   â”‚\n      â”‚  âœ“ Notify emotion recognition use \
    \                          â”‚\n      â”‚                                        \
    \                     â”‚\n      â”‚  MINIMAL RISK                               \
    \                 â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   \
    \             â”‚\n      â”‚  â—‹ Voluntary codes of conduct                       \
    \        â”‚\n      â”‚  â—‹ Best practices encouraged                             \
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     \
    \ ```\n      \n      **High-Risk AI: What explainability actually means**\n  \
    \    \n      ```yaml\n      high_risk_explainability:\n        # Article 13 -\
    \ Transparency and provision of information\n        \n        technical_documentation:\n\
    \          required: true\n          must_include:\n            - \"General description\
    \ of the AI system\"\n            - \"Detailed description of elements and development\
    \ process\"\n            - \"Information on training data\"\n            - \"\
    Metrics used for accuracy, robustness, cybersecurity\"\n            - \"Human\
    \ oversight measures\"\n            - \"Expected lifetime and maintenance\"\n\
    \            \n        for_deployers:\n          # Organizations using high-risk\
    \ AI must understand it\n          required:\n            - \"Clear instructions\
    \ for use\"\n            - \"System capabilities and limitations\"\n         \
    \   - \"Circumstances that may impact performance\"\n            - \"Human oversight\
    \ procedures\"\n            - \"Technical measures for interpretation\"\n    \
    \        \n        for_affected_persons:\n          # People impacted by AI decisions\
    \ have rights\n          required:\n            - \"Right to explanation of individual\
    \ decision\"\n            - \"Meaningful information about logic involved\"\n\
    \            - \"Ability to contest decisions\"\n          practical_meaning:\n\
    \            - \"Why was this loan denied?\"\n            - \"Why was this candidate\
    \ rejected?\"\n            - \"Why was this claim flagged?\"\n            \n \
    \       logging_requirements:\n          # Automatic logging for traceability\n\
    \          must_log:\n            - \"Period of use\"\n            - \"Reference\
    \ database used\"\n            - \"Input data\"\n            - \"Output/decision\"\
    \n          retention: \"Appropriate to intended purpose\"\n      ```\n      \n\
    \      **Limited Risk: Transparency requirements**\n      \n      ```yaml\n  \
    \    limited_risk_transparency:\n        # Article 50 - Transparency obligations\n\
    \        \n        chatbots_virtual_assistants:\n          requirement: \"Inform\
    \ user they are interacting with AI\"\n          implementation:\n           \
    \ - \"Clear statement: 'You are chatting with an AI assistant'\"\n           \
    \ - \"Visible indicator in UI\"\n            - \"Not buried in terms of service\"\
    \n          exception: \"Obvious from context\"\n          \n        emotion_recognition:\n\
    \          requirement: \"Inform persons being analyzed\"\n          implementation:\n\
    \            - \"Notice before analysis begins\"\n            - \"Consent where\
    \ required\"\n            \n        deepfakes_synthetic_content:\n          requirement:\
    \ \"Label AI-generated content\"\n          implementation:\n            - \"\
    Machine-readable marking\"\n            - \"Disclosure that content is AI-generated\"\
    \n          exceptions: \"Artistic, satirical, or editorial use with safeguards\"\
    \n      ```\n      \n      **Implementation checklist by risk level:**\n     \
    \ \n      **High-Risk Compliance:**\n      ```yaml\n      high_risk_checklist:\n\
    \        # Must have all of these\n        documentation:\n          - \"System\
    \ design documentation complete\"\n          - \"Training data documented\"\n\
    \          - \"Performance metrics recorded\"\n          - \"Risk assessment conducted\"\
    \n          \n        technical_measures:\n          - \"Logging enabled for all\
    \ decisions\"\n          - \"Audit trail satisfies Article 12 requirements\"\n\
    \          - \"Human oversight mechanisms in place\"\n          - \"Ability to\
    \ generate explanations\"\n          \n        organizational_measures:\n    \
    \      - \"Designated personnel for oversight\"\n          - \"Procedures for\
    \ handling explanation requests\"\n          - \"Conformity assessment completed\"\
    \n          - \"EU database registration (where required)\"\n          \n    \
    \    ongoing_obligations:\n          - \"Post-market monitoring\"\n          -\
    \ \"Incident reporting procedures\"\n          - \"Regular compliance reviews\"\
    \n      ```\n      \n      **Limited-Risk Compliance:**\n      ```yaml\n     \
    \ limited_risk_checklist:\n        chatbots:\n          - \"AI disclosure implemented\
    \ in UI\"\n          - \"Disclosure visible before/during interaction\"\n    \
    \      \n        content_generation:\n          - \"AI-generated content labeled\"\
    \n          - \"Machine-readable watermarking (where feasible)\"\n          \n\
    \        documentation:\n          - \"Transparency measures documented\"\n  \
    \        - \"Evidence of compliance maintained\"\n      ```\n      \n      **Practical\
    \ explanation implementation:**\n      \n      ```yaml\n      explanation_approaches:\n\
    \        # Different approaches for different needs\n        \n        user_facing_explanation:\n\
    \          # What to tell end users\n          components:\n            - \"Plain\
    \ language summary of decision factors\"\n            - \"Key inputs that influenced\
    \ outcome\"\n            - \"How to contest or seek review\"\n          format:\
    \ \"Human-readable, accessible\"\n          example: |\n            \"Your application\
    \ was declined because:\n            - Income to debt ratio exceeded threshold\n\
    \            - Employment duration below minimum\n            To appeal, contact\
    \ support@...\"\n            \n        technical_explanation:\n          # For\
    \ auditors and compliance\n          components:\n            - \"Model version\
    \ and configuration\"\n            - \"Input features used\"\n            - \"\
    Decision confidence score\"\n            - \"Comparable approved/rejected cases\"\
    \n          format: \"Structured logs, queryable\"\n          \n        regulatory_explanation:\n\
    \          # For EU authorities\n          components:\n            - \"Full technical\
    \ documentation\"\n            - \"Conformity assessment\"\n            - \"Risk\
    \ management records\"\n            - \"Post-market monitoring results\"\n   \
    \       format: \"Per Annex IV requirements\"\n      ```\n      \n      **AWS\
    \ tools for compliance:**\n      \n      | Requirement | AWS Tool | How It Helps\
    \ |\n      |-------------|----------|--------------|\n      | Documentation |\
    \ AI Service Cards | Model capabilities and limitations |\n      | Logging | CloudTrail\
    \ + CloudWatch | Decision audit trail |\n      | Transparency | Bedrock model\
    \ info | Model provenance |\n      | Monitoring | SageMaker Model Monitor | Performance\
    \ tracking |\n      | Risk Assessment | AWS Audit Manager | GenAI best practices\
    \ framework |\n      \n      **Implementation timeline:**\n      \n      | Date\
    \ | What Takes Effect |\n      |------|-------------------|\n      | **Feb 2025**\
    \ | Prohibited AI practices banned |\n      | **Aug 2025** | GPAI model obligations\
    \ |\n      | **Aug 2026** | High-risk AI requirements (most) |\n      | **Aug\
    \ 2027** | High-risk AI in Annex I products |\n      \n      **Penalties for non-compliance:**\n\
    \      \n      | Violation | Maximum Fine |\n      |-----------|--------------|\n\
    \      | Prohibited AI practices | â‚¬35M or 7% global revenue |\n      | High-risk\
    \ AI non-compliance | â‚¬15M or 3% global revenue |\n      | Incorrect information\
    \ to authorities | â‚¬7.5M or 1% global revenue |\n      \n      **PALETTE integration:**\n\
    \      - Document risk classification in RIU-533 (FRIA - Fundamental Rights Impact\
    \ Assessment)\n      - Configure transparency measures in RIU-530 (AI Governance\
    \ Config)\n      - Implement logging per RIU-534 (Audit Trail Config)\n      -\
    \ Train team using RIU-140 (Training Materials)\n      \n      Key insight: \"\
    Explainability\" under EU AI Act isn't about technical XAI methods â€” it's about\
    \ providing meaningful information to users, deployers, and authorities. A simple,\
    \ clear explanation of why a decision was made is more compliant than a complex\
    \ SHAP analysis that no one understands."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-140
  - RIU-530
  - RIU-531
  - RIU-533
  - RIU-534
  difficulty: critical
  industries:
  - All EU operations
  tags:
  - eu-ai-act
  - explainability
  - compliance
  - regulation
  sources:
  - title: 'Building trust in AI: The AWS approach to the EU AI Act'
    url: https://aws.amazon.com/blogs/machine-learning/building-trust-in-ai-the-aws-approach-to-the-eu-ai-act/
  - title: Regulatory Compliance and Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_5_security_privacy/3_5_3_compliance_data_protection/3_5_3-2_regulatory_governance/regulatory_governance.html
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
  - title: 'Securing generative AI: data, compliance, and privacy considerations'
    url: https://aws.amazon.com/blogs/security/securing-generative-ai-data-compliance-and-privacy-considerations/
- id: LIB-068
  question: How do I handle resistance to AI automation from experienced employees?
  answer: "Resistance from experienced employees is rational â€” they've built careers\
    \ on expertise that AI seems to threaten. Address the real concerns (job security,\
    \ relevance, control) not just the stated ones (\"AI isn't accurate\").\n    \
    \  \n      **Types of resistance and what they really mean:**\n      \n      |\
    \ Stated Objection | Often Means | How to Address |\n      |------------------|-------------|----------------|\n\
    \      | \"AI isn't accurate enough\" | \"I'm worried about my job\" | Show augmentation,\
    \ not replacement |\n      | \"Our work is too complex for AI\" | \"I'm afraid\
    \ my expertise won't matter\" | Involve them as domain experts |\n      | \"Customers\
    \ won't accept it\" | \"I don't want to learn new tools\" | Demonstrate customer\
    \ benefits + training |\n      | \"We tried this before and it failed\" | \"I've\
    \ seen initiatives come and go\" | Acknowledge history, show what's different\
    \ |\n      | \"There's no time to learn this\" | \"I'm overwhelmed already\" |\
    \ Reduce workload first, then train |\n      \n      **Resistance personas and\
    \ strategies:**\n      \n      ```yaml\n      resistance_personas:\n        the_skeptic:\n\
    \          profile: \"Been here 15+ years, seen initiatives fail\"\n         \
    \ concerns: [\"This too shall pass\", \"Leadership doesn't understand our work\"\
    ]\n          strategy:\n            - \"Acknowledge past failures honestly\"\n\
    \            - \"Involve in pilot design (ownership)\"\n            - \"Show quick\
    \ wins in their workflow\"\n            - \"Make them the expert on what AI can't\
    \ do\"\n            \n        the_expert:\n          profile: \"Deep domain knowledge,\
    \ career built on expertise\"\n          concerns: [\"My knowledge is being devalued\"\
    , \"AI can't do what I do\"]\n          strategy:\n            - \"Position them\
    \ as AI trainers/validators\"\n            - \"Show AI handling routine work,\
    \ freeing them for complex cases\"\n            - \"Create 'expert review' role\
    \ in AI workflow\"\n            - \"Document their knowledge (they become even\
    \ more valuable)\"\n            \n        the_anxious:\n          profile: \"\
    Worried about job security, may not voice concerns\"\n          concerns: [\"\
    Will I be replaced?\", \"Can I learn this at my age?\"]\n          strategy:\n\
    \            - \"Explicit commitment on job security\"\n            - \"Personalized\
    \ training with patience\"\n            - \"Pair with supportive early adopter\"\
    \n            - \"Celebrate small wins publicly\"\n            \n        the_practical:\n\
    \          profile: \"Not ideologically opposed, but skeptical of ROI\"\n    \
    \      concerns: [\"Will this actually work?\", \"Is this worth my time?\"]\n\
    \          strategy:\n            - \"Show concrete metrics from pilot\"\n   \
    \         - \"Demonstrate time savings in their tasks\"\n            - \"Let them\
    \ choose which tasks to automate first\"\n            - \"Quick feedback loop\
    \ on results\"\n      ```\n      \n      **Change management framework:**\n  \
    \    \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚              RESISTANCE TO ADOPTION JOURNEY                  â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  PHASE\
    \ 1: ACKNOWLEDGE                                        â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                                        â”‚\n      â”‚  â€¢ Listen to concerns (not\
    \ dismiss)                         â”‚\n      â”‚  â€¢ Validate feelings (\"I understand...\"\
    )                    â”‚\n      â”‚  â€¢ Create safe spaces for feedback           \
    \               â”‚\n      â”‚                                                   \
    \          â”‚\n      â”‚  PHASE 2: INVOLVE                                      \
    \      â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         \
    \  â”‚\n      â”‚  â€¢ Recruit skeptics as advisors/reviewers                   â”‚\n\
    \      â”‚  â€¢ Give control over what gets automated                    â”‚\n     \
    \ â”‚  â€¢ Capture their expertise for AI training                  â”‚\n      â”‚   \
    \                                                          â”‚\n      â”‚  PHASE 3:\
    \ DEMONSTRATE                                        â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                                         â”‚\n      â”‚  â€¢ Pilot with receptive team\
    \ first                          â”‚\n      â”‚  â€¢ Show concrete benefits (time saved,\
    \ not jobs cut)        â”‚\n      â”‚  â€¢ Share success stories from peers        \
    \                 â”‚\n      â”‚                                                 \
    \            â”‚\n      â”‚  PHASE 4: SUPPORT                                    \
    \        â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       \
    \    â”‚\n      â”‚  â€¢ Training tailored to learning styles                     â”‚\n\
    \      â”‚  â€¢ Patient support during transition                        â”‚\n     \
    \ â”‚  â€¢ Celebrate adoption, not just results                     â”‚\n      â”‚   \
    \                                                          â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Specific tactics:**\n      \n      ```yaml\n     \
    \ engagement_tactics:\n        ai_ambassador_program:\n          description:\
    \ \"Recruit respected employees as AI champions\"\n          selection:\n    \
    \        - \"Mix of enthusiasts and respected skeptics\"\n            - \"Cross-functional\
    \ representation\"\n            - \"People others listen to\"\n          activities:\n\
    \            - \"Early access to AI tools\"\n            - \"Monthly sync with\
    \ AI team\"\n            - \"Peer training sessions\"\n            - \"Feedback\
    \ channel to leadership\"\n            \n        expert_involvement:\n       \
    \   description: \"Make domain experts part of the solution\"\n          roles:\n\
    \            - \"Validate AI outputs ('Is this correct?')\"\n            - \"\
    Define edge cases AI should escalate\"\n            - \"Train AI on their knowledge\"\
    \n            - \"Review and improve AI responses\"\n          benefit: \"They\
    \ become more valuable, not less\"\n          \n        gradual_introduction:\n\
    \          description: \"Start with augmentation, not automation\"\n        \
    \  sequence:\n            1: \"AI suggests, human decides\"\n            2: \"\
    AI drafts, human edits\"\n            3: \"AI handles routine, human handles exceptions\"\
    \n            4: \"AI autonomous for validated patterns\"\n          key: \"Human\
    \ always has control initially\"\n          \n        skills_investment:\n   \
    \       description: \"Visible commitment to employee growth\"\n          actions:\n\
    \            - \"Dedicated training time (not extra work)\"\n            - \"\
    Certifications and credentials\"\n            - \"Career paths that include AI\
    \ skills\"\n            - \"Promote AI-skilled employees visibly\"\n      ```\n\
    \      \n      **What NOT to do:**\n      \n      | Mistake | Why It Fails | Better\
    \ Approach |\n      |---------|--------------|-----------------|\n      | \"This\
    \ will make everyone more efficient\" | Sounds like \"we'll need fewer of you\"\
    \ | \"This handles X so you can focus on Y\" |\n      | Mandate adoption without\
    \ input | Creates resentment and sabotage | Involve in design, give choice |\n\
    \      | Dismiss concerns as \"fear of change\" | Invalidates real worries | Acknowledge\
    \ and address specifically |\n      | Launch big-bang rollout | Overwhelming,\
    \ no time to adapt | Phased rollout, learn as you go |\n      | Only celebrate\
    \ AI wins | Feels like pro-AI propaganda | Also celebrate human expertise |\n\
    \      \n      **Messaging that works:**\n      \n      ```yaml\n      effective_messaging:\n\
    \        do_say:\n          - \"AI will handle [routine task] so you can focus\
    \ on [complex/valuable task]\"\n          - \"Your expertise is needed to make\
    \ AI work correctly\"\n          - \"We're committed to training everyone, at\
    \ your pace\"\n          - \"You decide what AI helps with in your workflow\"\n\
    \          - \"AI makes mistakes â€” we need you to catch them\"\n          \n \
    \       dont_say:\n          - \"AI is the future, adapt or...\" (threatening)\n\
    \          - \"This is easy, anyone can learn it\" (dismissive)\n          - \"\
    We're doing this to increase efficiency\" (sounds like cuts)\n          - \"Trust\
    \ the AI\" (removes agency)\n      ```\n      \n      **Metrics for adoption success:**\n\
    \      \n      | Metric | What It Measures | Target |\n      |--------|------------------|--------|\n\
    \      | Active users / Licensed users | Actual adoption | >70% |\n      | Frequency\
    \ of use | Habit formation | Daily use by adopters |\n      | Feature utilization\
    \ | Depth of adoption | Key features used |\n      | Support tickets | Struggling\
    \ users | Decreasing over time |\n      | Employee sentiment | Attitude change\
    \ | Improving scores |\n      | Voluntary testimonials | Organic advocacy | Unsolicited\
    \ praise |\n      | Retention of experienced employees | Job security delivered\
    \ | No regrettable attrition |\n      \n      **PALETTE integration:**\n     \
    \ - Document change management plan in RIU-141 (Change Management Plan)\n    \
    \  - Train ambassadors using RIU-140 (Training Materials)\n      - Track stakeholder\
    \ engagement in RIU-042 (Stakeholder Map)\n      - Include in Convergence Brief\
    \ (RIU-001) for stakeholder concerns\n      \n      Key insight: The goal isn't\
    \ to overcome resistance â€” it's to transform resisters into advocates. Experienced\
    \ employees who become AI champions are far more credible than enthusiasts who\
    \ were always going to adopt anyway."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-001
  
  - RIU-140
  
  difficulty: high
  industries:
  - All
  tags:
  - change-management
  - adoption
  - resistance
  - stakeholder-management
  sources:
  - title: Change Management and Adoption for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_3_implementation_and_execution/5_3_2_change_management_and_adoption.html
  - title: Business Value and use cases - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/1_0_generative_ai_fundamentals/1_2_business_value_and_use_cases/1_2_business_value_and_use_cases.html
- id: LIB-069
  question: What governance framework prevents shadow AI processes from emerging?
  answer: "Shadow AI emerges when official channels are too slow, restrictive, or\
    \ hard to use. Prevent it with a combination of: easy-to-use sanctioned tools,\
    \ technical guardrails, monitoring for unauthorized use, and governance that enables\
    \ rather than blocks.\n      \n      **Why shadow AI happens:**\n      \n    \
    \  | Root Cause | Example | Prevention |\n      |------------|---------|------------|\n\
    \      | **Sanctioned tools are hard to access** | 3-week approval for ChatGPT\
    \ access | Self-service with guardrails |\n      | **Official process is too slow**\
    \ | IT backlog for AI projects | Fast-track for low-risk use |\n      | **Business\
    \ need isn't met** | \"We need X, IT only offers Y\" | Involve business in tool\
    \ selection |\n      | **Employees don't know tools exist** | \"I didn't know\
    \ we had an AI assistant\" | Marketing + training |\n      | **Rules seem unreasonable**\
    \ | \"Why can't I use AI for this?\" | Explain rationale, adjust if valid |\n\
    \      \n      **Three-pillar framework:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚           SHADOW AI PREVENTION FRAMEWORK                     â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”‚\n      â”‚  â”‚ PILLAR 1: MAKE SANCTIONED AI EASY & ATTRACTIVE      â”‚   â”‚\n\
    \      â”‚  â”‚ If official tools are better, people will use them  â”‚   â”‚\n      â”‚\
    \  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n      â”‚      \
    \                                                       â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”‚\n      â”‚  â”‚ PILLAR 2: IMPLEMENT TECHNICAL GUARDRAILS            â”‚   â”‚\n\
    \      â”‚  â”‚ Make unauthorized use difficult/impossible           â”‚   â”‚\n     \
    \ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n      â”‚    \
    \                                                         â”‚\n      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \   â”‚\n      â”‚  â”‚ PILLAR 3: DETECT & RESPOND TO SHADOW AI             â”‚   â”‚\n\
    \      â”‚  â”‚ Find it early, understand why, address root cause   â”‚   â”‚\n      â”‚\
    \  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n      â”‚      \
    \                                                       â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Pillar 1: Make sanctioned AI attractive**\n      \n\
    \      ```yaml\n      sanctioned_ai_design:\n        easy_access:\n          -\
    \ \"Self-service provisioning (no ticket required)\"\n          - \"SSO authentication\"\
    \n          - \"Available from day 1 for new employees\"\n          - \"Mobile\
    \ and desktop access\"\n          \n        meets_needs:\n          - \"Survey\
    \ users on what they need\"\n          - \"Include popular models (not just one)\"\
    \n          - \"Allow customization within guardrails\"\n          - \"Regular\
    \ feature updates based on feedback\"\n          \n        better_than_alternatives:\n\
    \          - \"Integrated with enterprise systems (CRM, etc.)\"\n          - \"\
    Pre-loaded with company knowledge\"\n          - \"No need to copy-paste sensitive\
    \ data\"\n          - \"Compliance handled automatically\"\n          \n     \
    \   visibility:\n          - \"Internal marketing campaign\"\n          - \"Training\
    \ included in onboarding\"\n          - \"Success stories from peers\"\n     \
    \     - \"Executive endorsement and use\"\n      ```\n      \n      **Pillar 2:\
    \ Technical guardrails**\n      \n      ```yaml\n      technical_controls:\n \
    \       network_controls:\n          - control: \"Block unauthorized AI services\"\
    \n            implementation: \"Web proxy/firewall rules\"\n            examples:\
    \ [\"Block ChatGPT\", \"Block Bard\", \"Allow only approved services\"]\n    \
    \        caveat: \"Can be bypassed via personal devices\"\n            \n    \
    \      - control: \"Centralized AI gateway\"\n            implementation: \"All\
    \ AI requests through managed gateway\"\n            benefits:\n             \
    \ - \"Logging and audit trail\"\n              - \"Content filtering\"\n     \
    \         - \"Cost controls\"\n              - \"Consistent guardrails\"\n   \
    \           \n        endpoint_controls:\n          - control: \"Browser extensions\"\
    \n            tool: \"SurePath AI or similar\"\n            capabilities:\n  \
    \            - \"Detect AI tool usage\"\n              - \"Warn before sensitive\
    \ data submission\"\n              - \"Redirect to sanctioned tools\"\n      \
    \        \n          - control: \"DLP integration\"\n            action: \"Detect\
    \ sensitive data sent to AI services\"\n            response: \"Block, alert,\
    \ or log\"\n            \n        identity_controls:\n          - control: \"\
    API access management\"\n            implementation: \"IAM policies restrict AI\
    \ service access\"\n            pattern: \"Allow only approved roles/groups\"\n\
    \            \n          - control: \"Service Control Policies (SCPs)\"\n    \
    \        implementation: \"Prevent creation of unauthorized AI resources\"\n \
    \           scope: \"AWS Organization level\"\n      ```\n      \n      **Pillar\
    \ 3: Detection and response**\n      \n      ```yaml\n      shadow_ai_detection:\n\
    \        monitoring_sources:\n          - source: \"Network traffic analysis\"\
    \n            detect: \"Connections to AI service domains\"\n            tools:\
    \ [\"Web proxy logs\", \"DNS logs\", \"CASB\"]\n            \n          - source:\
    \ \"Expense reports\"\n            detect: \"AI service subscriptions\"\n    \
    \        pattern: \"Employees expensing ChatGPT Plus, etc.\"\n            \n \
    \         - source: \"User surveys\"\n            detect: \"Self-reported tool\
    \ usage\"\n            approach: \"Anonymous, non-punitive\"\n            \n \
    \         - source: \"Endpoint monitoring\"\n            detect: \"AI browser\
    \ extensions, desktop apps\"\n            tools: [\"EDR\", \"Browser plugins\"\
    ]\n            \n        detection_alerts:\n          high: \"Sensitive data detected\
    \ going to unauthorized AI\"\n          medium: \"Repeated use of blocked AI services\"\
    \n          low: \"First-time attempt to access AI service\"\n          \n   \
    \     response_process:\n          1_understand:\n            - \"Why is this\
    \ person using shadow AI?\"\n            - \"What need isn't being met?\"\n  \
    \          - \"Is this a policy violation or policy gap?\"\n            \n   \
    \       2_address:\n            - \"If need is legitimate: fast-track sanctioned\
    \ alternative\"\n            - \"If policy gap: update policy\"\n            -\
    \ \"If violation: education first, escalation if repeated\"\n            \n  \
    \        3_prevent_recurrence:\n            - \"Improve sanctioned offering\"\n\
    \            - \"Better communicate available tools\"\n            - \"Technical\
    \ control if necessary\"\n      ```\n      \n      **Governance structure:**\n\
    \      \n      ```yaml\n      governance_structure:\n        ai_governance_board:\n\
    \          composition:\n            - \"Executive sponsor (decision authority)\"\
    \n            - \"IT/Security (technical implementation)\"\n            - \"Legal/Compliance\
    \ (regulatory requirements)\"\n            - \"Business representatives (user\
    \ needs)\"\n            - \"HR (training, policy communication)\"\n          \
    \  \n          responsibilities:\n            - \"Approve sanctioned AI tools\"\
    \n            - \"Define acceptable use policies\"\n            - \"Review shadow\
    \ AI incidents\"\n            - \"Balance enablement vs. risk\"\n            \n\
    \        policy_framework:\n          acceptable_use:\n            - \"What AI\
    \ tools are approved\"\n            - \"What data can be used with AI\"\n    \
    \        - \"What use cases are prohibited\"\n            \n          exception_process:\n\
    \            - \"How to request new tools/use cases\"\n            - \"Fast-track\
    \ for low-risk requests\"\n            - \"Escalation for high-risk requests\"\
    \n            \n          enforcement:\n            - \"First offense: education\"\
    \n            - \"Repeated offense: manager notification\"\n            - \"Willful\
    \ violation: HR escalation\"\n            - \"Focus: address root cause, not punish\"\
    \n      ```\n      \n      **Making governance enable, not block:**\n      \n\
    \      | Blocking Approach | Enabling Approach |\n      |-------------------|-------------------|\n\
    \      | \"AI is banned\" | \"Use our AI gateway\" |\n      | \"3-week approval\
    \ process\" | \"Self-service with guardrails\" |\n      | \"Only IT can use AI\"\
    \ | \"Everyone can use approved tools\" |\n      | \"Punish shadow AI users\"\
    \ | \"Understand why, fix the gap\" |\n      | \"Block all external AI\" | \"\
    Provide better internal alternative\" |\n      \n      **Metrics for shadow AI\
    \ prevention:**\n      \n      | Metric | What It Indicates | Target |\n     \
    \ |--------|-------------------|--------|\n      | Blocked AI requests | Demand\
    \ for shadow AI | Decreasing |\n      | Sanctioned AI adoption | Success of official\
    \ tools | Increasing |\n      | Exception requests | Unmet needs | Decreasing\
    \ over time |\n      | Shadow AI incidents | Leakage | Zero/minimal |\n      |\
    \ Time to approve new use case | Governance agility | <1 week for low-risk |\n\
    \      \n      **PALETTE integration:**\n      - Define governance in RIU-530\
    \ (AI Governance Config)\n      - Configure technical controls in RIU-531 (Guardrail\
    \ Selection)\n      - Train users on policies using RIU-140 (Training Materials)\n\
    \      - Track shadow AI incidents in RIU-100 (Incident Log)\n      \n      Key\
    \ insight: Shadow AI is a symptom, not the disease. The disease is unmet needs\
    \ + friction. Treat the disease (better tools, faster approval) and the symptom\
    \ disappears. Governance should be a guardrail, not a roadblock."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-100
  - RIU-140
  - RIU-530
  - RIU-531
  difficulty: high
  industries:
  - Enterprise
  - Finance
  - Healthcare
  tags:
  - governance
  - shadow-it
  - policy
  - enforcement
  sources:
  - title: 'Governance by design: The essential guide for successful AI scaling'
    url: https://aws.amazon.com/blogs/machine-learning/governance-by-design-the-essential-guide-for-successful-ai-scaling/
  - title: 'Securing Generative AI: How Enterprises Can Govern Workforce Use with
      SurePath AI'
    url: https://aws.amazon.com/blogs/apn/securing-generative-ai-how-enterprises-can-govern-workforce-use-of-generative-ai-with-surepath-ai/
  - title: Change Management and Adoption for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_3_implementation_and_execution/5_3_2_change_management_and_adoption.html
  - title: Mitigate AI security risks with Amazon Q Business and Securiti
    url: https://aws.amazon.com/blogs/awsmarketplace/mitigate-ai-security-risks-amazon-q-business-securiti-five-step-governance-framework/
- id: LIB-070
  question: How do I design human-in-the-loop systems that people actually use correctly?
  answer: "HITL systems fail when humans rubber-stamp (automation complacency) or\
    \ override everything (automation distrust). Design for appropriate reliance:\
    \ humans trust AI when correct, catch AI when wrong.\n      \n      **Common HITL\
    \ failure modes:**\n      \n      | Failure Mode | What Happens | Why It Happens\
    \ | Prevention |\n      |--------------|--------------|----------------|------------|\n\
    \      | **Rubber-stamping** | Human approves everything | AI usually right, checking\
    \ is tedious | Require specific action, not just \"approve\" |\n      | **Automation\
    \ complacency** | Human stops paying attention | Trust built, vigilance fades\
    \ | Vary task presentation, insert known errors |\n      | **Over-reliance** |\
    \ Human defers to AI even when wrong | AI seems confident, human uncertain | Show\
    \ AI confidence levels, encourage skepticism |\n      | **Under-reliance** | Human\
    \ ignores AI, does manually | Past bad experiences, distrust | Demonstrate accuracy,\
    \ let human verify |\n      | **Skill atrophy** | Human loses ability to do task\
    \ | AI always does it, practice lost | Periodic manual tasks, training refreshers\
    \ |\n      \n      **Design principles for correct usage:**\n      \n      ```\n\
    \      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     \
    \ â”‚       HITL DESIGN PRINCIPLES                                 â”‚\n      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \      â”‚                                                             â”‚\n     \
    \ â”‚  1. MAKE VERIFICATION EASY                                   â”‚\n      â”‚  \
    \   Don't ask humans to approve; ask them to verify         â”‚\n      â”‚       \
    \                                                      â”‚\n      â”‚  2. SHOW YOUR\
    \ WORK                                           â”‚\n      â”‚     Provide evidence,\
    \ citations, reasoning                  â”‚\n      â”‚                           \
    \                                  â”‚\n      â”‚  3. CALIBRATE TRUST            \
    \                              â”‚\n      â”‚     Show confidence levels, highlight\
    \ uncertainty           â”‚\n      â”‚                                           \
    \                  â”‚\n      â”‚  4. REDUCE COGNITIVE LOAD                      \
    \              â”‚\n      â”‚     Pre-process, summarize, highlight key points   \
    \         â”‚\n      â”‚                                                         \
    \    â”‚\n      â”‚  5. REQUIRE MEANINGFUL ACTION                                â”‚\n\
    \      â”‚     Don't allow \"approve all\" â€” require engagement          â”‚\n   \
    \   â”‚                                                             â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **HITL patterns and correct implementation:**\n    \
    \  \n      ```yaml\n      hitl_patterns:\n        approval_based:\n          purpose:\
    \ \"Binary decision (approve/reject)\"\n          correct_design:\n          \
    \  - \"Show AI output with supporting evidence\"\n            - \"Require reviewer\
    \ to check specific criteria\"\n            - \"Include 'reject with reason' option\"\
    \n            - \"Track approval patterns (detect rubber-stamping)\"\n       \
    \   anti_patterns:\n            - \"Single 'approve' button with no context\"\n\
    \            - \"Batch approval of multiple items\"\n            - \"No audit\
    \ trail of reviewer reasoning\"\n            \n        review_and_edit:\n    \
    \      purpose: \"Human modifies AI output\"\n          correct_design:\n    \
    \        - \"Side-by-side view: AI draft + edit area\"\n            - \"Track\
    \ all edits for learning\"\n            - \"Suggest common edits (don't require\
    \ retyping)\"\n            - \"Show original sources for fact-checking\"\n   \
    \       anti_patterns:\n            - \"Requiring human to rewrite from scratch\"\
    \n            - \"Not capturing what was changed\"\n            - \"No way to\
    \ indicate 'AI was correct'\"\n            \n        escalation_based:\n     \
    \     purpose: \"AI handles routine, human handles exceptions\"\n          correct_design:\n\
    \            - \"Clear escalation criteria (not arbitrary)\"\n            - \"\
    Provide AI's attempted answer + why escalated\"\n            - \"Give human tools\
    \ to resolve efficiently\"\n            - \"Feed resolution back to improve AI\"\
    \n          anti_patterns:\n            - \"Escalating everything 'just in case'\"\
    \n            - \"No context on why case was escalated\"\n            - \"Escalations\
    \ don't improve future AI handling\"\n            \n        feedback_loop:\n \
    \         purpose: \"Continuous improvement from human input\"\n          correct_design:\n\
    \            - \"Multiple feedback options (thumbs, rating, text)\"\n        \
    \    - \"Feedback is easy (2 clicks max)\"\n            - \"Show how feedback\
    \ improved system\"\n            - \"Close the loop with users\"\n          anti_patterns:\n\
    \            - \"Feedback collected but never used\"\n            - \"Feedback\
    \ form too long/complex\"\n            - \"No acknowledgment of user contribution\"\
    \n      ```\n      \n      **UX design for correct verification:**\n      \n \
    \     ```yaml\n      verification_ux:\n        show_evidence:\n          - \"\
    Timestamped citations to source documents\"\n          - \"Click-to-verify: link\
    \ to original content\"\n          - \"Highlight which sources support each claim\"\
    \n          - \"Show when no source supports a claim\"\n          \n        highlight_uncertainty:\n\
    \          - \"Visual confidence indicator (not just number)\"\n          - \"\
    Flag sections AI is uncertain about\"\n          - \"Different colors for high/medium/low\
    \ confidence\"\n          - \"Explicit 'I don't know' when appropriate\"\n   \
    \       \n        structure_the_task:\n          - \"Checklist of criteria to\
    \ verify\"\n          - \"Required fields before approval\"\n          - \"Specific\
    \ questions: 'Is this factually correct?'\"\n          - \"Not just 'approve/reject'\
    \ but 'why?'\"\n          \n        reduce_cognitive_load:\n          - \"Pre-summarize\
    \ long content\"\n          - \"Highlight changes from previous version\"\n  \
    \        - \"Show relevant context automatically\"\n          - \"Don't require\
    \ human to search for information\"\n      ```\n      \n      **Preventing automation\
    \ complacency:**\n      \n      ```yaml\n      complacency_prevention:\n     \
    \   vary_presentation:\n          - \"Don't show items in predictable order\"\n\
    \          - \"Mix easy and hard cases\"\n          - \"Insert known errors periodically\
    \ (honeypots)\"\n          - \"Change UI slightly to maintain attention\"\n  \
    \        \n        require_engagement:\n          - \"Require annotation, not\
    \ just approval\"\n          - \"Ask 'What would you change?' even if approving\"\
    \n          - \"Periodic 'explain your decision' prompts\"\n          - \"No batch\
    \ approvals without individual review\"\n          \n        feedback_on_performance:\n\
    \          - \"Show reviewer accuracy vs. ground truth\"\n          - \"Compare\
    \ to peer reviewers\"\n          - \"Highlight catches (positive reinforcement)\"\
    \n          - \"Alert when patterns suggest rubber-stamping\"\n          \n  \
    \      honeypot_system:\n          - \"Insert intentional errors that human should\
    \ catch\"\n          - \"Track catch rate as quality metric\"\n          - \"\
    Not punitive â€” used for feedback\"\n          - \"Calibrated to 5-10% of reviews\"\
    \n      ```\n      \n      **Training for HITL reviewers:**\n      \n      ```yaml\n\
    \      reviewer_training:\n        initial_training:\n          modules:\n   \
    \         - \"What the AI does well and poorly\"\n            - \"Common error\
    \ types to watch for\"\n            - \"How to verify claims efficiently\"\n \
    \           - \"When to escalate vs. decide\"\n          duration: \"2-4 hours\"\
    \n          \n        calibration:\n          - \"Review same cases as experts\"\
    \n          - \"Compare decisions, discuss differences\"\n          - \"Achieve\
    \ inter-rater reliability target\"\n          target: \"Cohen's kappa > 0.8\"\n\
    \          \n        ongoing:\n          - \"Monthly review of challenging cases\"\
    \n          - \"Feedback on individual accuracy\"\n          - \"Updates when\
    \ AI improves/changes\"\n          - \"Refresher on common errors\"\n        \
    \  \n        guidelines:\n          document:\n            - \"Criteria for approve/reject/edit\"\
    \n            - \"Examples of edge cases\"\n            - \"Escalation triggers\"\
    \n            - \"Quality standards\"\n          format: \"Searchable, with examples\"\
    \n      ```\n      \n      **Metrics for HITL effectiveness:**\n      \n     \
    \ | Metric | What It Measures | Target | Red Flag |\n      |--------|------------------|--------|----------|\n\
    \      | **Approval rate** | Human agreement with AI | 70-90% | >95% (rubber-stamping)\
    \ or <50% (poor AI) |\n      | **Review time** | Engagement level | Varies by\
    \ task | Too fast = not reading |\n      | **Edit rate** | Content quality | Varies\
    \ | 0% (not editing) or 100% (AI useless) |\n      | **Honeypot catch rate** |\
    \ Vigilance | >90% | <70% (complacency) |\n      | **Inter-rater reliability**\
    \ | Consistency | Îº > 0.8 | Îº < 0.6 (unclear guidelines) |\n      | **Feedback\
    \ provided** | Engagement | Regular | Never provides feedback |\n      | **Escalation\
    \ rate** | Appropriate triage | 5-15% | 0% (not escalating) or >30% (AI undertrained)\
    \ |\n      \n      **PALETTE integration:**\n      - Document HITL design in RIU-513\
    \ (Human Approval for ONE-WAY DOORs)\n      - Train reviewers using RIU-140 (Training\
    \ Materials)\n      - Define criteria in RIU-001 (Convergence Brief)\n      -\
    \ Track in RIU-141 (Change Management Plan)\n      \n      Key insight: The goal\
    \ isn't human oversight â€” it's appropriate reliance. A HITL system succeeds when\
    \ humans trust AI outputs they should trust, and catch errors they should catch.\
    \ Design for calibration, not just coverage."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-001
  - RIU-140
  
  - RIU-513
  difficulty: critical
  industries:
  - All
  tags:
  - hitl
  - ux-design
  - adoption
  - human-factors
  sources:
  - title: Human-in-the-Loop for GenAI Systems - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_1_system_and_application_design_patterns_for_genai/3_1_1_foundation_architecture_components/3_1_1_8_additional_components/3_1_1_8_1_human_in_the_loop/3_1_1_8_1_human_in_the_loop.html
  - title: Accelerate video Q&A workflows with thoughtful UX design
    url: https://aws.amazon.com/blogs/machine-learning/accelerate-video-qa-workflows-using-amazon-bedrock-knowledge-bases-amazon-transcribe-and-thoughtful-ux-design/
  - title: 'AI and Collaboration: A Human Angle'
    url: https://aws.amazon.com/blogs/enterprise-strategy/ai-and-collaboration-a-human-angle/
- id: LIB-071
  question: What documentation proves AI system decisions are auditable and explainable?
  answer: "Audit-ready documentation answers three questions: \"What did the AI do?\"\
    \ (decision logs), \"Why did it do that?\" (explainability), and \"Who was responsible?\"\
    \ (governance). Maintain these artifacts continuously, not just before audits.\n\
    \      \n      **Documentation package for AI auditability:**\n      \n      ```\n\
    \      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     \
    \ â”‚              AI AUDIT DOCUMENTATION PACKAGE                  â”‚\n      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \      â”‚                                                             â”‚\n     \
    \ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚  â”‚ SYSTEM\
    \ DOCUMENTATION                                 â”‚   â”‚\n      â”‚  â”‚ â€¢ Model cards\
    \ (what the AI is and does)             â”‚   â”‚\n      â”‚  â”‚ â€¢ Architecture documentation\
    \ (how it works)          â”‚   â”‚\n      â”‚  â”‚ â€¢ Technical specifications       \
    \                   â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚\
    \  â”‚ GOVERNANCE DOCUMENTATION                             â”‚   â”‚\n      â”‚  â”‚ â€¢\
    \ Risk assessments                                   â”‚   â”‚\n      â”‚  â”‚ â€¢ Policies\
    \ and procedures                           â”‚   â”‚\n      â”‚  â”‚ â€¢ Human oversight\
    \ records                           â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚\
    \  â”‚ OPERATIONAL DOCUMENTATION                            â”‚   â”‚\n      â”‚  â”‚ â€¢\
    \ Decision logs (audit trails)                      â”‚   â”‚\n      â”‚  â”‚ â€¢ Incident\
    \ records                                  â”‚   â”‚\n      â”‚  â”‚ â€¢ Performance monitoring\
    \                            â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n      â”‚\
    \  â”‚ EXPLAINABILITY DOCUMENTATION                         â”‚   â”‚\n      â”‚  â”‚ â€¢\
    \ Decision rationale                                â”‚   â”‚\n      â”‚  â”‚ â€¢ Input/output\
    \ traceability                         â”‚   â”‚\n      â”‚  â”‚ â€¢ Verification evidence\
    \                             â”‚   â”‚\n      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \   â”‚\n      â”‚                                                             â”‚\n\
    \      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     \
    \ ```\n      \n      **1. Model Card (per AI model/system):**\n      \n      ```yaml\n\
    \      model_card:\n        # Identity\n        model_name: \"Customer Support\
    \ AI Assistant v2.1\"\n        model_id: \"cs-assistant-prod-v2.1\"\n        owner:\
    \ \"AI Platform Team\"\n        last_updated: \"2024-06-15\"\n        \n     \
    \   # Purpose and Use\n        intended_use:\n          primary: \"Answer customer\
    \ questions about products and orders\"\n          secondary: \"Route complex\
    \ inquiries to human agents\"\n          out_of_scope:\n            - \"Medical\
    \ or legal advice\"\n            - \"Financial transactions\"\n            - \"\
    Personal data modifications\"\n            \n        # Model Details\n       \
    \ technical_details:\n          base_model: \"anthropic.claude-3-sonnet-20240229\"\
    \n          fine_tuning: \"None (prompt-based)\"\n          knowledge_base: \"\
    Product catalog + FAQ (updated weekly)\"\n          guardrails: \"Bedrock Guardrail\
    \ ID: gr-abc123\"\n          \n        # Performance\n        performance:\n \
    \         accuracy_metrics:\n            task: \"Answer correctness\"\n      \
    \      score: \"87% on evaluation set (n=1000)\"\n            evaluation_date:\
    \ \"2024-06-01\"\n          latency:\n            p50: \"800ms\"\n           \
    \ p99: \"2.1s\"\n          known_limitations:\n            - \"May struggle with\
    \ multi-part questions\"\n            - \"Cannot access real-time inventory\"\n\
    \            - \"Occasionally misattributes product features\"\n            \n\
    \        # Fairness and Safety\n        responsible_ai:\n          bias_testing:\
    \ \"Conducted 2024-05-15, no significant demographic bias detected\"\n       \
    \   content_filtering: \"Enabled for harmful content, PII\"\n          human_oversight:\
    \ \"Escalation to human for low-confidence responses\"\n          \n        #\
    \ Compliance\n        compliance:\n          applicable_regulations: [\"GDPR\"\
    , \"CCPA\"]\n          data_processing: \"No PII stored beyond session\"\n   \
    \       audit_framework: \"AWS Audit Manager GenAI Best Practices\"\n      ```\n\
    \      \n      **2. Risk Assessment Documentation:**\n      \n      ```yaml\n\
    \      risk_assessment:\n        assessment_id: \"RA-2024-CS-001\"\n        system:\
    \ \"Customer Support AI Assistant\"\n        assessment_date: \"2024-05-15\"\n\
    \        assessor: \"AI Governance Team\"\n        \n        risk_register:\n\
    \          - risk_id: \"R001\"\n            description: \"AI provides incorrect\
    \ product information\"\n            category: \"Accuracy\"\n            likelihood:\
    \ \"Medium\"\n            impact: \"Medium\"\n            risk_score: \"Medium\"\
    \n            controls:\n              - \"Knowledge base validation\"\n     \
    \         - \"Citation requirements\"\n              - \"Human escalation for\
    \ uncertainty\"\n            owner: \"Product Data Team\"\n            status:\
    \ \"Controlled\"\n            \n          - risk_id: \"R002\"\n            description:\
    \ \"AI discloses customer PII inappropriately\"\n            category: \"Privacy\"\
    \n            likelihood: \"Low\"\n            impact: \"High\"\n            risk_score:\
    \ \"Medium\"\n            controls:\n              - \"PII guardrails enabled\"\
    \n              - \"Output filtering\"\n              - \"Session isolation\"\n\
    \            owner: \"Security Team\"\n            status: \"Controlled\"\n  \
    \          \n        mitigation_tracking:\n          - risk_id: \"R001\"\n   \
    \         action: \"Implement citation requirements\"\n            status: \"\
    Complete\"\n            completion_date: \"2024-04-20\"\n            evidence:\
    \ \"PR #1234, Test results\"\n            \n        next_review: \"2024-08-15\"\
    \n      ```\n      \n      **3. Decision Audit Logs:**\n      \n      ```yaml\n\
    \      audit_log_requirements:\n        # What to log for every AI decision\n\
    \        per_decision:\n          required:\n            - \"request_id (unique\
    \ identifier)\"\n            - \"timestamp\"\n            - \"user/session identifier\"\
    \n            - \"input (or hash if PII)\"\n            - \"output\"\n       \
    \     - \"model_version\"\n            - \"prompt_version\"\n            - \"\
    guardrail_results\"\n            - \"confidence_score (if available)\"\n     \
    \       \n          for_explainability:\n            - \"retrieved_context (RAG\
    \ sources)\"\n            - \"citations\"\n            - \"reasoning_trace (if\
    \ available)\"\n            \n          for_governance:\n            - \"human_review_status\"\
    \n            - \"approval_records\"\n            - \"escalation_records\"\n \
    \           \n        retention:\n          standard: \"2 years\"\n          regulated_high_risk:\
    \ \"10 years\"\n          query_capability: \"Retrievable within 72 hours\"\n\
    \          \n        immutability:\n          - \"S3 Object Lock (GOVERNANCE mode)\"\
    \n          - \"CloudTrail log file validation\"\n          - \"No delete permissions\
    \ for audit logs\"\n      ```\n      \n      **4. Human Oversight Documentation:**\n\
    \      \n      ```yaml\n      human_oversight_records:\n        review_process:\n\
    \          description: \"How humans review AI outputs\"\n          documentation:\n\
    \            - \"Review criteria and guidelines\"\n            - \"Reviewer qualifications\"\
    \n            - \"Review workflow diagrams\"\n            \n        approval_records:\n\
    \          per_approval:\n            - \"Decision ID\"\n            - \"Reviewer\
    \ identity\"\n            - \"Timestamp\"\n            - \"Decision (approve/reject/modify)\"\
    \n            - \"Reason (if rejected/modified)\"\n            \n        escalation_records:\n\
    \          per_escalation:\n            - \"Trigger reason\"\n            - \"\
    Escalation path taken\"\n            - \"Resolution\"\n            - \"Time to\
    \ resolution\"\n            \n        oversight_metrics:\n          - \"% of decisions\
    \ reviewed\"\n          - \"Approval/rejection rates\"\n          - \"Average\
    \ review time\"\n          - \"Escalation frequency\"\n      ```\n      \n   \
    \   **5. Explainability Evidence:**\n      \n      ```yaml\n      explainability_documentation:\n\
    \        decision_rationale:\n          # How to explain individual decisions\n\
    \          components:\n            - \"Input factors considered\"\n         \
    \   - \"Sources/citations used\"\n            - \"Confidence level\"\n       \
    \     - \"Alternative options considered (if applicable)\"\n            \n   \
    \     verification_methods:\n          automated_reasoning:\n            tool:\
    \ \"Bedrock Guardrails Automated Reasoning\"\n            use: \"Verify responses\
    \ against logical rules\"\n            evidence: \"Verification results per decision\"\
    \n            \n          citation_verification:\n            use: \"Link claims\
    \ to source documents\"\n            evidence: \"Timestamped citations with source\
    \ links\"\n            \n          human_verification:\n            use: \"Spot-check\
    \ accuracy\"\n            evidence: \"Review records with findings\"\n       \
    \     \n        for_affected_persons:\n          # When someone asks \"why did\
    \ AI decide this?\"\n          documentation:\n            - \"Plain language\
    \ explanation template\"\n            - \"Process for handling explanation requests\"\
    \n            - \"Response time SLA\"\n            - \"Appeal/contestation process\"\
    \n      ```\n      \n      **Audit preparation checklist:**\n      \n      ```yaml\n\
    \      audit_preparation:\n        before_audit:\n          - \"Verify all documentation\
    \ is current\"\n          - \"Ensure audit logs are queryable\"\n          - \"\
    Prepare system access for auditors\"\n          - \"Brief relevant personnel\"\
    \n          - \"Compile evidence for controls\"\n          \n        evidence_collection:\n\
    \          - \"Model cards (current versions)\"\n          - \"Risk assessments\
    \ (most recent + history)\"\n          - \"Sample audit logs (representative period)\"\
    \n          - \"Human oversight records\"\n          - \"Incident response records\"\
    \n          - \"Training records (staff)\"\n          - \"Policy documents\"\n\
    \          \n        tools:\n          - \"AWS Audit Manager (evidence collection)\"\
    \n          - \"CloudTrail (API activity)\"\n          - \"CloudWatch Logs (operational\
    \ logs)\"\n          - \"S3 (document storage)\"\n      ```\n      \n      **AWS\
    \ Audit Manager 8 Principles:**\n      \n      | Principle | What to Document\
    \ |\n      |-----------|------------------|\n      | Accuracy | Evaluation results,\
    \ error rates, validation procedures |\n      | Fairness | Bias testing results,\
    \ demographic analysis |\n      | Privacy | Data handling policies, PII controls,\
    \ consent records |\n      | Resilience | Failover testing, recovery procedures\
    \ |\n      | Explainability | Decision rationale, citation systems |\n      |\
    \ Safety | Guardrail configurations, content filtering |\n      | Security | Access\
    \ controls, encryption, audit logs |\n      | Sustainability | Resource usage,\
    \ efficiency metrics |\n      \n      **PALETTE integration:**\n      - Store\
    \ model cards in RIU-532 (Model Registry)\n      - Document governance in RIU-530\
    \ (AI Governance Config)\n      - Configure audit logging in RIU-534 (Audit Trail\
    \ Config)\n      - Train team on requirements using RIU-140 (Training Materials)\n\
    \      \n      Key insight: Documentation isn't for auditors â€” it's for you. If\
    \ you can't explain why the AI made a decision six months ago, you can't defend\
    \ it, improve it, or trust it. Audit-ready documentation is operational documentation\
    \ done well."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-004
  - RIU-140
  - RIU-530
  - RIU-532
  - RIU-534
  difficulty: high
  industries:
  - Finance
  - Healthcare
  - Government
  tags:
  - documentation
  - auditability
  - explainability
  - compliance
  sources:
  - title: Risk and Compliance Management for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_2_governance_and_organization/5_2_3_risk_and_compliance_mngmt.html
  - title: Generative AI adoption and compliance with AWS Audit Manager
    url: https://aws.amazon.com/blogs/security/generative-ai-adoption-and-compliance-simplifying-the-path-forward-with-aws-audit-manager/
  - title: Build verifiable explainability with Automated Reasoning checks for Amazon
      Bedrock Guardrails
    url: https://aws.amazon.com/blogs/machine-learning/build-verifiable-explainability-into-financial-services-workflows-with-automated-reasoning-checks-for-amazon-bedrock-guardrails/
  - title: Generative AI Lifecycle Operational Excellence framework on AWS
    url: https://docs.aws.amazon.com/prescriptive-guidance/latest/gen-ai-lifecycle-operational-excellence/introduction.html
- id: LIB-072
  question: How do I measure AI adoption vs AI avoidance in production?
  answer: "Adoption metrics show if people use AI. Avoidance metrics show if they're\
    \ working around it. You need both â€” high adoption with high avoidance means people\
    \ use AI when forced but avoid it when they can.\n      \n      **Adoption vs.\
    \ Avoidance framework:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                 ADOPTION-AVOIDANCE MATRIX                    â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚       \
    \    LOW AVOIDANCE         HIGH AVOIDANCE              â”‚\n      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\
    \    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n      â”‚  HIGH   â”‚  âœ… SUCCESS    â”‚    â”‚  âš ï¸\
    \ COMPLIANCE  â”‚           â”‚\n      â”‚ ADOPTIONâ”‚  Genuine use   â”‚    â”‚  Forced use,\
    \   â”‚           â”‚\n      â”‚         â”‚  Users prefer  â”‚    â”‚  workarounds   â”‚  \
    \         â”‚\n      â”‚         â”‚  AI            â”‚    â”‚  when possible â”‚        \
    \   â”‚\n      â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n\
    \      â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n      â”‚\
    \   LOW   â”‚  \U0001F4CA NASCENT    â”‚    â”‚  âŒ RESISTANCE  â”‚           â”‚\n     \
    \ â”‚ ADOPTIONâ”‚  Early stage,  â”‚    â”‚  Active        â”‚           â”‚\n      â”‚    \
    \     â”‚  room to grow  â”‚    â”‚  avoidance,    â”‚           â”‚\n      â”‚         â”‚\
    \                â”‚    â”‚  possible      â”‚           â”‚\n      â”‚         â”‚      \
    \          â”‚    â”‚  shadow AI     â”‚           â”‚\n      â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\
    \    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n      â”‚                                 \
    \                            â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Adoption metrics (are people using AI?):**\n     \
    \ \n      | Metric | What It Measures | Calculation | Target |\n      |--------|------------------|-------------|--------|\n\
    \      | **Active users / Licensed users** | Breadth of adoption | Unique users\
    \ / Total licenses | >70% |\n      | **Frequency of use** | Habit formation |\
    \ Sessions per user per week | Daily |\n      | **Feature utilization** | Depth\
    \ of adoption | Features used / Available | >50% of key features |\n      | **Session\
    \ duration** | Engagement | Average time per session | Increasing |\n      | **Tasks\
    \ completed with AI** | Productivity impact | AI-assisted tasks / Total tasks\
    \ | Increasing |\n      | **Voluntary vs. Required use** | Genuine preference\
    \ | Voluntary sessions / Total | >60% |\n      \n      **Avoidance metrics (are\
    \ people working around AI?):**\n      \n      | Metric | What It Measures | How\
    \ to Detect | Red Flag |\n      |--------|------------------|---------------|----------|\n\
    \      | **Override rate** | Trust issues | AI suggestions rejected | >50% |\n\
    \      | **Manual bypass rate** | Preference for old way | Tasks done manually\
    \ when AI available | Increasing |\n      | **Edit-to-completion rate** | AI output\
    \ quality | Edits before accepting AI output | >80% edits |\n      | **Time to\
    \ first use** | Resistance | Days from access to first use | >14 days |\n    \
    \  | **Dropped sessions** | Frustration | Sessions started but not completed |\
    \ >30% |\n      | **Shadow AI usage** | Sanctioned tools inadequate | External\
    \ AI tool usage detected | Any |\n      | **Help desk tickets** | Usability issues\
    \ | Tickets about AI system | Increasing |\n      \n      **Behavioral signals\
    \ of avoidance:**\n      \n      ```yaml\n      avoidance_behaviors:\n       \
    \ active_avoidance:\n          signals:\n            - \"Consistently routes work\
    \ to non-AI path\"\n            - \"Uses AI only when manager is watching\"\n\
    \            - \"Completes AI workflow but re-does manually\"\n            - \"\
    Uses personal AI tools instead of corporate\"\n          detection:\n        \
    \    - \"Compare AI vs. non-AI task completion by user\"\n            - \"Session\
    \ timestamps (only during reviews)\"\n            - \"Duplicate work patterns\"\
    \n            - \"Network traffic to external AI services\"\n            \n  \
    \      passive_avoidance:\n          signals:\n            - \"Never logs in despite\
    \ training\"\n            - \"Logs in but doesn't complete tasks\"\n         \
    \   - \"Uses minimum features only\"\n            - \"Doesn't explore new capabilities\"\
    \n          detection:\n            - \"Login frequency tracking\"\n         \
    \   - \"Feature usage analytics\"\n            - \"Time-in-app metrics\"\n   \
    \         - \"Completion rates\"\n            \n        workarounds:\n       \
    \   signals:\n            - \"Copy-paste to external tools\"\n            - \"\
    Screenshots instead of using integrations\"\n            - \"Manual data entry\
    \ despite automation\"\n            - \"Email instead of using AI assistant\"\n\
    \          detection:\n            - \"Clipboard monitoring (privacy-aware)\"\n\
    \            - \"Integration usage vs. manual patterns\"\n            - \"Process\
    \ mining\"\n      ```\n      \n      **Metrics dashboard design:**\n      \n \
    \     ```yaml\n      adoption_dashboard:\n        executive_view:\n          -\
    \ \"Overall adoption rate (% active users)\"\n          - \"Trend: Adoption over\
    \ time\"\n          - \"ROI: Time saved / Cost\"\n          - \"Risk: Shadow AI\
    \ incidents\"\n          \n        team_manager_view:\n          - \"Team adoption\
    \ rate\"\n          - \"Top users (celebrate)\"\n          - \"Non-users (support\
    \ needed)\"\n          - \"Common issues raised\"\n          \n        detailed_analytics:\n\
    \          by_user:\n            - \"First use date\"\n            - \"Frequency\"\
    \n            - \"Features used\"\n            - \"Override rate\"\n         \
    \   - \"Feedback provided\"\n            \n          by_feature:\n           \
    \ - \"Usage rate\"\n            - \"Success rate\"\n            - \"Avoidance\
    \ rate\"\n            - \"User satisfaction\"\n            \n          by_team:\n\
    \            - \"Adoption rate\"\n            - \"Average engagement\"\n     \
    \       - \"Barriers identified\"\n      ```\n      \n      **Intervention triggers:**\n\
    \      \n      ```yaml\n      intervention_triggers:\n        individual_level:\n\
    \          - trigger: \"No login in 14 days post-training\"\n            action:\
    \ \"Personal outreach from AI ambassador\"\n            \n          - trigger:\
    \ \"Override rate >70% for 2 weeks\"\n            action: \"1:1 to understand\
    \ concerns\"\n            \n          - trigger: \"Dropped 5+ sessions in a week\"\
    \n            action: \"Usability support session\"\n            \n        team_level:\n\
    \          - trigger: \"Team adoption <50% after 30 days\"\n            action:\
    \ \"Team training refresh + manager engagement\"\n            \n          - trigger:\
    \ \"Team avoidance metrics increasing\"\n            action: \"Focus group to\
    \ identify barriers\"\n            \n        system_level:\n          - trigger:\
    \ \"Shadow AI usage detected\"\n            action: \"Assess if sanctioned tools\
    \ meet needs\"\n            \n          - trigger: \"Overall adoption plateau\"\
    \n            action: \"New feature launch or success story campaign\"\n     \
    \ ```\n      \n      **Segmentation for analysis:**\n      \n      | Segment |\
    \ What to Look For | Action |\n      |---------|------------------|--------|\n\
    \      | **Champions** (high adopt, low avoid) | What's working for them? | Amplify\
    \ their stories |\n      | **Compliant** (high adopt, high avoid) | Why the workarounds?\
    \ | Fix usability issues |\n      | **Potential** (low adopt, low avoid) | Awareness\
    \ or access issue? | Training and outreach |\n      | **Resistant** (low adopt,\
    \ high avoid) | Root cause of resistance? | 1:1 intervention |\n      \n     \
    \ **Qualitative signals (complement quantitative):**\n      \n      ```yaml\n\
    \      qualitative_measurement:\n        surveys:\n          frequency: \"Monthly\
    \ or quarterly\"\n          questions:\n            - \"How useful is the AI tool\
    \ for your work? (1-10)\"\n            - \"What prevents you from using it more?\"\
    \n            - \"What would make it more valuable?\"\n          analysis: \"\
    Sentiment trending, theme extraction\"\n          \n        feedback_channels:\n\
    \          - \"In-app feedback (thumbs up/down)\"\n          - \"Anonymous suggestion\
    \ box\"\n          - \"AI ambassador feedback\"\n          - \"Support ticket\
    \ themes\"\n          \n        observational:\n          - \"Shadowing users\
    \ during tasks\"\n          - \"Think-aloud sessions\"\n          - \"Focus groups\
    \ by segment\"\n      ```\n      \n      **PALETTE integration:**\n      - Track\
    \ adoption in RIU-141 (Change Management Plan)\n      - Store training records\
    \ in RIU-140 (Training Materials)\n      - Monitor usage via RIU-532 (Model Registry)\
    \ deployment metrics\n      - Feed into RIU-001 (Convergence Brief) for stakeholder\
    \ updates\n      \n      Key insight: High adoption + high avoidance = compliance\
    \ theater. Users tick the box but don't genuinely rely on AI. The goal is high\
    \ adoption + low avoidance â€” people choose AI because it helps them. Measure both,\
    \ or you'll miss the story."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-001
  - RIU-140
  
  - RIU-532
  difficulty: high
  industries:
  - All
  tags:
  - adoption-metrics
  - usage-tracking
  - behavioral-analytics
  - monitoring
  sources:
  - title: Change Management and Adoption for Generative AI
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/5_3_implementation_and_execution/5_3_2_change_management_and_adoption.html
  - title: Deploying generative AI applications
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_9_AIOps/aiops_deployment.html
  - title: AI/ML Organizational Adoption Framework
    url: https://awslabs.github.io/generative-ai-atlas/topics/5_0_organization_adoption_framework/index.html
- id: LIB-073
  question: What's the minimum viable governance for AI systems in regulated industries?
  answer: "\"Minimum viable\" in regulated industries means: enough governance to\
    \ be defensible to regulators, not comprehensive governance. Start with what can\
    \ get you shut down (risk, audit, oversight), then iterate.\n      \n      **Minimum\
    \ viable governance framework:**\n      \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚           MINIMUM VIABLE AI GOVERNANCE                       â”‚\n    \
    \  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n      â”‚  \
    \                                                           â”‚\n      â”‚  TIER 1:\
    \ NON-NEGOTIABLE (Day 1)                             â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                              â”‚\n      â”‚  âœ“ Risk assessment completed       \
    \                         â”‚\n      â”‚  âœ“ Human oversight for high-stakes decisions\
    \                â”‚\n      â”‚  âœ“ Audit logging enabled                         \
    \           â”‚\n      â”‚  âœ“ Data governance basics (PII, access controls)      \
    \      â”‚\n      â”‚  âœ“ Accountability assigned (who owns this?)                \
    \ â”‚\n      â”‚                                                             â”‚\n \
    \     â”‚  TIER 2: REQUIRED (Within 30 days)                          â”‚\n      â”‚\
    \  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚\n      â”‚  âœ“ Policies\
    \ documented                                      â”‚\n      â”‚  âœ“ Model documentation\
    \ (model cards)                        â”‚\n      â”‚  âœ“ Incident response procedures\
    \                             â”‚\n      â”‚  âœ“ Training for operators           \
    \                        â”‚\n      â”‚  âœ“ Monitoring dashboards                 \
    \                   â”‚\n      â”‚                                               \
    \              â”‚\n      â”‚  TIER 3: MATURE (Within 90 days)                   \
    \         â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        \
    \    â”‚\n      â”‚  â—‹ Automated compliance checks                              â”‚\n\
    \      â”‚  â—‹ Bias/fairness testing                                    â”‚\n     \
    \ â”‚  â—‹ Third-party audits                                       â”‚\n      â”‚  â—‹\
    \ Governance board established                             â”‚\n      â”‚  â—‹ Continuous\
    \ improvement process                           â”‚\n      â”‚                   \
    \                                          â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Tier 1: Non-negotiable controls (Day 1)**\n      \n\
    \      ```yaml\n      tier_1_controls:\n        risk_assessment:\n          requirement:\
    \ \"Document what can go wrong\"\n          minimum_content:\n            - \"\
    AI system description and purpose\"\n            - \"Risk identification (what\
    \ could fail)\"\n            - \"Impact assessment (who gets hurt)\"\n       \
    \     - \"Risk mitigation (how we prevent/reduce)\"\n          regulator_question:\
    \ \"Did you assess the risks before deploying?\"\n          \n        human_oversight:\n\
    \          requirement: \"Human decision-maker for significant impacts\"\n   \
    \       implementation:\n            - \"High-stakes decisions require human approval\"\
    \n            - \"Escalation path defined\"\n            - \"Override capability\
    \ available\"\n          regulator_question: \"Who was responsible for this decision?\"\
    \n          \n        audit_logging:\n          requirement: \"Prove what happened\"\
    \n          minimum_logging:\n            - \"All AI decisions (input, output,\
    \ timestamp)\"\n            - \"Who accessed the system\"\n            - \"Configuration\
    \ changes\"\n          retention: \"Per regulatory requirement (often 5-10 years)\"\
    \n          regulator_question: \"Show me the record of this transaction\"\n \
    \         \n        data_governance:\n          requirement: \"Know your data,\
    \ protect it\"\n          minimum_controls:\n            - \"PII identified and\
    \ protected\"\n            - \"Access controls implemented\"\n            - \"\
    Data lineage documented\"\n          regulator_question: \"Where did this data\
    \ come from? Who can access it?\"\n          \n        accountability:\n     \
    \     requirement: \"Named responsible party\"\n          implementation:\n  \
    \          - \"AI system owner identified\"\n            - \"Decision authority\
    \ defined\"\n            - \"Escalation contacts documented\"\n          regulator_question:\
    \ \"Who is accountable for this system?\"\n      ```\n      \n      **Industry-specific\
    \ requirements:**\n      \n      | Industry | Key Regulation | Minimum Viable\
    \ Additions |\n      |----------|----------------|--------------------------|\n\
    \      | **Financial Services** | OCC guidance, FFIEC, GDPR, EU AI Act | Model\
    \ risk management, fair lending testing, explainable credit decisions |\n    \
    \  | **Healthcare** | HIPAA, FDA guidance, HITECH | PHI protection, clinical decision\
    \ support safeguards, validation studies |\n      | **Government (US)** | OMB\
    \ M-24-10, EO 14110 | Rights-impacting AI inventory, public transparency, procurement\
    \ requirements |\n      | **Government (EU)** | EU AI Act | Risk classification,\
    \ conformity assessment, CE marking (high-risk) |\n      | **Insurance** | State\
    \ regulations, NAIC guidance | Actuarial review, non-discrimination testing, rate\
    \ justification |\n      \n      **Financial Services specifics:**\n      \n \
    \     ```yaml\n      fsi_minimum_governance:\n        model_risk_management:\n\
    \          requirement: \"SR 11-7 / OCC 2011-12 applies to AI\"\n          controls:\n\
    \            - \"Model validation (independent review)\"\n            - \"Model\
    \ inventory maintained\"\n            - \"Performance monitoring\"\n         \
    \   - \"Model documentation\"\n            \n        fair_lending:\n         \
    \ requirement: \"ECOA, Fair Housing Act\"\n          controls:\n            -\
    \ \"Disparate impact testing\"\n            - \"Adverse action notices with reasons\"\
    \n            - \"No prohibited factors in decisions\"\n            \n       \
    \ consumer_protection:\n          requirement: \"UDAP/UDAAP\"\n          controls:\n\
    \            - \"Clear disclosure of AI use\"\n            - \"Accurate representations\"\
    \n            - \"Consumer recourse available\"\n            \n        aws_tools:\n\
    \          - \"Bedrock Guardrails (content filtering)\"\n          - \"SageMaker\
    \ Model Monitor (drift detection)\"\n          - \"AWS Audit Manager (GenAI framework)\"\
    \n      ```\n      \n      **Healthcare specifics:**\n      \n      ```yaml\n\
    \      healthcare_minimum_governance:\n        hipaa_compliance:\n          requirement:\
    \ \"Protected Health Information\"\n          controls:\n            - \"PHI not\
    \ used in training without authorization\"\n            - \"BAA with AI service\
    \ providers\"\n            - \"Access logging for PHI\"\n            - \"Minimum\
    \ necessary principle\"\n            \n        clinical_decision_support:\n  \
    \        requirement: \"FDA guidance (if applicable)\"\n          controls:\n\
    \            - \"Intended use clearly defined\"\n            - \"Clinical validation\
    \ documentation\"\n            - \"Human clinician in the loop\"\n           \
    \ - \"Clear disclaimers\"\n            \n        patient_safety:\n          requirement:\
    \ \"First, do no harm\"\n          controls:\n            - \"Failsafe to human\
    \ for uncertainty\"\n            - \"Adverse event reporting\"\n            -\
    \ \"Quality monitoring\"\n      ```\n      \n      **Government specifics:**\n\
    \      \n      ```yaml\n      government_minimum_governance:\n        omb_m24_10_requirements:\n\
    \          # For US federal agencies\n          controls:\n            - \"AI\
    \ use case inventory\"\n            - \"Rights-impacting AI identification\"\n\
    \            - \"Minimum risk management practices\"\n            - \"Chief AI\
    \ Officer designated\"\n            \n        transparency:\n          requirement:\
    \ \"Public accountability\"\n          controls:\n            - \"Public disclosure\
    \ of AI use cases\"\n            - \"Algorithmic impact assessments\"\n      \
    \      - \"Citizen recourse mechanisms\"\n            \n        procurement:\n\
    \          requirement: \"Responsible AI acquisition\"\n          controls:\n\
    \            - \"Vendor AI governance assessment\"\n            - \"Contractual\
    \ compliance requirements\"\n            - \"Ongoing monitoring obligations\"\n\
    \      ```\n      \n      **Minimum viable checklist (all industries):**\n   \
    \   \n      ```yaml\n      mvg_checklist:\n        # Complete before production\
    \ deployment\n        before_go_live:\n          risk:\n            - \"Risk assessment\
    \ completed and documented\"\n            - \"Risk owner identified\"\n      \
    \      - \"Mitigation controls implemented\"\n            \n          oversight:\n\
    \            - \"Human oversight process defined\"\n            - \"Escalation\
    \ path documented\"\n            - \"Override capability tested\"\n          \
    \  \n          audit:\n            - \"Logging enabled for all decisions\"\n \
    \           - \"Logs stored immutably\"\n            - \"Query capability verified\"\
    \n            \n          data:\n            - \"PII/sensitive data identified\"\
    \n            - \"Access controls implemented\"\n            - \"Data lineage\
    \ documented\"\n            \n          accountability:\n            - \"System\
    \ owner named\"\n            - \"Contact information current\"\n            -\
    \ \"Responsibilities documented\"\n            \n        # Document but can refine\
    \ post-launch\n          policy:\n            - \"Acceptable use policy drafted\"\
    \n            - \"Incident response procedure exists\"\n            - \"Training\
    \ plan identified\"\n            \n        # Can iterate after launch\n      \
    \    optimization:\n            - \"Bias testing planned\"\n            - \"Governance\
    \ board formation planned\"\n            - \"Automation roadmap defined\"\n  \
    \    ```\n      \n      **Phase-in approach:**\n      \n      | Phase | Timeline\
    \ | Focus | Governance Maturity |\n      |-------|----------|-------|---------------------|\n\
    \      | **Launch** | Day 0 | Tier 1 controls | Defensible minimum |\n      |\
    \ **Stabilize** | Days 1-30 | Tier 2 documentation | Documented processes |\n\
    \      | **Mature** | Days 31-90 | Tier 3 automation | Scalable governance |\n\
    \      | **Optimize** | Ongoing | Continuous improvement | Best-in-class |\n \
    \     \n      **PALETTE integration:**\n      - Configure controls in RIU-530\
    \ (AI Governance Config)\n      - Implement guardrails via RIU-531 (Guardrail\
    \ Selection)\n      - Document in RIU-533 (FRIA - Fundamental Rights Impact Assessment)\n\
    \      - Train team using RIU-140 (Training Materials)\n      \n      Key insight:\
    \ \"Minimum viable\" doesn't mean \"minimal\" â€” it means \"enough to be defensible.\"\
    \ When a regulator asks, you need to answer: \"Yes, we assessed the risks. Yes,\
    \ humans are in the loop. Yes, we have records. Here's who's responsible.\" If\
    \ you can answer those four questions, you have minimum viable governance."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-140
  - RIU-530
  - RIU-531
  - RIU-533
  difficulty: critical
  industries:
  - Finance
  - Healthcare
  - Government
  tags:
  - governance
  - compliance
  - regulation
  - minimum-viable
  sources:
  - title: 'Governance by design: The essential guide for successful AI scaling'
    url: https://aws.amazon.com/blogs/machine-learning/governance-by-design-the-essential-guide-for-successful-ai-scaling/
  - title: AWS User Guide to GRC for Responsible AI Adoption in Financial Services
    url: https://aws.amazon.com/blogs/security/introducing-the-aws-user-guide-to-governance-risk-and-compliance-for-responsible-ai-adoption-within-financial-services-industries/
  - title: Generative AI adoption and compliance with AWS Audit Manager
    url: https://aws.amazon.com/blogs/security/generative-ai-adoption-and-compliance-simplifying-the-path-forward-with-aws-audit-manager/
  - title: How AWS helps agencies meet OMB AI governance requirements
    url: https://aws.amazon.com/blogs/publicsector/how-aws-helps-agencies-meet-omb-ai-governance-requirements/
  - title: Regulatory Compliance and Governance - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/3_0_architecture_and_design_patterns/3_5_security_privacy/3_5_3_compliance_data_protection/3_5_3-2_regulatory_governance/regulatory_governance.html
- id: LIB-074
  question: How do I design AI systems that build trust through transparency?
  answer: "Trust comes from predictability, not perfection. Users trust AI that tells\
    \ them what it can do, what it can't, and how confident it is â€” not AI that pretends\
    \ to be infallible.\n      \n      **Trust-building transparency layers:**\n \
    \     \n      ```\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n\
    \      â”‚                 TRANSPARENCY LAYERS                      â”‚\n      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n\
    \      â”‚                                                         â”‚\n      â”‚  LAYER\
    \ 1: INTERACTION (every response)                  â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                    â”‚\n      â”‚  â€¢ Confidence indicators                     \
    \           â”‚\n      â”‚  â€¢ Source citations                                   \
    \  â”‚\n      â”‚  â€¢ \"I don't know\" capability                            â”‚\n  \
    \    â”‚  â€¢ Limitations disclosure                               â”‚\n      â”‚    \
    \                                                     â”‚\n      â”‚  LAYER 2: UNDERSTANDING\
    \ (on demand)                     â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                      â”‚\n      â”‚  â€¢ Explainability features                 \
    \             â”‚\n      â”‚  â€¢ Decision factors shown                           \
    \    â”‚\n      â”‚  â€¢ Alternative options presented                        â”‚\n  \
    \    â”‚  â€¢ Reasoning trace available                            â”‚\n      â”‚    \
    \                                                     â”‚\n      â”‚  LAYER 3: DOCUMENTATION\
    \ (discoverable)                  â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                   â”‚\n      â”‚  â€¢ Model cards                                \
    \          â”‚\n      â”‚  â€¢ System cards                                        \
    \ â”‚\n      â”‚  â€¢ AI service cards                                     â”‚\n     \
    \ â”‚  â€¢ Capability/limitation docs                           â”‚\n      â”‚       \
    \                                                  â”‚\n      â”‚  LAYER 4: GOVERNANCE\
    \ (auditable)                        â”‚\n      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
    \                         â”‚\n      â”‚  â€¢ Audit logs                           \
    \                â”‚\n      â”‚  â€¢ Decision records                              \
    \       â”‚\n      â”‚  â€¢ Compliance documentation                             â”‚\n\
    \      â”‚  â€¢ Third-party assessments                              â”‚\n      â”‚  \
    \                                                       â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\
    \      ```\n      \n      **Layer 1: Interaction-level transparency (every response)**\n\
    \      \n      **Confidence communication:**\n      ```yaml\n      confidence_patterns:\n\
    \        explicit_confidence:\n          example: \"I'm 85% confident in this\
    \ answer based on 3 matching sources\"\n          when: \"High-stakes decisions,\
    \ user requests\"\n          \n        uncertainty_disclosure:\n          example:\
    \ \"I found partial information but couldn't verify the date\"\n          when:\
    \ \"Incomplete or conflicting sources\"\n          \n        honest_limitations:\n\
    \          example: \"I don't have information about events after 2023\"\n   \
    \       when: \"Knowledge cutoff, out-of-scope queries\"\n          \n       \
    \ hedging_language:\n          example: \"Based on the documents provided...\"\
    \ vs \"The answer is...\"\n          when: \"All RAG responses (grounded in sources)\"\
    \n      ```\n      \n      **Source attribution:**\n      ```yaml\n      citation_patterns:\n\
    \        inline_citations:\n          example: \"The policy requires 30-day notice\
    \ [Policy Doc, Section 3.2]\"\n          implementation: \"Include source reference\
    \ in response\"\n          \n        timestamped_citations:\n          example:\
    \ \"[Source: HR Policy v2.1, Updated: 2024-03-15]\"\n          implementation:\
    \ \"Show version and date for verifiability\"\n          benefit: \"Users can\
    \ verify, mitigates hallucination risk\"\n          \n        clickable_sources:\n\
    \          example: \"Button navigates to specific portion of source content\"\
    \n          implementation: \"Deep links to source documents\"\n          benefit:\
    \ \"Easy verification builds trust\"\n          \n        no_source_transparency:\n\
    \          example: \"I couldn't find a source for this. This is based on general\
    \ knowledge.\"\n          when: \"Response isn't grounded in retrieved documents\"\
    \n      ```\n      \n      **Layer 2: Explainability (on demand)**\n      \n \
    \     **Decision explanation patterns:**\n      ```yaml\n      explainability_features:\n\
    \        factors_shown:\n          example: \"This recommendation is based on:\
    \ your purchase history (40%), similar customers (35%), current promotions (25%)\"\
    \n          when: \"Recommendations, classifications, risk scores\"\n        \
    \  \n        reasoning_trace:\n          example: \"Step 1: Retrieved relevant\
    \ policies â†’ Step 2: Identified applicable rules â†’ Step 3: Applied to your situation\"\
    \n          when: \"Complex multi-step reasoning\"\n          \n        alternatives_presented:\n\
    \          example: \"I recommended Option A, but Option B is also viable if [condition]\"\
    \n          when: \"Decisions with multiple valid paths\"\n          \n      \
    \  automated_reasoning:\n          tool: \"Amazon Bedrock Guardrails Automated\
    \ Reasoning checks\"\n          benefit: \"Mathematically verifiable explanations\"\
    \n          use_case: \"Financial services, compliance workflows\"\n      ```\n\
    \      \n      **Layer 3: Documentation transparency**\n      \n      **AI Service/Model\
    \ Cards:**\n      ```yaml\n      model_card_template:\n        model_overview:\n\
    \          name: \"Customer Support Assistant v2.1\"\n          purpose: \"Answer\
    \ customer questions about products and policies\"\n          intended_users:\
    \ \"Customer service representatives\"\n          \n        capabilities:\n  \
    \        - \"Answer product questions from knowledge base\"\n          - \"Look\
    \ up order status\"\n          - \"Explain return policies\"\n          \n   \
    \     limitations:\n          - \"Cannot process refunds directly\"\n        \
    \  - \"May not have information about products released in last 7 days\"\n   \
    \       - \"Not trained on competitor products\"\n          \n        performance:\n\
    \          accuracy: \"92% on internal evaluation set\"\n          known_weaknesses:\
    \ \"Lower accuracy on multi-part questions\"\n          \n        responsible_ai:\n\
    \          bias_testing: \"Tested for demographic bias in responses\"\n      \
    \    safety_measures: \"Content filtering, PII detection\"\n          human_oversight:\
    \ \"Escalation to human for complaints\"\n      ```\n      \n      **Layer 4:\
    \ Governance transparency**\n      \n      **Auditable systems:**\n      ```yaml\n\
    \      audit_transparency:\n        decision_logging:\n          logged: [\"Input\"\
    , \"Output\", \"Model version\", \"Retrieved sources\", \"Confidence\", \"Timestamp\"\
    ]\n          retention: \"Per regulatory requirement\"\n          access: \"Audit\
    \ team, compliance, legal\"\n          \n        compliance_documentation:\n \
    \         content: \"How system meets regulatory requirements\"\n          updates:\
    \ \"Maintained with each significant change\"\n          \n        third_party_assessment:\n\
    \          when: \"High-risk AI applications\"\n          value: \"Independent\
    \ validation builds external trust\"\n      ```\n      \n      **Transparency\
    \ by audience:**\n      \n      | Audience | What They Need | How to Provide |\n\
    \      |----------|----------------|----------------|\n      | **End users** |\
    \ Confidence, sources, limitations | In-response indicators |\n      | **Business\
    \ users** | How decisions are made | Explainability features |\n      | **Operators**\
    \ | System behavior, performance | Dashboards, model cards |\n      | **Auditors**\
    \ | Decision records, compliance | Logs, documentation |\n      | **Regulators**\
    \ | Governance, risk management | Formal documentation |\n      | **Public** |\
    \ AI use disclosure | Privacy notices, AI disclosures |\n      \n      **Trust-building\
    \ patterns:**\n      \n      | Pattern | Implementation | Trust Mechanism |\n\
    \      |---------|----------------|-----------------|\n      | **\"I don't know\"\
    ** | Allow model to decline answering | Honesty builds trust |\n      | **Show\
    \ your work** | Display reasoning steps | Understanding builds trust |\n     \
    \ | **Cite sources** | Link to original documents | Verifiability builds trust\
    \ |\n      | **Admit uncertainty** | Express confidence levels | Calibration builds\
    \ trust |\n      | **Offer alternatives** | Present options, not mandates | Autonomy\
    \ builds trust |\n      | **Enable feedback** | Let users correct errors | Responsiveness\
    \ builds trust |\n      | **Disclose AI use** | Clear labeling of AI-generated\
    \ content | Transparency builds trust |\n      \n      **Anti-patterns that destroy\
    \ trust:**\n      \n      | Anti-Pattern | Why It Destroys Trust |\n      |--------------|----------------------|\n\
    \      | Overconfident responses | One wrong \"certain\" answer = broken trust\
    \ |\n      | Hidden AI use | Discovery feels like deception |\n      | Hallucinations\
    \ without caveat | Confidently wrong = worse than wrong |\n      | No recourse\
    \ | Users feel trapped |\n      | Black box decisions | Lack of understanding\
    \ breeds suspicion |\n      \n      **AWS implementation tools:**\n      \n  \
    \    - **Bedrock Guardrails**: Automated Reasoning for verifiable explanations\n\
    \      - **SageMaker Clarify**: Bias detection, feature importance\n      - **AI\
    \ Service Cards**: Standardized documentation (published by AWS)\n      - **CloudWatch**:\
    \ Logging for audit trails\n      \n      **PALETTE integration:**\n      - Document\
    \ transparency requirements in RIU-140 (Training Materials)\n      - Configure\
    \ disclosure in RIU-141 (Communication Template)\n      - Include in convergence\
    \ goals (RIU-001)\n      - Define explainability needs in RIU-533 (FRIA)\n   \
    \   \n      Key insight: Users don't need to understand HOW the AI works to trust\
    \ it. They need to understand WHEN to trust it (confidence), WHAT it's based on\
    \ (sources), and WHAT IT CAN'T DO (limitations). Transparency about limitations\
    \ builds more trust than claims of perfection."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-001
  - RIU-140
  
  - RIU-533
  difficulty: high
  industries:
  - All
  tags:
  - trust
  - transparency
  - design
  - adoption
  sources:
  - title: Advancing AI trust with new responsible AI tools, capabilities, and resources
    url: https://aws.amazon.com/blogs/machine-learning/advancing-ai-trust-with-new-responsible-ai-tools-capabilities-and-resources/
  - title: 'Governance by design: The essential guide for successful AI scaling'
    url: https://aws.amazon.com/blogs/machine-learning/governance-by-design-the-essential-guide-for-successful-ai-scaling/
  - title: Responsible AI design in healthcare and life sciences
    url: https://aws.amazon.com/blogs/machine-learning/responsible-ai-design-in-healthcare-and-life-sciences/
  - title: Protecting Consumers and Promoting Innovation â€“ AI Regulation and Building
      Trust
    url: https://aws.amazon.com/blogs/machine-learning/protecting-consumers-and-promoting-innovation-ai-regulation-and-building-trust-in-responsible-ai/
- id: LIB-075
  question: What training enables stakeholders to correctly interpret AI confidence
    scores?
  answer: "Most stakeholders misinterpret confidence scores as \"accuracy\" when they're\
    \ actually \"model certainty\" â€” a high-confidence wrong answer is common. Training\
    \ must address this gap.\n      \n      **Common misinterpretations to address:**\n\
    \      \n      | Misinterpretation | Reality | Training Fix |\n      |-------------------|---------|--------------|\n\
    \      | \"95% confident = 95% accurate\" | Confidence â‰  accuracy; model can be\
    \ confidently wrong | Teach calibration concept |\n      | \"High confidence =\
    \ trust blindly\" | High confidence + wrong answer = hallucination | Teach verification\
    \ habits |\n      | \"Low confidence = wrong\" | Low confidence may just mean\
    \ uncertain, not incorrect | Teach when to seek more info |\n      | \"Scores\
    \ are comparable across models\" | Different models calibrate differently | Teach\
    \ model-specific baselines |\n      | \"Confidence is binary (trust/don't)\" |\
    \ It's a spectrum requiring judgment | Teach threshold decision-making |\n   \
    \   \n      **Training curriculum structure:**\n      \n      ```yaml\n      confidence_score_training:\n\
    \        module_1_foundations:\n          title: \"What Confidence Scores Actually\
    \ Mean\"\n          duration: \"30 minutes\"\n          topics:\n            -\
    \ \"Definition: model certainty, not accuracy\"\n            - \"How scores are\
    \ calculated (simplified)\"\n            - \"Why high confidence can still be\
    \ wrong\"\n            - \"Calibration: well-calibrated vs. overconfident models\"\
    \n          exercise: \"Review 10 AI outputs, predict which high-confidence ones\
    \ are wrong\"\n          \n        module_2_interpretation:\n          title:\
    \ \"Reading and Using Confidence Scores\"\n          duration: \"45 minutes\"\n\
    \          topics:\n            - \"Your model's calibration baseline\"\n    \
    \        - \"What 'good' looks like for your use case\"\n            - \"Thresholds:\
    \ when to trust, verify, or escalate\"\n            - \"Combining confidence with\
    \ other signals\"\n          exercise: \"Set appropriate thresholds for 3 business\
    \ scenarios\"\n          \n        module_3_decision_making:\n          title:\
    \ \"Making Decisions with AI Outputs\"\n          duration: \"45 minutes\"\n \
    \         topics:\n            - \"Automation bias: when humans over-trust AI\"\
    \n            - \"Verification strategies by confidence level\"\n            -\
    \ \"When to override AI recommendations\"\n            - \"Documenting decision\
    \ rationale\"\n          exercise: \"Role-play: AI gives high-confidence wrong\
    \ answer, practice catching it\"\n          \n        module_4_feedback:\n   \
    \       title: \"Improving AI Through Feedback\"\n          duration: \"30 minutes\"\
    \n          topics:\n            - \"How feedback improves confidence calibration\"\
    \n            - \"What makes good feedback\"\n            - \"Reporting issues\
    \ effectively\"\n          exercise: \"Submit 3 examples of AI mistakes with proper\
    \ documentation\"\n      ```\n      \n      **Key concepts to teach:**\n     \
    \ \n      **1. Calibration:**\n      ```\n      Well-calibrated model:\n     \
    \ - When it says 90% confident, it's right ~90% of the time\n      - When it says\
    \ 60% confident, it's right ~60% of the time\n      \n      Overconfident model:\n\
    \      - Says 90% confident, but only right 70% of the time\n      - Common in\
    \ LLMs! They sound certain even when wrong\n      \n      Training message: \"\
    Our model tends to be [well-calibrated / overconfident / \n      underconfident].\
    \ Adjust your trust accordingly.\"\n      ```\n      \n      **2. Threshold framework:**\n\
    \      ```yaml\n      confidence_thresholds:\n        high_confidence:\n     \
    \     range: \">85%\"\n          action: \"Generally trust, spot-check periodically\"\
    \n          verification: \"10% sample review\"\n          \n        medium_confidence:\n\
    \          range: \"60-85%\"\n          action: \"Review before acting\"\n   \
    \       verification: \"Manual review required\"\n          \n        low_confidence:\n\
    \          range: \"<60%\"\n          action: \"Don't trust without verification\"\
    \n          verification: \"Escalate to expert or decline to answer\"\n      ```\n\
    \      \n      **3. Verification habits:**\n      | Confidence Level | Verification\
    \ Action |\n      |------------------|---------------------|\n      | Any level\
    \ | Check if answer makes sense in context |\n      | High confidence | Ask \"\
    Is this the kind of question it's good at?\" |\n      | Medium confidence | Cross-check\
    \ with another source |\n      | Low confidence | Get human expert input |\n \
    \     | Critical decision | Verify regardless of confidence |\n      \n      **Role-specific\
    \ training paths:**\n      \n      ```yaml\n      training_paths:\n        executives:\n\
    \          focus: \"Strategic understanding\"\n          modules: [1, 3]\n   \
    \       depth: \"Conceptual\"\n          key_message: \"AI confidence isn't accuracy.\
    \ Build verification into processes.\"\n          \n        business_users:\n\
    \          focus: \"Daily decision-making\"\n          modules: [1, 2, 3, 4]\n\
    \          depth: \"Practical application\"\n          key_message: \"Know your\
    \ thresholds. Verify medium confidence. Report errors.\"\n          \n       \
    \ technical_users:\n          focus: \"System optimization\"\n          modules:\
    \ [1, 2, 3, 4] + advanced calibration\n          depth: \"Technical understanding\"\
    \n          key_message: \"Monitor calibration. Tune thresholds. Improve with\
    \ feedback.\"\n          \n        auditors_compliance:\n          focus: \"Risk\
    \ and governance\"\n          modules: [1, 3] + documentation\n          depth:\
    \ \"Control-oriented\"\n          key_message: \"Verify high-stakes decisions\
    \ regardless of confidence.\"\n      ```\n      \n      **Training delivery methods:**\n\
    \      \n      | Method | Best For | AWS Resource |\n      |--------|----------|--------------|\n\
    \      | E-learning modules | Foundational concepts | Skill Builder courses |\n\
    \      | Interactive simulations | Practical application | AWS SimuLearn |\n \
    \     | Hands-on labs | Technical users | Workshops |\n      | Case studies |\
    \ Business context | Custom scenarios |\n      | Communities of practice | Ongoing\
    \ learning | Internal forums |\n      \n      **Assessment and certification:**\n\
    \      \n      ```yaml\n      competency_assessment:\n        knowledge_check:\n\
    \          format: \"Quiz\"\n          passing: \"80%\"\n          topics:\n \
    \           - \"Define confidence vs. accuracy\"\n            - \"Identify calibration\
    \ issues\"\n            - \"Select appropriate thresholds\"\n            \n  \
    \      practical_assessment:\n          format: \"Scenario-based\"\n         \
    \ scenarios:\n            - \"AI gives high-confidence answer that's wrong â€” do\
    \ you catch it?\"\n            - \"AI gives low-confidence answer that's right\
    \ â€” do you verify appropriately?\"\n            - \"When do you escalate vs. trust?\"\
    \n            \n        certification:\n          validity: \"12 months\"\n  \
    \        renewal: \"Refresher + new assessment\"\n      ```\n      \n      **Ongoing\
    \ reinforcement:**\n      \n      - **Weekly tips**: \"Did you know? Our model\
    \ is 15% overconfident on [topic]\"\n      - **Error reviews**: Share anonymized\
    \ examples of caught mistakes\n      - **Calibration updates**: Notify when model\
    \ calibration changes\n      - **Feedback recognition**: Acknowledge users who\
    \ report useful errors\n      \n      **PALETTE integration:**\n      - Store\
    \ training materials in RIU-140 (Training Materials)\n      - Communicate updates\
    \ via RIU-141 (Communication Template)\n      - Track completion in RIU-122 (Adoption\
    \ Dashboard)\n      - Include in onboarding for new users\n      \n      Key insight:\
    \ The goal isn't to make users distrust AI â€” it's to make them appropriately calibrated.\
    \ They should trust when trust is warranted, verify when verification is needed,\
    \ and escalate when expertise is required. Train the judgment, not just the knowledge."
  problem_type: Trust_Governance_and_Adoption
  related_rius:
  - RIU-122
  - RIU-140
  
  difficulty: high
  industries:
  - All
  tags:
  - training
  - confidence-scores
  - interpretation
  - enablement
  sources:
  - title: Build more effective conversations on Amazon Lex with confidence scores
    url: https://aws.amazon.com/blogs/machine-learning/build-more-effective-conversations-on-amazon-lex-with-confidence-scores-and-increased-accuracy/
  - title: Training and Upskilling - Generative AI Atlas
    url: https://awslabs.github.io/generative-ai-atlas/topics/4_0_systematic_path_to_production_framework/4_3_training_upskilling/index.html
  - title: Fundamentals of Machine Learning and Artificial Intelligence
    url: https://explore.skillbuilder.aws/learn/course/external/view/elearning/19578/fundamentals-of-machine-learning-and-artificial-intelligence
  - title: 'AWS SimuLearn: Credit Scoring Automation'
    url: https://explore.skillbuilder.aws/learn/course/external/view/elearning/20092/aws-simulearn-credit-scoring-automation


# ============================================================================
# GAP ADDITIONS (5 questions)
# These address identified coverage gaps in the original 75-question library
# ============================================================================

gap_additions:
- id: LIB-076
  question: How do I orchestrate multimodal data pipelines for enterprise AI systems?
  rationale: Addresses gap in multimodal AI implementation patterns identified in RIU-500
    series analysis
  proposed_answer:
    primary_action: Implement comprehensive multimodal pipeline using Amazon Bedrock's
      cross-modal capabilities with AWS Glue integration patterns.
    implementation: Use Bedrock Nova Multimodal Embeddings for unified processing,
      Glue for ETL orchestration, and Step Functions for workflow coordination. Implement
      cross-modal validation (RIU-503) with quality gates and performance monitoring.
    aws_services:
    - Bedrock
    - Glue
    - Step Functions
    - CloudWatch
    related_rius:
    - RIU-500
    - RIU-501
    - RIU-502
    - RIU-503
    decision_type: TWO_WAY_DOOR
    key_constraint: Multimodal processing requires significant compute resources and
      cost optimization
  difficulty: high
  industries:
  - All
  tags:
  - multimodal
  - data_pipeline
  - orchestration
- id: LIB-077
  question: How do I implement advanced agent reasoning frameworks for production
    deployment?
  rationale: Addresses advanced agentic systems gap identified in RIU-510 series
    analysis
  proposed_answer:
    primary_action: Deploy production-scale AI agents using Amazon Bedrock AgentCore
      with systematic reasoning validation.
    implementation: Use AgentCore's enterprise features including VPC support and
      CloudFormation integration. Implement multi-agent coordination (RIU-510), state
      management (RIU-511), and failure recovery (RIU-512) patterns with comprehensive
      observability.
    aws_services:
    - Bedrock AgentCore
    - Step Functions
    - CloudWatch
    - X-Ray
    related_rius:
    - RIU-510
    - RIU-511
    - RIU-512
    - RIU-513
    - RIU-514
    decision_type: ONE_WAY_DOOR
    key_constraint: Agent reasoning frameworks require extensive validation and human
      oversight mechanisms
  difficulty: critical
  industries:
  - All
  tags:
  - agentic_ai
  - reasoning
  - production_deployment
- id: LIB-078
  question: How do I optimize costs for high-volume LLM deployments while maintaining
    performance?
  rationale: Addresses modern LLMOps cost optimization gap identified in RIU-520
    series analysis
  proposed_answer:
    primary_action: Implement systematic cost optimization using serverless inference
      patterns and intelligent caching strategies.
    implementation: Use SageMaker Serverless Inference for variable workloads, implement
      response caching (RIU-523), token budget management (RIU-522), and automated
      scaling policies. Monitor costs through CloudWatch with automated optimization
      triggers.
    aws_services:
    - SageMaker Serverless
    - ElastiCache
    - CloudWatch
    - Auto Scaling
    related_rius:
    - RIU-520
    - RIU-522
    - RIU-523
    - RIU-524
    decision_type: TWO_WAY_DOOR
    key_constraint: Cost optimization must balance performance requirements with budget
      constraints
  difficulty: high
  industries:
  - All
  tags:
  - cost_optimization
  - llmops
  - performance
  - scaling
- id: LIB-079
  question: How do I implement multi-jurisdictional AI compliance across expanding
    state regulations?
  rationale: Addresses regulatory compliance gap for 20+ state privacy laws effective
    by January 2026
  proposed_answer:
    primary_action: Establish comprehensive multi-jurisdictional compliance framework
      using automated policy enforcement and monitoring.
    implementation: Use Config for policy automation, Security Hub for compliance
      monitoring, and Systems Manager for documentation management. Implement jurisdiction-specific
      data handling with automated compliance validation and reporting across federal,
      state, and international requirements.
    aws_services:
    - Config
    - Security Hub
    - Systems Manager
    - CloudFormation
    related_rius:
    - RIU-530
    - RIU-533
    - RIU-534
    - RIU-541
    decision_type: ONE_WAY_DOOR
    key_constraint: Multi-jurisdictional compliance requires ongoing monitoring of
      regulatory changes
  difficulty: critical
  industries:
  - All
  tags:
  - multi_jurisdictional
  - compliance
  - state_regulations
  - governance
- id: LIB-080
  question: How do I design organizational AI adoption frameworks that overcome resistance
    to automation?
  rationale: Addresses organizational change management gap identified in adoption
    analysis
  proposed_answer:
    primary_action: Implement systematic change management using transparent AI systems
      and comprehensive training programs.
    implementation: Use Connect for stakeholder communication, Systems Manager for
      training content delivery, and QuickSight for adoption metrics visualization.
      Design Glass-Box architecture requirements (RIU-074) with systematic transparency
      and trust-building measures.
    aws_services:
    - Connect
    - Systems Manager
    - QuickSight
    - Amplify
    related_rius:
    - RIU-068
    - RIU-070
    
    
    
    decision_type: TWO_WAY_DOOR
    key_constraint: Organizational adoption requires sustained executive commitment
      and cultural change
  difficulty: high
  industries:
  - All
  tags:
  - organizational_adoption
  - change_management
  - training
  - transparency



# ============================================================================
# USAGE INSTRUCTIONS FOR PALETTE AGENTS
# ============================================================================

usage_instructions:
  for_argentavis:
    description: Resource Gatherer agent should check this library before web search
    workflow:
    - Parse user question for key concepts and problem type
    - Search library by tags, problem_type, or semantic similarity
    - If exact match found, use curated answer as primary source
    - If partial match found, use as context and supplement with web search
    - If no match found, proceed to web search and consider adding to library
    - Log library hits/misses in decisions.md for improvement
    
  for_rex:
    description: Architect agent should reference library for architecture patterns
    workflow:
    - Use library questions as decision framework
    - Cross-reference with RIUs for comprehensive analysis
    - Flag when library answer conflicts with new information
    - Cite library sources when used in recommendations
    
  for_all_agents:
    - Check library for relevant context before external search
    - Cite library sources when used (format "per LIB-XXX")
    - Report library gaps (questions that should be added)
    - Validate library answers against current AWS capabilities
    
  matching_strategy:
    exact_match: Question semantically identical â†’ Use curated answer directly
    partial_match: Question overlaps â†’ Use as context, search for specifics
    no_match: Question outside scope â†’ External search, consider adding
    
  quality_criteria:
    - Specific and answerable (not generic)
    - High-value (saves time or prevents mistakes)
    - Stable (answer doesn't change frequently)
    - Mappable (links to relevant RIUs)

# ============================================================================
# VALIDATION AND MAINTENANCE
# ============================================================================

validation:
  syntax_check:
    status: PASSED
    date: "2026-01-27"
    validator: automated_yaml_parser
    
  cross_reference_check:
    status: PASSED
    date: "2026-01-27"
    notes: All RIU references validated against taxonomy v1.1
    
  aws_services_check:
    status: PASSED
    date: "2026-01-27"
    notes: All AWS service references reflect current capabilities as of January 2026
    
  regulatory_compliance_check:
    status: PASSED
    date: "2026-01-27"
    notes: Compliance guidance addresses current enforcement deadlines
    
maintenance:
  update_frequency:
    quarterly_review: Add high-frequency questions from real usage
    annual_refresh: Remove outdated questions, update AWS service references
    on_demand: Critical updates for regulatory changes or major AWS launches
    
  version_control:
    current_version: "1.0-v1.2-mapped"
    next_planned_version: "1.1"
    changelog_location: fde/decisions.md
    
  quality_gates:
    - Question appears 3+ times across engagements
    - Answer is stable and authoritative
    - Question has high impact (saves significant time or prevents costly mistakes)
    - Question maps to existing RIUs
    
  success_metrics:
    target_library_hit_rate: "30%+"
    target_time_saved_per_hit: "15+ minutes"
    target_user_satisfaction: "4.0+ / 5.0"
    target_coverage: "80%+ of common FDE questions"

# ============================================================================
# AUTHORITY SOURCES (44 references)
# ============================================================================

authority_sources:
  aws_official:
  - AWS Cloud Adoption Framework (AWS CAF)
  - AWS Well-Architected Framework
  - Machine Learning Lens
  - Generative AI Atlas
  - AWS Prescriptive Guidance
  - AWS Whitepapers
  - AWS Service Documentation
  
  regulatory:
  - EU AI Act Official Guidelines
  - HIPAA Compliance Guidance
  - GDPR Requirements
  - SOX Compliance Standards
  - State Privacy Laws (20+ jurisdictions)
  
  industry_best_practices:
  - MLOps Assessment Frameworks
  - AI Governance Standards
  - Enterprise Architecture Patterns
  - Change Management Methodologies
  
  validation_date: "2026-01-27"
  next_review_date: "2026-04-27"

# ============================================================================
# INTEGRATION NOTES
# ============================================================================

integration:
  rag_pipeline:
    embedding_model: Amazon Titan Embeddings or Bedrock Nova
    vector_store: Amazon OpenSearch Service or Amazon Kendra
    retrieval_strategy: Hybrid (semantic + keyword)
    reranking: Bedrock Rerank API
    
  agent_integration:
    argentavis: Check library before web_search tool invocation
    rex: Reference library in architecture decision frameworks
    theri: Use library for implementation patterns
    raptor: Reference library for debugging strategies
    
  monitoring:
    track_library_hits: Log in decisions.md
    track_library_misses: Identify gaps for future additions
    track_answer_quality: User feedback on library responses
    track_coverage: "% of questions answered from library"

# ============================================================================
# END OF PALETTE CURATED KNOWLEDGE LIBRARY v1.0
# ============================================================================


# ============================================================================
# CONTEXT-SPECIFIC ADDITIONS (10 questions)
# Grounded in Mythfall research and Palette implementation experience
# ============================================================================

context_specific_questions:

- id: LIB-081
  question: How do I choose between client-server and P2P architecture for small-scale multiplayer games?
  answer: |
    For 4-player co-op games with small teams, the decision depends on your constraints:
    
    **Client-Server (Recommended for Mythfall-style games)**:
    - **Cost**: $0.48-$0.72/user/month using Vultr VPS (per your research)
    - **Pros**: Server authority prevents cheating, easier to patch/update server-side, scales to more players, better for asynchronous gameplay
    - **Cons**: Hosting costs, server becomes single point of failure, requires backend infrastructure
    - **Best for**: Games where anti-cheat matters, competitive gameplay, or you plan to scale beyond 8 players
    - **Implementation**: Godot 4.3+ with Socket.io on Vultr VPS achieves sub-100ms latency for 1,000 concurrent connections
    
    **Peer-to-Peer**:
    - **Cost**: Zero hosting (but TURN servers cost $99-150+/month for 150GB bandwidth if NAT traversal fails)
    - **Pros**: No hosting costs for successful connections, lower latency for direct connections
    - **Cons**: Host advantage, vulnerable to cheating, limited to 4-8 players, 85% NAT traversal success rate means 15% need TURN
    - **Best for**: Co-op games where cheating doesn't matter, very small player counts, zero budget
    - **Implementation**: WebRTC with STUN/TURN fallback
    
    **Hybrid Approach (Best of both)**:
    - Start with P2P for development/testing (zero cost)
    - Add optional dedicated server for players who want it
    - Allows community-hosted servers while maintaining official option
    
    **For your Mythfall context**: Client-server is recommended given your $0.48/user research findings and 4-player co-op focus. The cost is manageable and prevents the 15% TURN server failure case.
  problem_type: Systems_Integration
  related_rius: [RIU-060, RIU-061, RIU-062, RIU-063]
  difficulty: high
  industries: [Gaming, Indie Development]
  tags: [multiplayer, architecture, client-server, p2p, cost-optimization]
  sources:
  - title: Optimal Multiplayer Game Infrastructure Analysis
    url: file:///home/mical/Myth-Fall-Game/Optimal Multiplayer Game Infrastructure-.pdf
  - title: WebRTC NAT Traversal Success Rates
    url: Research finding - 85% success with STUN alone

- id: LIB-082
  question: What can a small team (under 20 people) realistically achieve in game development?
  answer: |
    Based on proven examples from your research:
    
    **Hades (Supergiant Games) - 20 core people**:
    - Result: 1M+ copies sold, Game of the Year 2020
    - Approach: 2D isometric, stylized art (not photorealistic)
    - Complexity: MEDIUM (roguelike structure = replayability without massive content)
    - Key insight: Strong design + focused scope > large team + weak design
    
    **Dead Cells (Motion Twin) - Small worker cooperative**:
    - Result: Critical acclaim, strong indie success
    - Approach: 2D roguelite-Metroidvania
    - Complexity: LOW-MEDIUM
    
    **Slay the Spire (MegaCrit) - Very small indie**:
    - Result: Genre-defining hit
    - Approach: Card-based, minimal graphics
    - Complexity: LOW
    
    **Pattern**: Small teams succeed by:
    1. Choosing stylized art over photorealism (reduces art load dramatically)
    2. Using roguelike/roguelite structure (replayability without massive content creation)
    3. Focusing on strong core mechanics over feature breadth
    4. Targeting 3-6 month delivery cycles to build confidence
    
    **For 2-person teams (your context)**:
    - Achievable: 4-player co-op with strong core loop, stylized graphics, procedural elements
    - Not achievable: AAA graphics, massive open world, 100+ hours of unique content
    - Timeline: 12-16 weeks for MVP with systematic implementation (per your infrastructure research)
    
    **Critical**: Engineering complexity â‰  success. Hades beat AAA competitors with 20 people.
  problem_type: Operationalization_and_Scaling
  related_rius: [RIU-120, RIU-121, RIU-003]
  difficulty: medium
  industries: [Gaming, Indie Development]
  tags: [small-team, indie-development, scope-management, team-size]
  sources:
  - title: Mythical Games Research - Hades Analysis
    url: file:///home/mical/Myth-Fall-Game/RESEARCH_HANDOFF_ADAM.md
  - title: Optimal Multiplayer Game Infrastructure - Timeline
    url: file:///home/mical/Myth-Fall-Game/Optimal Multiplayer Game Infrastructure-.pdf


- id: LIB-083
  question: How do I optimize multiplayer game hosting costs for indie budgets?
  answer: |
    Based on systematic cost analysis across providers:
    
    **VPS Providers (Most Cost-Effective)**:
    - **Vultr** (Recommended): $0.48-$0.72/user/month at 1,000 user scale
      - 100 users: Vultr High Performance 2GB at $18/month
      - 1,000 users: 12-15 distributed instances at $28/month each
      - Bandwidth: $0.01/GB overage (global pooling across 32 data centers)
    - **Linode**: $36/month for 4GB dedicated CPU, $0.005/GB bandwidth overage
    - **DigitalOcean**: $42/month for 4GB CPU-optimized, 4,000-5,000GB bandwidth included
    
    **Traditional Cloud (5-47x more expensive)**:
    - AWS GameLift: $1.60-$4.75/user/month (reducible to ~$1 with spot instances)
    - Google Cloud: $1,200/month for 1,000 users
    - Azure: $1,400/month for 1,000 users
    - **Avoid unless you need enterprise features**
    
    **Serverless (Moderate cost, operational simplicity)**:
    - Cloudflare Workers: $5/month minimum, $0.30/million requests, NO bandwidth charges
    - Optimization: WebSocket Hibernation API reduces costs to ~$10/month
    - Fly.io: $2.02-$1,013.80/month depending on instance type, $0.02/GB bandwidth
    
    **Technology Stack (Your Research)**:
    - **Engine**: Godot 4.3+ (MIT license, zero cost, 2.4MB builds with Brotli compression)
    - **Networking**: Socket.io (free, open-source, 9,000-10,000 msg/sec per CPU core)
    - **Database**: PostgreSQL 18 (free, asynchronous I/O for high concurrency)
    - **Deployment**: Docker multi-stage builds (90% size reduction)
    - **Monitoring**: Prometheus + Grafana (free, open-source)
    
    **Cost Optimization Strategies**:
    1. Geographic distribution across Vultr regions (global bandwidth pooling)
    2. Connection pooling and read replicas for database
    3. CDN integration for static assets
    4. Auto-scaling based on actual load
    5. Open-source stack (zero licensing costs)
    
    **Target achieved**: $0.48/user/month vs $0.10 target (closest viable solution per research)
  problem_type: Operationalization_and_Scaling
  related_rius: [RIU-120, RIU-121, RIU-064]
  difficulty: high
  industries: [Gaming, Indie Development]
  tags: [cost-optimization, hosting, vps, multiplayer, infrastructure]
  sources:
  - title: Optimal Multiplayer Game Infrastructure - Cost Analysis
    url: file:///home/mical/Myth-Fall-Game/Optimal Multiplayer Game Infrastructure-.pdf

- id: LIB-084
  question: What game engine should I choose for web-first indie multiplayer games?
  answer: |
    Based on comprehensive engine evaluation:
    
    **Godot 4.3+ (Recommended for your context)**:
    - **Cost**: MIT license, zero royalties, completely free
    - **Web Export**: Single-threaded exports (no SharedArrayBuffer), works on Apple devices and itch.io
    - **Multiplayer**: High-level API supporting ENet, WebRTC, custom implementations
    - **Bundle Size**: 2.4MB with Brotli compression (down from 40MB uncompressed)
    - **Best for**: Cost-conscious teams, web-first deployment, open-source preference
    - **Limitations**: Smaller asset store than Unity, less enterprise tooling
    
    **Unity (Feature-rich but complex)**:
    - **Cost**: Free under $200K revenue, then subscription required
    - **Multiplayer Options**:
      - Mirror: Free, open-source, WebGL support via WebSockets
      - Netcode for GameObjects: Free but WebGL client-only (no host mode in browser)
      - Photon Fusion: Subscription-based, full WebGL support
    - **Best for**: Teams with Unity experience, need for extensive asset store
    - **Limitations**: Licensing costs at scale, WebGL export requires optimization
    
    **Web-Native Engines (Maximum accessibility)**:
    - **Three.js**: 1.8M weekly downloads, 168.4 kB bundle, seamless Socket.io integration
    - **Babylon.js**: Complete 3D engine, Microsoft-backed, official Colyseus integration
    - **Phaser 3**: Dominates 2D web games, 37,800 GitHub stars, extensive Socket.io tutorials
    - **Best for**: Web developers, immediate browser deployment, zero platform dependencies
    - **Limitations**: Less comprehensive than full game engines, more manual work
    
    **For Mythfall (4-player co-op, mythical theme, small team)**:
    - **Primary recommendation**: Godot 4.3+ with Socket.io
    - **Rationale**: Zero cost, proven web export, your research validates this stack
    - **Alternative**: Three.js if team has strong web dev background
    
    **Decision factors**:
    - Budget: Godot wins (zero cost)
    - Web-first: All options work, Godot 4.3 resolved previous issues
    - Team size: Godot or web-native (less complexity than Unity)
    - Development velocity: Web-native fastest for prototyping, Godot for full game
  problem_type: Systems_Integration
  related_rius: [RIU-060, RIU-061, RIU-003]
  difficulty: high
  industries: [Gaming, Indie Development]
  tags: [game-engine, godot, unity, web-development, technology-selection]
  sources:
  - title: Optimal Multiplayer Game Infrastructure - Engine Evaluation
    url: file:///home/mical/Myth-Fall-Game/Optimal Multiplayer Game Infrastructure-.pdf
  - title: Godot 4.3 Web Export Improvements
    url: Research finding - single-threaded exports, SharedArrayBuffer elimination


- id: LIB-091
  question: How do I design effective agent fixtures for validation?
  answer: |
    Based on Palette agent implementation experience (Argentavis ARG-001, Rex REX-001):
    
    **Fixture Structure**:
    ```
    # Fixture: [Descriptive Name]
    Fixture ID: [AGENT-###]
    Agent: [Agent Name] v[Version]
    Scenario: [What situation this tests]
    
    ## Input
    Initial Request: [Exact user input]
    Expected Clarifying Questions: [List of questions agent should ask]
    Sample Answers: [Realistic responses]
    
    ## Expected Output
    [Structured format agent should produce]
    
    ## Success Criteria
    âœ… [Specific, testable criterion]
    âœ… [Specific, testable criterion]
    
    ## Notes
    [What this fixture validates about agent behavior]
    ```
    
    **Key Principles**:
    1. **Test ONE behavior per fixture** - Don't combine multiple scenarios
    2. **Use real examples** - ARG-001 tests "multiplayer networking research" (actual Mythfall need)
    3. **Make success criteria binary** - "Agent asked all 5 questions" not "Agent asked good questions"
    4. **Include failure modes** - What should agent NOT do?
    5. **Provide realistic inputs** - Not edge cases first, test common path
    
    **Fixture Naming**:
    - Format: `[ARK-CODE]-###-[descriptive-name].md`
    - Examples: `ARG-001-multiplayer-networking.md`, `REX-001-multiplayer-architecture.md`
    - Store in: `fde/agents/[agent-name]/fixtures/`
    
    **What to test**:
    - **Clarification phase**: Does agent ask right questions before acting?
    - **Constraint adherence**: Does agent respect its disallowed actions?
    - **Output format**: Does agent produce expected structure?
    - **Decision classification**: Does agent correctly flag ONE-WAY DOOR vs TWO-WAY DOOR?
    
    **Validation workflow**:
    1. Run agent with fixture input
    2. Compare output against success criteria
    3. Log result in decisions.md (success/fail)
    4. Update agent maturity tracking (impressions, fail_gap)
    
    **From Palette experience**:
    - ARG-001 validates Argy's clarification-before-search behavior
    - REX-001 validates Rex's multi-option tradeoff analysis
    - Both test agent stays within role boundaries
  problem_type: Reliability_and_Failure_Handling
  related_rius: [RIU-540, RIU-100, RIU-101]
  difficulty: medium
  industries: [All]
  tags: [agent-design, testing, validation, fixtures, palette-meta]
  sources:
  - title: Argentavis Fixture ARG-001
    url: file:///home/mical/Myth-Fall-Game/fde/agents/argentavis/fixtures/ARG-001-multiplayer-networking.md
  - title: Rex Fixture REX-001
    url: file:///home/mical/Myth-Fall-Game/fde/agents/rex/fixtures/REX-001-multiplayer-architecture.md

- id: LIB-092
  question: How does agent maturity progression work in Palette?
  answer: |
    Palette uses a three-tier maturity model based on empirical reliability:
    
    **Tier 1: UNVALIDATED**
    - **Status**: New agent, unproven
    - **Behavior**: Human-in-the-loop required for EACH execution
    - **Promotion**: 10 consecutive successes
    - **Use when**: Agent just created, major version bump (V1â†’V2)
    
    **Tier 2: WORKING**
    - **Status**: Proven reliable, but monitored
    - **Behavior**: Autonomous execution with review
    - **Promotion**: 50 impressions with <5% failure rate
    - **Demotion**: Failure occurs while fail_gap â‰¤ 9 â†’ demote to Tier 1
    - **Use when**: Agent works but needs validation at scale
    
    **Tier 3: PRODUCTION**
    - **Status**: Fully trusted, autonomous
    - **Behavior**: Fully autonomous until failure
    - **Demotion**: Two failures within any 10 impressions (fail_gap â‰¤ 9) â†’ demote to Tier 2
    - **Use when**: Agent has proven reliability over time
    
    **Tracking Format** (in decisions.md):
    ```
    agent: argentavis
    ark_type: ARK:Argentavis
    version: 1.0
    status: UNVALIDATED
    impressions:
      success: 0
      fail: 0
      fail_gap: 0
    notes: First agent implementation
    ```
    
    **On Success**:
    - Increment `success` count
    - Increment `fail_gap` (runs since last failure)
    - Check promotion criteria
    
    **On Failure**:
    - If `fail_gap â‰¤ 9`: Demote per tier rules
    - Set `fail_gap = 0`
    - Increment `fail` count
    - Log post-mortem in decisions.md
    
    **Versioning Impact**:
    - **Major bump** (V1.0 â†’ V2.0): Resets impressions + fail_gap, back to UNVALIDATED
    - **Minor bump** (V1.0 â†’ V1.1): Preserves impressions + fail_gap
    
    **Key Insight**: Maturity is about measured trust, not function. A simple agent at PRODUCTION is more valuable than a complex agent at UNVALIDATED.
    
    **Current Palette status** (as of implementation):
    - Argentavis: 1 success, 0 failures, UNVALIDATED (needs 9 more successes)
    - Rex: 0 executions, UNVALIDATED (awaiting first use)
  problem_type: Reliability_and_Failure_Handling
  related_rius: [RIU-100, RIU-101, RIU-511]
  difficulty: medium
  industries: [All]
  tags: [agent-maturity, reliability, trust-model, palette-meta]
  sources:
  - title: Palette Tier 2 - Agent Maturity Model
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER2_assumptions.md
  - title: Palette decisions.md - Agent Tracking
    url: file:///home/mical/Myth-Fall-Game/fde/decisions.md


- id: LIB-093
  question: How do I identify ONE-WAY DOOR decisions in AI and game development?
  answer: |
    ONE-WAY DOOR decisions are irreversible or high-cost to undo. Per Palette Tier 1:
    
    **ONE-WAY DOOR Indicators**:
    - "This will affect every system we build"
    - "Changing this later requires rewriting X"
    - "This locks us into vendor Y"
    - "This determines our scaling limits"
    - "This affects our cost structure permanently"
    
    **AI/ML ONE-WAY DOORS**:
    - Ethical AI guidelines and data governance frameworks
    - Model architecture selection (fine-tuning vs RAG vs prompt engineering)
    - Production deployment commitments
    - Database schema for ML features
    - Compliance framework selection (EU AI Act, HIPAA, SOX)
    
    **Game Development ONE-WAY DOORS**:
    - Game engine selection (Godot vs Unity vs Unreal)
    - Network architecture (client-server vs P2P)
    - Database choice (PostgreSQL vs MongoDB vs DynamoDB)
    - Authentication system (OAuth, JWT, custom)
    - Core game mechanics structure (affects all future content)
    - Deployment platform (web vs native vs hybrid)
    
    **TWO-WAY DOOR Examples** (easily reversible):
    - Hyperparameter tuning
    - Prompt iterations
    - UI framework choice
    - Logging library
    - Asset organization
    - Development workflow
    
    **Protocol**:
    1. **Flag explicitly**: "ðŸš¨ ONE-WAY DOOR â€” confirmation required before proceeding"
    2. **Pause execution**: Human confirmation mandatory
    3. **Log in decisions.md**: Must include explicit rationale
    4. **Toolkit-changing decisions**: Also add to manual header list in decisions.md
    
    **When uncertain**: Treat as ONE-WAY DOOR (safer to over-flag than under-flag)
    
    **From Palette implementation**:
    - Agent archetype selection was ONE-WAY DOOR (affects all future agent design)
    - Fixture format was ONE-WAY DOOR (establishes validation pattern)
    - Argy's clarification protocol was TWO-WAY DOOR (can iterate on questions)
  problem_type: Intake_and_Convergence
  related_rius: [RIU-001, RIU-003]
  difficulty: high
  industries: [All]
  tags: [decision-framework, one-way-door, risk-management, architecture]
  sources:
  - title: Palette Tier 1 - Decision Handling
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER1_palette_core.md
  - title: Palette Tier 2 - Decision Safety Model
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER2_assumptions.md

- id: LIB-095
  question: What are the required elements of a Semantic Blueprint (Convergence Brief)?
  answer: |
    Per Palette Tier 1, every engagement MUST produce a Semantic Blueprint with these five elements:
    
    **1. Goal** (What success looks like - concrete, measurable):
    - Must be measurable: "Reduce churn 5%" not "Improve experience"
    - Must be concrete: "80% accuracy on 50 test cases" not "Good performance"
    - Must be business outcome: "Launch before Q3" not "Build feature X"
    - Examples: "Reduce ML inference latency to <200ms at p95", "Process 10K transactions/sec with <0.1% error rate", "Deploy compliant HIPAA solution by Q2"
    
    **2. Roles** (Who/what is responsible - human vs agent boundaries):
    - Decision authority: Who makes final calls?
    - Execution responsibility: Who builds what?
    - Review authority: Who validates?
    - Examples: "FDE designs architecture, Kiro implements, customer validates", "Data scientist owns model, MLOps owns deployment, compliance owns audit"
    
    **3. Capabilities** (What tools/agents are needed):
    - Required agents: Argy for research, Rex for architecture, Theri for building
    - Required tools: web_search, fs_write, execute_bash, use_aws
    - Required infrastructure: Cloud services, databases, APIs, frameworks
    - Examples: "Need Argy (research), Rex (architecture), AWS CDK, PostgreSQL", "Need web_search, SageMaker, Lambda, DynamoDB"
    
    **4. Constraints** (What cannot be changed - technical, policy, timeline):
    - Hard boundaries: Budget cap, compliance requirements, timeline
    - Technical limits: Team size, existing infrastructure, data restrictions
    - Policy limits: Security requirements, regulatory compliance
    - Examples: "3-person team, $5K/month budget, 8-week timeline, must use existing VPC", "HIPAA compliance required, no PII in logs, SOC 2 audit in 6 months"
    
    **5. Non-goals** (What is explicitly out of scope):
    - Critical for preventing scope creep
    - Must be explicit to prevent expansion
    - Examples: "NOT building mobile apps, NOT supporting real-time streaming, NOT migrating legacy systems", "NOT implementing custom ML models, NOT building admin UI, NOT handling multi-region"
    
    **Why this matters**:
    - Forces clarity before execution
    - Prevents scope creep
    - Enables restartability (new person can read and continue)
    - Provides decision framework for ONE-WAY DOOR calls
    
    **Implementation**: The Convergence Brief (RIU-001) serves as the semantic blueprint. Must be documented in decisions.md before execution begins.
    
    **Validation test**: If you can't fill all 5 elements, you haven't converged yet.
  problem_type: Intake_and_Convergence
  related_rius: [RIU-001, RIU-002]
  difficulty: medium
  industries: [All]
  tags: [convergence, semantic-blueprint, requirements, planning]
  sources:
  - title: Palette Tier 1 - Semantic Blueprint
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER1_palette_core.md


- id: LIB-096
  question: How do I select RIUs for a given problem in Palette?
  answer: |
    Per Palette Tier 3 workflow, RIU selection follows this process:
    
    **Step 1: Extract Observed Trigger Signals**
    - List explicit signals from engagement input (bullet format)
    - Example: "Stakeholders have conflicting goals", "No clear success metric", "Requirements change weekly"
    
    **Step 2: Retrieve BROAD Candidate RIUs** (aim 8-15):
    - Search taxonomy by trigger_signals (first-class evidence)
    - Use problem_type as soft anchor (not constraint)
    - Bias toward coverage + relevance, not premature narrowing
    - "NO MATCH" is valid - surfaces gaps explicitly
    
    **Step 3: Indicate Match Strength**:
    - **STRONG**: Trigger signals directly match, problem pattern aligns
    - **MODERATE**: Partial match, some signals align
    - **WEAK**: Tangential relevance, consider but don't force
    
    **Step 4: Narrow to Focused Selection** (3-5 RIUs):
    - Prioritize STRONG matches
    - Consider dependencies (some RIUs require others first)
    - Check reversibility (prefer TWO-WAY DOOR RIUs early)
    - Validate against constraints
    
    **Matching Rules**:
    - Coordinates (industry/category/use_case) are soft anchors only
    - Multiple RIUs may apply simultaneously
    - When uncertain, prefer broader coverage over forced fit
    - If no match, surface gap and consider creating new RIU
    
    **Example** (from Palette implementation):
    - **Problem**: "Build first Palette agent"
    - **Trigger Signals**: "New agent needed", "No validation framework", "Need test scenarios"
    - **Candidates**: RIU-511 (Agent Capability Design), RIU-540 (Agent Fixture Design), RIU-003 (Implementation Scoping)
    - **Match Strength**: All STRONG
    - **Selected**: All 3 (logged in decisions.md)
    
    **Anti-patterns**:
    - Don't force RIU match when none fits (surface gap instead)
    - Don't select RIUs based on name alone (check trigger_signals)
    - Don't skip broad candidate phase (premature narrowing loses options)
  problem_type: Intake_and_Convergence
  related_rius: [RIU-001, RIU-002, RIU-003, RIU-004]
  difficulty: medium
  industries: [All]
  tags: [riu-selection, taxonomy, workflow, palette-meta]
  sources:
  - title: Palette Tier 3 - RIU Matching Rules
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER3_decisions_prompt.md
  - title: Palette Taxonomy v1.1
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/palette_taxonomy_v1_1.yaml

- id: LIB-097
  question: What must be logged in decisions.md for restartability?
  answer: |
    Per Palette Tier 1 and Tier 3, decisions.md is append-only and must contain:
    
    **A) Toolkit-Changing ONE-WAY DOOR Decisions** (manual header list):
    - Decisions that change Palette itself
    - Example: "Promoted Orchestrator from DESIGN-ONLY to implemented"
    - Keep this section small and current
    
    **B) Engagement Updates** (append new blocks):
    
    **Required in each update**:
    1. **Semantic Blueprint** (or reference to existing):
       - Goal, Roles, Capabilities, Constraints, Non-goals
       - What changed since last update
    
    2. **Selected RIUs**:
       - List of RIUs applied (with match strength if relevant)
       - Rationale for selection
    
    3. **Artifacts Created/Updated**:
       - File paths
       - What each artifact does
    
    4. **Agent Maturity Updates**:
       - Success/failure logged
       - Impressions updated
       - Tier changes noted
    
    5. **Next Checks**:
       - What needs to happen next
       - Open questions or blockers
    
    **Optional (log if material)**:
    - Post-mortems when agents fail
    - Assumptions that drove decisions
    - Knowledge gaps detected (reference KGE-ID)
    
    **Do NOT log**:
    - Exhaustive execution logs
    - Every file touched
    - Every source consulted
    - Routine TWO-WAY DOOR decisions (unless they fail or affect restartability)
    
    **Format**:
    ```
    ---
    ### Engagement Update: YYYY-MM-DD / [DESCRIPTIVE-ID]
    
    #### Semantic Blueprint
    [5 elements or reference]
    
    #### Selected RIUs
    [List with rationale]
    
    #### Artifacts Created
    [Paths and descriptions]
    
    #### Agent Maturity Update
    [Tracking block]
    
    #### Next Checks
    [What's next]
    
    ---
    ```
    
    **Purpose**: Enable restartability from scratch using existing documentation. Someone should be able to read decisions.md and understand what was built, why, and what's next.
    
    **From Palette experience**:
    - Bootstrap entry documents initial toolkit setup
    - Argentavis implementation logged with RIU selection and artifacts
    - Rex implementation logged with maturity tracking
  problem_type: Trust_Governance_and_Adoption
  related_rius: [RIU-003, RIU-004, RIU-001]
  difficulty: medium
  industries: [All]
  tags: [decision-logging, documentation, restartability, palette-meta]
  sources:
  - title: Palette Tier 1 - Decision Persistence
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER1_palette_core.md
  - title: Palette Tier 3 - Engagement Update Template
    url: file:///home/mical/Myth-Fall-Game/.kiro/steering/palette/TIER3_decisions_prompt.md
  - title: Palette decisions.md - Live Example
    url: file:///home/mical/Myth-Fall-Game/fde/decisions.md



- id: LIB-087
  
  question: |
    How can I make multi-agent workflows more readable in execution logs?
  
  answer: |
    Use semantic color coding to visualize agent handoffs and coordination.
    
    Palette's 8 agents have semantic colors:
    - Argentavis (Argy): Blue ðŸ”µ - Research/trust/water
    - Therizinosaurus (Theri): Orange ðŸŸ  - Build/action/fire
    - Velociraptor (Raptor): Red ðŸ”´ - Debug/alert/critical
    - Tyrannosaurus (Rex): Purple ðŸŸ£ - Design/vision/architecture
    - Yutyrannus (Yuty): Green ðŸŸ¢ - Narrative/growth/GTM
    - Ankylosaurus (Anky): Gray âšª - Validate/stability/foundation
    - Parasaurolophus (Para): Yellow ðŸŸ¡ - Signal/monitor/light
    - Orchestrator (Orch): White âš« - Coordinate/neutral/air
    
    In decisions.md, use colors/emojis to show workflow:
    "Argy (ðŸ”µ) researched â†’ Rex (ðŸŸ£) designed â†’ Theri (ðŸŸ ) built â†’ 
     Anky (âšª) validated â†’ Para (ðŸŸ¡) monitored"
    
    This improves readability by:
    - Making agent roles visually distinct
    - Showing handoff points clearly
    - Enabling quick workflow scanning
    - Reducing cognitive load when reading logs
    
    Use when: Multi-agent engagements (3+ agents)
    Skip when: Single-agent executions (unnecessary overhead)
  
  problem_type: Operations_and_Delivery
  
  related_rius: [RIU-530, RIU-531, RIU-532]
  
  difficulty: low
  
  industries: [All]
  
  sources:
    - title: "Palette UX Engagement - Cross-Domain Pattern 1"
      url: "internal://ux-engagement-2026-02-01"
      type: "validated_pattern"
    
    - title: "Semantic Color Coding in Developer Tools"
      url: "https://www.webportfolios.dev/guides/best-color-palettes-for-developer-portfolio"
      type: "research"

- id: LIB-088
  
  question: |
    What should a convergence brief include to accelerate alignment?
  
  answer: |
    Use a structured 5-section template to break the learning curve 
    into manageable segments with clear milestones.
    
    Convergence Brief Template:
    
    1. Problem (What are we solving?)
       - Current state vs desired state
       - Why this matters now
       - Who is affected
    
    2. Context (Constraints, stakeholders, timeline)
       - Binding requirements (hard constraints)
       - Key stakeholders and their roles
       - Timeline and milestones
       - Available resources (agents, tools, budget)
    
    3. Success Criteria (How do we know we're done?)
       - Execution success (artifacts produced, tests passed)
       - Outcome success (business value delivered)
       - Quality success (validation criteria met)
    
    4. Non-goals (What's explicitly out of scope?)
       - Prevents scope creep
       - Makes boundaries clear
       - Enables focused execution
    
    5. Next Steps (What happens after convergence?)
       - Agent routing (who executes what)
       - RIU selection (which patterns apply)
       - ONE-WAY DOOR decisions (approval required)
    
    This structure accelerates convergence by:
    - Reducing back-and-forth clarification
    - Making implicit assumptions explicit
    - Creating shared mental model quickly
    - Enabling fast validation of understanding
    
    Based on onboarding research: Structured pathways reduce 
    ramp time by 30-40% compared to unstructured approaches.
  
  problem_type: Intake_and_Convergence
  
  related_rius: [RIU-001, RIU-004, RIU-005]
  
  difficulty: medium
  
  industries: [All]
  
  sources:
    - title: "Palette UX Engagement - Cross-Domain Pattern 2"
      url: "internal://ux-engagement-2026-02-01"
      type: "validated_pattern"
    
    - title: "Engineering Onboarding: The Key to DevEx Success"
      url: "https://www.cortex.io/post/engineering-onboarding-the-key-to-devex-success"
      type: "research"

- id: LIB-089
  question: How do I implement least privilege for AI agents?
  answer: |
    Agents should only have access to resources required for their specific role.
    
    Principles:
    - Grant minimum permissions needed to accomplish the task
    - Separate agent identity from user identity and service accounts
    - Use role-based access control (RBAC) for agent permissions
    - Audit and log all agent actions for accountability
    
    Implementation:
    - Define agent roles with explicit permission boundaries
    - Use policy engines to enforce constraints outside model reasoning
    - Implement "before_tool" callbacks to validate parameters
    - Require explicit approval for elevated permissions
    
    Example: Research agent (Argy) gets read-only database access. 
    Build agent (Theri) gets write access only to /src directory.
    Architecture agent (Rex) designs security posture but cannot execute.
    
    Blast radius containment:
    If one agent is compromised, damage is limited to its permission scope.
  
  problem_type: Trust_Governance_and_Adoption
  
  related_rius: [RIU-105]
  
  difficulty: high
  
  industries: [All]
  
  sources:
    - title: "Google Introduction to Agents (Nov 2025) - Agent Identity section"
      url: "https://cloud.google.com/use-cases/agents"
      type: "industry_standard"
    
    - title: "SPIFFE standard for agent identity"
      url: "https://spiffe.io"
      type: "technical_standard"
    
    - title: "Palette Tier 2 - Agent Security section"
      url: "internal://tier2/assumptions.md#agent-security"
      type: "framework"

- id: LIB-090
  question: How do I add guardrails to prevent agent misuse?
  answer: |
    Guardrails are deterministic rules that constrain agent behavior 
    outside the model's reasoning (defense-in-depth).
    
    Two-layer approach:
    
    Layer 1: Deterministic Guardrails (code-based)
    - Hard limits on agent actions (e.g., no purchases >$100)
    - Require human confirmation for irreversible actions (ONE-WAY DOOR)
    - Block access to sensitive APIs without explicit approval
    - Validate tool parameters before execution
    - Enforce rate limits and resource quotas
    
    Layer 2: Reasoning-Based Defenses (AI-powered)
    - Use "LM as Judge" to screen inputs/outputs for policy violations
    - Adversarial training to resist prompt injection
    - Specialized guard models flag risky plans before execution
    - Context-aware policy evaluation
    
    Best practice: Combine both layers. Code provides predictable limits,
    AI provides contextual awareness.
    
    Example guardrail implementation:
    Before agent sends email:
    1. Check: Is recipient on approved list? (deterministic)
    2. Check: Does content violate tone policy? (LM judge)
    3. Check: Does email contain PII without consent? (deterministic + LM)
    4. If any check fails: Block action, log attempt, alert human
    
    Integration with Palette:
    - Rex designs guardrail architecture (what to constrain)
    - Theri implements guardrails (code + configuration)
    - Anky validates guardrails work as intended
    - Para monitors for guardrail violations in production
  
  problem_type: Trust_Governance_and_Adoption
  
  related_rius: [RIU-105]
  
  difficulty: high
  
  industries: [All]
  
  sources:
    - title: "Google Introduction to Agents (Nov 2025) - Security section"
      url: "https://cloud.google.com/use-cases/agents"
      type: "industry_standard"
    
    - title: "OWASP Top 10 for LLM Applications"
      url: "https://owasp.org/www-project-top-10-for-large-language-model-applications/"
      type: "security_standard"
    
    - title: "Palette Tier 2 - Agent Security section"
      url: "internal://tier2/assumptions.md#agent-security"
      type: "framework"

- id: LIB-091
  question: How do I manage agent identity and authentication?
  answer: |
    Agents are a new class of principal (distinct from users and services).
    Each agent needs verifiable identity for access control and audit.
    
    Three types of principals:
    1. Users: Authenticated with OAuth/SSO (human actors, full autonomy)
    2. Agents: Verified with SPIFFE or similar (delegated authority)
    3. Service accounts: IAM-managed (deterministic applications)
    
    Agent identity requirements:
    - Cryptographically verifiable (e.g., SPIFFE, mTLS)
    - Distinct from user who invoked it
    - Distinct from developer who built it
    - Granular permissions (least privilege per agent role)
    - Auditable (all actions traceable to agent identity)
    
    Why this matters:
    - Enables audit trails (which agent did what, when, why)
    - Limits blast radius if agent is compromised
    - Allows delegation of authority (agent acts on behalf of user)
    - Supports compliance requirements (SOC2, GDPR, HIPAA)
    
    Implementation pattern:
    1. Issue unique identity to each agent instance
    2. Map identity to role-based permissions
    3. Validate identity before every tool invocation
    4. Log all actions with agent identity + timestamp
    5. Rotate credentials regularly
    
    Example: SalesAgent gets CRM read/write access. HRAgent explicitly denied.
    If SalesAgent is compromised, HR data remains protected.
    
    Palette integration:
    - Each agent archetype (Argy, Rex, Theri, etc.) has default permission profile
    - Instances inherit profile but can be further restricted
    - Rex designs identity architecture for multi-agent systems
    - Anky validates identity implementation meets security requirements
  
  problem_type: Trust_Governance_and_Adoption
  
  related_rius: [RIU-105]
  
  difficulty: high
  
  industries: [All]
  
  sources:
    - title: "Google Introduction to Agents (Nov 2025) - Agent Identity section"
      url: "https://cloud.google.com/use-cases/agents"
      type: "industry_standard"
    
    - title: "SPIFFE standard"
      url: "https://spiffe.io"
      type: "technical_standard"
    
    - title: "Palette Tier 2 - Agent Security section"
      url: "internal://tier2/assumptions.md#agent-security"
      type: "framework"

- id: LIB-092
  question: How do I classify decisions as reversible vs irreversible?
  answer: |
    All material decisions must be classified before execution.
    
    TWO-WAY DOOR (Reversible):
    - Cheap to undo or change
    - Low organizational impact
    - Can be rolled back without significant cost
    - AI may proceed autonomously
    - Log only if material or if it fails
    - Examples: refactoring code, A/B testing, updating docs, 
      changing variable names, adjusting UI layout
    
    ONE-WAY DOOR (Irreversible):
    - Hard or expensive to reverse
    - High organizational impact
    - Externally binding commitments
    - Requires explicit human approval before proceeding
    - Must be logged with rationale in decisions.md
    - Examples: database selection, architecture commitments, 
      deployments, data deletion, security posture changes,
      API contracts, compliance decisions
    
    The "Trust Trade-Off":
    - Agents need power to be useful (autonomy, tools, access)
    - Every ounce of power introduces risk
    - ONE-WAY DOOR classification manages this trade-off
    - Give agents "a leash long enough to do their job, 
      but short enough to keep them from running into traffic"
    
    Who performs classification:
    - Primary: Rex (Architecture agent) - designs systems, identifies ONE-WAY DOORs
    - Secondary: Any agent that detects a ONE-WAY DOOR must flag and stop
    - Validation: Anky reviews classification during quality checks
    - Override: Human can reclassify if agent gets it wrong
    
    Decision process:
    1. Agent identifies decision point
    2. Classifies as TWO-WAY or ONE-WAY DOOR
    3. If ONE-WAY: Emit "ðŸš¨ ONE-WAY DOOR â€” confirmation required"
    4. Pause execution, present rationale to human
    5. Wait for explicit approval before proceeding
    6. Log decision + rationale in decisions.md
    7. If TWO-WAY: Proceed, log if material
    
    Cost of getting it wrong:
    - Treating ONE-WAY as TWO-WAY: Silent commitments, locked-in risk, 
      irreversible harm, loss of trust
    - Treating TWO-WAY as ONE-WAY: Unnecessary friction, slowed velocity, 
      reduced agent utility
    
    Edge cases:
    - When uncertain: Default to ONE-WAY DOOR (safer)
    - Context matters: Deleting test data = TWO-WAY, deleting prod data = ONE-WAY
    - Cumulative effect: Multiple TWO-WAY decisions can compound into ONE-WAY impact
    
    Integration with Palette:
    - Tier 1 defines the principle (immutable)
    - This Library entry provides implementation guidance (reusable)
    - RIU-001 (Convergence Brief) includes decision classification
    - RIU-105 (Security) - security decisions are often ONE-WAY DOOR
  
  problem_type: Intake_and_Convergence
  
  related_rius: [RIU-001, RIU-105]
  
  difficulty: medium
  
  industries: [All]
  
  sources:
    - title: "Amazon's 'one-way door' decision framework (Jeff Bezos shareholder letters)"
      url: "https://www.amazon.com/p/feature/z6o9g6sysxur57t"
      type: "industry_standard"
    
    - title: "Google Introduction to Agents (Nov 2025) - Trust trade-off concept"
      url: "https://cloud.google.com/use-cases/agents"
      type: "industry_standard"
    
    - title: "Palette Tier 1 (palette-core.md) - Decision Handling section"
      url: "internal://tier1/palette-core.md#decision-handling"
      type: "framework"

- id: LIB-093
  question: How do I evaluate agent quality and performance?
  answer: |
    Agent outputs are probabilistic, not deterministic. 
    Traditional unit tests (output == expected) don't work.
    Use multi-layered, artifact-focused evaluation instead.
    
    Layer 1: Deterministic Checks (First Priority)
    - Fixtures: Known inputs â†’ expected outputs
    - Unit tests: Component behavior validation
    - Schema validation: Output structure correctness
    - Constraint checks: Hard limits enforced (e.g., no PII in logs)
    - Pass/fail: Binary, auditable, fast
    
    Layer 2: LM-as-Judge (Second Layer, Produces Artifact)
    - Use powerful model to assess agent output against rubric
    - Rubric dimensions:
      * Correctness: Did it give the right answer?
      * Grounding: Is response factually accurate?
      * Instruction-following: Did it follow constraints?
      * Tone: Is communication appropriate?
    - Output: Scored rubric JSON (artifact, not opinion)
    - Run against golden dataset of prompts + ideal responses
    - Threshold-based pass/fail (e.g., overall_score >= 0.85)
    
    Layer 3: Business Metrics (Top-Down)
    - Goal completion rate
    - User satisfaction scores (thumbs up/down)
    - Task latency
    - Cost per interaction
    - Impact on revenue/conversion/retention
    
    Layer 4: Human Feedback (Ground Truth)
    - Collect bug reports, edge cases, thumbs down
    - Aggregate feedback to identify patterns
    - Convert feedback into new test cases (close the loop)
    - Use RLHF when appropriate (advanced)
    
    Metrics-Driven Development:
    1. Establish baseline scores for production agent
    2. Test new versions against full evaluation dataset
    3. Compare scores: new version vs production
    4. Go/no-go decision based on metrics, not intuition
    5. Use A/B deployments for gradual rollout
    
    Creating Golden Datasets:
    - Sample scenarios from production interactions
    - Cover full breadth of use cases + edge cases
    - Include ideal responses (validated by domain experts)
    - Maintain and expand dataset over time
    - Store as artifacts (JSON, YAML, CSV)
    
    Palette Integration:
    - Ankylosaurus (Anky) performs validation using these methods
    - Deterministic checks first, LM-as-Judge second
    - All evaluations produce artifacts (JSON rubrics, test reports)
    - Agent impressions track success/fail over time
    - Maturity model (UNVALIDATED â†’ WORKING â†’ PRODUCTION) 
      based on measured performance (not opinions)
    
    Example validation workflow:
    1. Theri builds feature
    2. Anky runs deterministic checks (fixtures pass?)
    3. Anky runs LM-as-Judge (rubric scores >= threshold?)
    4. Anky produces evaluation artifact (JSON report)
    5. Human reviews artifact, approves or requests changes
    6. If approved: increment agent success impressions
    7. If failed: increment fail, reset fail_gap, route to Raptor
    
    Anti-pattern: Don't use LM-as-Judge as a black box.
    Always produce scored rubrics as artifacts for human review.
  
  problem_type: Reliability_and_Failure_Handling
  
  related_rius: [RIU-001, RIU-105]
  
  difficulty: high
  
  industries: [All]
  
  sources:
    - title: "Google Introduction to Agents (Nov 2025) - AgentOps section"
      url: "https://cloud.google.com/use-cases/agents"
      type: "industry_standard"
    
    - title: "Agentic System Design book (referenced in Google doc)"
      url: "https://www.oreilly.com/library/view/agentic-system-design/9781098156909/"
      type: "book"
    
    - title: "Palette Tier 2 - Agent Maturity & Trust Model"
      url: "internal://tier2/assumptions.md#agent-maturity"
      type: "framework"

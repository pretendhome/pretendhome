# PALETTE DEMO SCRIPT FOR GLEAN INTERVIEW

**Purpose**: Show how you design and build AI agents in practice  
**Duration**: 10-15 minutes (if they ask)  
**Proof Point**: You've shipped AI outcomes, not just demos

---

## SETUP (30 seconds)

"Let me show you how Palette works. I'll walk through a real example from one of my active projects â€” preparing a 30-minute demo on agentic systems for an interview. This demonstrates how I design multi-agent workflows, ship outcomes, and measure success."

---

## THE PROBLEM (1 minute)

**Context**: I needed to prepare a 30-minute demo on agentic systems for an interview. The demo needed to:
- Teach agentic concepts clearly
- Include concrete examples
- Have interview scenarios with answer frameworks
- Be ready in 20 minutes (not 4-6 hours)

**Why This Is Hard**:
- Requires research (what are the top AI use cases?)
- Requires structure (how do I teach this in 30 minutes?)
- Requires examples (what are realistic interview scenarios?)
- Requires validation (is this coherent and accurate?)

**Traditional Approach**: 4-6 hours of manual work (research, outline, write, validate)

**Palette Approach**: 20 minutes with multi-agent workflow

---

## STEP 1: PROBLEM CLASSIFICATION (1 minute)

**What Happened**:
- I described the problem to Palette
- Palette classified it as:
  - **RIU-47** (Training & Enablement) â€” teaching agentic concepts
  - **RIU-89** (Agentic Orchestration) â€” demonstrating multi-agent workflows

**Why This Matters**:
- Palette has 105 validated problem-solution pairs (RIUs)
- Each RIU maps to proven solutions and agent archetypes
- Classification routes the problem to the right agents

**Glean Parallel**: When a customer says "I want AI to help sales reps close deals faster," you need to classify the problem (sales enablement? proposal generation? competitive intel?) before designing a solution.

---

## STEP 2: AGENT ROUTING (1 minute)

**What Happened**:
- Palette routed the problem to **Yutyrannus (Yuty)** â€” Narrative/GTM agent
- Yuty's job: Create a narrative structure and research directive

**Why Yuty**:
- This is a GTM/teaching problem (not a technical build problem)
- Yuty specializes in narrative structure, messaging, and enablement
- Yuty knows how to break down complex topics into teachable frameworks

**What Yuty Did**:
- Created a research directive for **Argentavis (Argy)** â€” Research agent
- Defined what Argy needed to research:
  - Top AI use cases (what problems are companies solving?)
  - Market validation (which use cases have funding and traction?)
  - Mapping to Palette RIUs (which RIUs are validated by the market?)

**Glean Parallel**: When working with a Glean customer, you need to route the problem to the right workflow (discovery â†’ research â†’ design â†’ build â†’ validate).

---

## STEP 3: RESEARCH (2 minutes)

**What Happened**:
- Argy researched 127 AI companies across 12 use cases
- Mapped them to Palette RIUs
- Validated which RIUs have market demand (funded companies = real problems)

**Key Findings**:
- **Top 3 hot use cases**: Sales ($2B+ funding), Customer Support ($700M+), Code Gen ($500M+)
- **Emerging category**: Agentic orchestration (15+ companies, 80% agentic-native)
- **Gap RIUs**: Decision logs, demo generation (services, not products)

**What This Proved**:
- Market validation: If venture-backed companies are building solutions, the problem is real
- Differentiation risk: 20+ companies in customer support = crowded market
- Category risk: 15+ companies in agentic orchestration = emerging category, less proven

**Glean Parallel**: When a customer says "I want AI for customer support," you need to research:
- What are similar companies doing?
- What's the market validation?
- What are the risks (differentiation, category, validation)?

---

## STEP 4: BUILD (2 minutes)

**What Happened**:
- **Therizinosaurus (Theri)** â€” Build agent created:
  - 30-minute demo script
  - 18-slide deck outline
  - 2 interview scenarios with answer frameworks
  - 4-Question Method framework (pattern, data, repetitive, measurable)

**Why Theri**:
- Theri specializes in building deliverables (scripts, decks, frameworks)
- Theri has access to the knowledge library (93 Q&A pairs) and taxonomy (105 RIUs)
- Theri knows how to structure content for teaching and enablement

**What Theri Built**:

**1. 4-Question Method Framework**:
- Is there a pattern?
- Is there data?
- Is it repetitive or high-volume?
- Can you measure success?

**2. Interview Scenario 1: Mature Market Assessment**
- Problem: Customer wants AI customer support chatbot
- Key Insight: 20+ companies, $700M+ = high validation
- Risk: Differentiation in crowded market
- MVP: Top 10 question types, human-in-the-loop
- Metrics: 85% accuracy (0-3 mo), 30% deflection (3-6 mo)

**3. Interview Scenario 2: Emerging Category Risk**
- Problem: Customer wants agentic workflow automation
- Key Insight: 15+ companies, $150M+ = emerging category
- Risk: Category validation + reliability
- MVP: ONE workflow, 3-5 tools, human-in-the-loop
- Metrics: 70% completion (0-3 mo), 50% time saved (3-6 mo)

**Glean Parallel**: When designing an AI agent for a Glean customer, you need to build:
- Agent workflow (input â†’ retrieval â†’ generation â†’ human-in-the-loop)
- Success metrics (usage, efficiency, business outcomes)
- Pilot scope (12 weeks, 5-10 users, kill criteria)

---

## STEP 5: VALIDATION (1 minute)

**What Happened**:
- **Ankylosaurus (Anky)** â€” Validation agent checked:
  - Factual accuracy (does the output match the source data?)
  - Coherence (does the output make sense?)
  - Completeness (does the output answer the question?)

**Why Anky**:
- Anky specializes in quality validation
- Anky has access to the knowledge library and taxonomy
- Anky checks for hallucinations, inconsistencies, and gaps

**What Anky Found**:
- âœ… Factual accuracy: All company data and funding numbers are verifiable
- âœ… Coherence: Demo script flows logically, frameworks are clear
- âœ… Completeness: All interview scenarios have answer frameworks

**Glean Parallel**: When building an AI agent for a Glean customer, you need to validate:
- Accuracy (does the agent retrieve the right information?)
- Coherence (does the agent's output make sense?)
- Completeness (does the agent answer the user's question?)

---

## OUTCOME (1 minute)

**What Was Delivered**:
- Complete interview prep package in 20 minutes (vs. 4-6 hours manually)
- 30-minute demo script
- 18-slide deck outline
- 2 interview scenarios with answer frameworks
- 4-Question Method framework
- Market validation data (127 companies, 12 use cases)

**What This Proves**:
1. **Multi-agent workflow**: Yuty â†’ Argy â†’ Theri â†’ Anky (not a single monolithic agent)
2. **Real outcome**: Used in a real interview (not a demo)
3. **Measurable efficiency**: 20 min vs. 4-6 hours (15X faster)
4. **Production-ready**: Has promotion/demotion logic (agents learn from success/failure)

**Glean Parallel**: When working with a Glean customer, you need to:
- Design multi-step workflows (discovery â†’ research â†’ design â†’ build â†’ validate)
- Ship real outcomes (not just demos)
- Measure efficiency (time saved, tasks completed, business results)
- Iterate based on feedback (usage data, customer satisfaction)

---

## KEY TAKEAWAYS (1 minute)

**What This Demonstrates**:

**1. I've Shipped AI Outcomes, Not Just Demos**
- Palette is production-ready (105 RIUs, 93 knowledge library entries, 7 agents)
- Used in 3 active projects (teaching, business planning, game dev)
- Has promotion/demotion logic (agents learn from success/failure)

**2. I Understand Multi-Agent Workflows**
- Not a single monolithic agent
- Each agent has a specific role (research, architecture, build, validate)
- Agents collaborate to solve complex problems

**3. I Can Design Agent Workflows for Customers**
- 4-Question Method (pattern, data, repetitive, measurable)
- Agent workflow design (input â†’ retrieval â†’ generation â†’ human-in-the-loop)
- Pilot scoping (12 weeks, 5-10 users, success metrics, kill criteria)

**4. I Measure Outcomes**
- Time saved (20 min vs. 4-6 hours)
- Quality (factual accuracy, coherence, completeness)
- Production-readiness (promotion/demotion logic)

---

## CLOSING (30 seconds)

"This is exactly what I'd do with a Glean customer:
1. Classify the problem (what use case is this?)
2. Route to the right workflow (discovery â†’ research â†’ design â†’ build â†’ validate)
3. Design the agent (input â†’ retrieval â†’ generation â†’ human-in-the-loop)
4. Scope a pilot (12 weeks, 5-10 users, clear success metrics)
5. Measure outcomes (usage, efficiency, business results)
6. Iterate based on feedback

That's how I ship AI outcomes, not just demos."

---

## IF THEY ASK: "HOW DOES PROMOTION/DEMOTION WORK?"

**Answer** (1 minute):

"Palette has three tiers:

**Tier 1: Core Prompt** (immutable rules, never changes)
- Defines how agents should behave
- Sets quality standards and safety guardrails

**Tier 2: Agents** (middle tier, tests problems against solutions)
- Each agent has a specific role (research, build, validate)
- Agents have access to knowledge library (93 Q&A pairs) and taxonomy (105 RIUs)

**Tier 3: Testing at Scale** (logs what works, enables learning)
- Every agent action is logged with success/failure
- Agents get promoted based on success rates:
  - UNVALIDATED â†’ WORKING (after 10 successes)
  - WORKING â†’ PRODUCTION (after 50 runs with <5% failure rate)
- Agents get demoted if they fail:
  - 2 failures within 10 runs â†’ demoted to UNVALIDATED

**Why This Matters**:
- Agents learn from success and failure
- System auto-improves over time
- Bad agents get demoted, good agents get promoted
- This is production, not a demo"

---

## IF THEY ASK: "WHAT ARE THE 7 AGENT ARCHETYPES?"

**Answer** (1 minute):

"Palette has 7 agent archetypes, each with a specific role:

1. **Argentavis (Argy)** â€” Research agent
   - Gathers information, validates sources, maps to taxonomy

2. **Rex** â€” Architecture agent
   - Designs system architecture, defines workflows, sets standards

3. **Therizinosaurus (Theri)** â€” Build agent
   - Creates deliverables (scripts, decks, frameworks, code)

4. **Velociraptor (Raptor)** â€” Debug agent
   - Identifies issues, proposes fixes, validates solutions

5. **Yutyrannus (Yuty)** â€” Narrative/GTM agent
   - Creates narrative structure, messaging, enablement materials

6. **Ankylosaurus (Anky)** â€” Validation agent
   - Checks factual accuracy, coherence, completeness

7. **Parasaurolophus (Para)** â€” Monitoring agent
   - Tracks usage, measures outcomes, identifies trends

**Why These Archetypes**:
- Each agent has a specific role (not a general-purpose agent)
- Agents collaborate to solve complex problems
- Mirrors how real teams work (research â†’ design â†’ build â†’ validate â†’ monitor)"

---

## FINAL NOTE

**When to Use This Demo**:
- If they ask: "Tell me about a time you built an AI agent"
- If they ask: "How do you design multi-agent workflows?"
- If they ask: "Show me an example of AI outcomes you've shipped"

**When NOT to Use This Demo**:
- If they're short on time (save it for a follow-up)
- If they're more interested in customer-facing work (focus on AWS GTM instead)
- If they want to dive into Glean-specific use cases (pivot to demo scenario instead)

---

**Good luck!** ðŸš€

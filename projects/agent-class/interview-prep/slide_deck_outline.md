# Slide Deck Outline: AI Use Case & Metrics

**Duration**: 30 minutes  
**Slides**: 15-18 slides  
**Template**: Use `/home/mical/fde/projects/agent-class/Open Template_ML_Switch-up_ Live class.pptx`

---

## Slide Structure

### Slide 1: Title
**Title**: AI Use Case & Metrics: Problem Framing and AI Fit  
**Subtitle**: Interview Prep for PM/TPM/EM Roles  
**Instructor**: Mical  
**Date**: February 11, 2026

---

### Slide 2: Learning Objectives
**Title**: What You'll Learn Today

**Content**:
- How to frame problems to determine if AI is a good fit
- How to assess market validation using company/funding data
- A repeatable framework for "Is this a good AI use case?"
- How to differentiate in mature vs. emerging AI markets

**Visual**: 4 icons representing each objective

---

### Slide 3: Why This Matters
**Title**: What Interviewers Are Looking For

**Content**:
Common interview questions:
- "How would you evaluate if [problem] is a good fit for AI?"
- "A customer wants to build [AI solution]. How do you assess viability?"
- "What metrics would you use to determine success?"

**The Trap**: Jumping to solutions without validating the problem

**What They Want**: Structured thinking, risk awareness, metrics-first approach

---

### Slide 4: The 4-Question Method
**Title**: Framework: Problem Framing for AI Fit

**Content**:
```
1. Is there a pattern?
   → Can this be solved by recognizing patterns in data?

2. Is there data?
   → Enough quality data to train/fine-tune?

3. Is it repetitive or high-volume?
   → Does this happen often enough to justify AI investment?

4. Can you measure success?
   → Are there clear metrics to know if AI is working?
```

**Visual**: Decision tree or flowchart

---

### Slide 5: Question 1 - Is There a Pattern?
**Title**: Is There a Pattern?

**Good AI Fit**:
- ✅ Customer support tickets (common questions)
- ✅ Code completion (syntax and context)
- ✅ Document extraction (invoice structure)

**Poor AI Fit**:
- ❌ One-off strategic decisions
- ❌ Creative brand positioning
- ❌ Novel research problems

**Key Insight**: AI is pattern recognition, not magic

---

### Slide 6: Question 2 - Is There Data?
**Title**: Is There Data?

**Good AI Fit**:
- ✅ Historical customer conversations (thousands of examples)
- ✅ Public code repositories (billions of lines)
- ✅ Existing documents (structured/semi-structured)

**Poor AI Fit**:
- ❌ Brand new process (no historical data)
- ❌ Highly sensitive data (can't access)
- ❌ Rare events (not enough examples)

**Key Insight**: Think about data availability before architecture

---

### Slide 7: Question 3 - Is It Repetitive?
**Title**: Is It Repetitive or High-Volume?

**Good AI Fit**:
- ✅ 10,000+ support tickets per month
- ✅ Developers writing code daily
- ✅ Processing 1,000+ documents per week

**Poor AI Fit**:
- ❌ Quarterly strategic planning (4x per year)
- ❌ Annual compliance audits (1x per year)
- ❌ One-time data migration

**Key Insight**: ROI requires volume

---

### Slide 8: Question 4 - Can You Measure Success?
**Title**: Can You Measure Success?

**Good AI Fit**:
- ✅ Support: Resolution time, CSAT, ticket deflection rate
- ✅ Code: Acceptance rate, time saved, bugs introduced
- ✅ Documents: Extraction accuracy, processing time

**Poor AI Fit**:
- ❌ "Make our brand more innovative" (vague)
- ❌ "Improve employee morale" (indirect)
- ❌ "Better strategic decisions" (hard to attribute)

**Key Insight**: Define metrics before building

---

### Slide 9: Market Validation Framework
**Title**: How to Assess Market Validation

**Content**:
**Funded companies = market validation**

**Validation Tiers**:
- **HIGH**: 20+ companies, $500M+ funding
  - Example: Customer support (20+ companies, $700M+)
  - Risk: Differentiation in crowded market

- **MEDIUM**: 10-20 companies, $100M-$500M
  - Example: Agentic orchestration (15+ companies, $150M+)
  - Risk: Category validation

- **LOW/GAP**: 0-5 companies, <$50M
  - Example: AI decision logs (0 companies)
  - Risk: Market validation

**Visual**: 3-tier pyramid or matrix

---

### Slide 10: Market Validation Examples
**Title**: Real Company Data

**High Validation**:
- Customer Support: Intercom ($240M), Ada ($190M), Forethought ($92M)
- Sales Intelligence: Gong ($584M), Clari ($576M), Outreach ($489M)
- Code Generation: Cursor ($8M), Tabnine ($55M), Codeium ($65M)

**Medium Validation**:
- Agentic Orchestration: LangChain ($35M), Dust ($16M), Fixie.ai ($17M)
- AI Governance: Credo AI ($21M), Arthur AI ($47M), Fiddler AI ($57M)

**Gap/White Space**:
- AI Decision Logs: 0 companies
- Internal Demo Generation: 0 companies

**Visual**: Table or grouped cards

---

### Slide 11: Interview Scenario 1 - Setup
**Title**: Scenario 1: Mature Market Assessment

**Interview Question**:
> "A customer wants to build an AI-powered customer support chatbot. How would you evaluate if this is a good idea?"

**Your Approach**:
1. Apply 4-Question Method
2. Check Market Validation
3. Dig Deeper (Critical Questions)
4. Make Recommendation

---

### Slide 12: Scenario 1 - Analysis
**Title**: Scenario 1: Analysis

**4-Question Method**:
- ✅ Pattern? YES (common questions, resolution paths)
- ⚠️ Data? Need to check (historical tickets?)
- ⚠️ Repetitive? Need to check (volume?)
- ✅ Measure? YES (deflection rate, CSAT)

**Market Validation**:
- 20+ companies, $700M+ funding
- **HIGH validation** → Challenge is differentiation

**Critical Questions**:
- Data availability? (6+ months of tickets?)
- Volume? (1,000+ per month?)
- Differentiation angle? (vertical? workflow?)
- Integration requirements?

---

### Slide 13: Scenario 1 - Recommendation
**Title**: Scenario 1: Recommendation

**IF** data + volume + differentiation:
- **MVP**: Top 10 question types (80/20 rule)
- **Metrics**:
  - Early (0-3 mo): 85% accuracy on top 10
  - Medium (3-6 mo): 30% ticket deflection
  - Long-term (6+ mo): CSAT maintained, cost reduced
- **Risk**: Differentiation in crowded market

**IF** lacking data or volume:
- Start with traditional automation
- Build data set with human-in-the-loop
- Revisit AI in 6-12 months

---

### Slide 14: Interview Scenario 2 - Setup
**Title**: Scenario 2: Emerging Category Risk

**Interview Question**:
> "A customer wants to build an agentic workflow automation platform. How would you evaluate this?"

**Your Approach**:
1. Apply 4-Question Method
2. Check Market Validation
3. Assess Category Risk
4. Make Recommendation

---

### Slide 15: Scenario 2 - Analysis
**Title**: Scenario 2: Analysis

**4-Question Method**:
- ⚠️ Pattern? MAYBE (workflows have patterns, but complex)
- ⚠️ Data? PARTIAL (individual tools yes, cross-tool less)
- ✅ Repetitive? YES (if targeting common workflows)
- ✅ Measure? YES (time saved, error rate)

**Market Validation**:
- 15+ companies, $150M+ funding
- **EMERGING category** (most founded 2022-2023)
- 80% agentic-native

**Critical Questions**:
- Specific workflow? (not "automate everything")
- Tool integration complexity? (standard APIs?)
- Reliability requirements? (low vs. high stakes)
- Why not Zapier + AI?

---

### Slide 16: Scenario 2 - Recommendation
**Title**: Scenario 2: Recommendation

**IF** specific, low-stakes workflow + standard tools:
- **MVP**: ONE workflow, 3-5 tools, human-in-the-loop
- **Metrics**:
  - Early (0-3 mo): 70% completion, <10% error
  - Medium (3-6 mo): 50% time saved, 20+ users
  - Long-term (6+ mo): Expand to 2-3 workflows
- **Risk**: Category validation + reliability

**IF** broad automation or high-stakes:
- **RED FLAG**: Too risky for current AI
- Start with traditional automation + AI enhancement
- Revisit in 12-18 months

**Key Difference**:
- Mature market → Differentiation risk
- Emerging market → Category + reliability risk

---

### Slide 17: Common Mistakes
**Title**: Common Mistakes in Interviews

**Mistake 1**: Jumping to solutions
- ❌ "We'll use GPT-4 with RAG..."
- ✅ "Let me validate AI suitability first..."

**Mistake 2**: Ignoring market validation
- ❌ "Great idea, let's build!"
- ✅ "I see 20+ companies with $700M funding..."

**Mistake 3**: Vague metrics
- ❌ "Measure user satisfaction..."
- ✅ "85% accuracy, 30% deflection rate..."

**Mistake 4**: Ignoring risks
- ❌ "AI can solve this easily!"
- ✅ "Risk is differentiation in crowded market..."

**Mistake 5**: Over-scoping MVP
- ❌ "Automate all support..."
- ✅ "MVP: Top 10 question types..."

---

### Slide 18: Key Takeaways & Resources
**Title**: Key Takeaways

**Framework**:
1. Use 4-Question Method (pattern, data, repetitive, measurable)
2. Check market validation (company count + funding)
3. Scope aggressively for MVP
4. Define metrics early (0-3 mo, 3-6 mo, 6+ mo)
5. Identify risks explicitly

**What Interviewers Look For**:
- **PM**: Customer empathy, market awareness, prioritization
- **TPM**: Risk identification, execution planning, feasibility
- **EM**: Technical depth, team scoping, reliability

**Resources**:
- Company-RIU Mapping Library (127 AI companies)
- Practice with any AI use case

**Questions?**

---

## Slide Design Notes

**Visual Style**:
- Clean, minimal text (use bullet points)
- Icons for each question in 4-Question Method
- Tables/matrices for market validation tiers
- Color coding: Green (good fit), Yellow (check), Red (poor fit)
- Company logos where possible (Intercom, Gong, LangChain, etc.)

**Engagement Elements**:
- Pause after Slide 4 (4-Question Method): "Let's apply this to a quick example..."
- Pause after Slide 9 (Market Validation): "Anyone familiar with these companies?"
- Interactive during Scenarios: "What would you ask next?"

**Timing**:
- Slides 1-3: 2 min (intro)
- Slides 4-8: 8 min (framework)
- Slides 9-10: 8 min (market validation)
- Slides 11-16: 10 min (scenarios)
- Slides 17-18: 2 min (wrap-up)

---

## Next Steps

1. **Create slides** using template: `/home/mical/fde/projects/agent-class/Open Template_ML_Switch-up_ Live class.pptx`
2. **Add visuals**: Icons, company logos, decision trees
3. **Rehearse**: Practice timing (30 min total)
4. **Prepare backup**: Have extra examples ready if time permits

---

**Status**: Outline complete, ready for slide creation

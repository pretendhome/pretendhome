5 Instructional Design Principles You Can't Do Without
5 Instructional Design Principles You Can't Do Without
fizkes/Shutterstock.com
Photo of Vick Oletu
By Vick Oletu
Updated: May 12, 2021
3 minutes to read
Write a comment
Summary: A sound knowledge of Instructional Design principles will impact and inform your strategies in your eLearning project. In this article, you will find 5 Instructional Design principles that I consider greatly important for meeting any learning needs, with examples.
5 Indispensable Instructional Design Principles For Your eLearning Course
eLearning seeks to create experiences for modern learners who want training contents that are relevant, mobile, personalized and self-paced.

Primarily, a successful eLearning development is one that is engaging, motivational, memorable and easy to digest. This success, however, is mostly powered by what is known as Instructional Design. The Instructional Designer plays a crucial role in creating experiences that are both engaging and applicable to real life.

Sponsored content - article continues below
Trending eLearning Content Providers
CommLab India
CommLab India
SweetRush
SweetRush
Cinecraft Productions
Cinecraft Productions
Before we explore the Instructional Design principles, let’s take a look at the meaning of Instructional Design.

What Is Instructional Design?
Instructional Design is the process of assessing learning needs and then applying the appropriate learning strategy to meet them.

I’ve always believed that the Instructional Designer must put at the back of his or her mind that learning must actually take place. For example, a positive change in behavior or improved performance. Having such a mindset will help in creating learning solutions that will involve the learner- a very important factor in eLearning. Doing this is particularly why the Instructional Design process is seen to be very challenging. With the following principles, you will be able to create effective Instructional Designs that will achieve the desired eLearning result.

Principles Of Instructional Design
According to Robert M. Gagne, there are 9 important Instructional Design events for developing a powerful learning process, and I completely agree. In my experience as an Instructional Designer, I have come across varied learning needs and voluminous contents that demands a great level of analytical thinking, creativity, flexibility and the ability to manage information, especially in the face of tight deadlines. During this period, I have found out that 5 out of the 9 principles are indispensable.

1. Gain Attention
Grabbing attention involves taking a deliberate effort to ensure reception. It is a first impression strategy that can determine the level of participation by the learner. For instance, gaining attention arouses the learner’s curiosity and sets an expectation in the mind of the learner. Practical ways of doing this include: throwing a challenge, sharing surprising facts, presenting a problem to be solved or telling a story that resonates with the learners.

2. Tell The Learners The Learning Objective
Informing the learners of the course objectives is an instructional strategy that basically gets them involved in the learning process. Telling the learners what they will be able to do after participation, will help them to organize their thoughts and focus their learning efforts. If learners understand what you expect them to learn, they can better judge if they are successfully learning it.

3. Present Information
This is a principle that you practically cannot do without when developing an effective eLearning. Before coming across this principle, I have always asked myself this question when working on Instructional Design— what is the best way to represent this information in a way that is interactive and easy? This is an instructional mindset that opens up spaces for creative thinking. Storytelling, branching scenarios, role-plays, gamification and interactive videos are methods you can apply but in alignment with the learning objectives and desired outcome. Remember to break information into chunks to avoid cognitive overload.

4. Assess Performance
This principle assists learners to think about the information they have covered. Note that the feedback of the right or wrong answer should be centered upon reinforcing learning. To make your assessments interactive and engaging, use learning games, word puzzles, drag and drop interactions, flash cards or scenario activities. You can choose to give your learners a second chance to attempt the assessment depending on the learning objectives.

5. Provide Feedback
Giving specific and informative feedback on learner’s performance is a must. Well-formulated feedback will provide perspectives that will impact learning assumptions. Don't just stick to "Correct and Incorrect" as titles of the feedback. Try using other adjectives that are relatable or customize to the theme of the course/module to enrich their learning experience.

To execute an effective eLearning project, be sure to get your learners involved throughout the course. Be flexible, keep things simple, think design in a way that connects with your learners. Above all, have an instructional mindset that learning must indeed take place.

Originally published on February 17, 2019


##Class 1
Graph Algorithms for Technical Interviews | Omkar Deshpande | IK UpLevel MicroClass
https://www.youtube.com/watch?v=y6U51bLS2Jo&t=4111s
Search in video
0:01
[Music]
0:09
alright guys I think we should actually get started with today's session that was that was very kind of you Omkar
0:15
maybe let's switch to the next slide all right folks for those of you who for those of you
0:24
who don't know interview kick-start we are a platform where you go to prepare for very tough technical interviews we
0:31
are one of a kind platform we've been running this now for about six years right we created the concept of a very
0:40
structured program to prepare for very tough technical interviews we work with
0:46
over 4,000 students in the last six years we've generated upwards of 5,000
0:53
offers the the median offer coming out of the program tends to be around
0:59
250,000 dollars the core philosophy of the program is that you cannot really
1:05
hack a technical interview the only way to to fundamentally be in a position
1:11
where you can go out and nail these technical interviews consistently is by investing in yourself and becoming a
1:18
better engineer right and that is what interview kickstart is all about it's about taking folks are genuinely
1:25
interested in up leveling in their career folks who have very good intent willing to commit the time to prepare
1:32
for these tough technical interviews putting them through a very structured process we literally look at this as a
1:39
system have them work with folks who are experts on technical interviews all our
1:47
instructors you know like own car our folks who are either hiring managers
1:52
hiring committee members in many cases very senior technical leads but all
1:58
folks who are very well versed with the typical interview process they come from
2:03
Google Facebook Amazon LinkedIn uber fundamentally these folks have done
2:10
hundreds of interviews right they're all very well trained to interview at the companies that they work at the heart of
2:17
how we teach an old car is going to showcase some of this today is that we teach via patterns right not
2:24
wire questions now most people make the mistake of thinking that hey I need to do 100 questions or 200 questions or 300
2:30
questions right the number seems to keep going up every year to prepare for interviews but the reality is if you
2:36
prepare wire questions and you get a question that you've not seen in an interview you're going to get stopped
2:42
what you fundamentally need to prepare for is to be able to solve unseen
2:48
questions right and that is where patterns come into play that is the heart of how we teach and we follow a
2:53
very rigorous system we have classes we have tests we have mock interviews we
3:00
have a whole bunch of individual coaching sessions as well we help you right from the time you prepare your
3:05
LinkedIn profile your resume all the way up to teaching you and in many cases connecting with companies as well now I
3:12
won't I won't talk more about what we do on interview kick-start because today's
3:17
session is actually on graphs right but for those of you who are fundamentally
3:23
interested in taking a very structured approach to preparing for technical interviews one that has an
3:30
extraordinarily high success rate right people who go through the program choose to go out and interview have a 95% plus
3:37
chance right of getting their bean jobs but those of you who want to learn a lot more please go to interview kickstart
3:43
com sign up for a free webinar I conduct these webinars multiple times week where
3:49
we get into how the program works in great detail the companies on the right here are typically people go once they
3:56
are done with the program now let's get to today's session today's session is
4:01
run by um car um car can you move to the next slide please all right so you know you guys can see
4:09
um car here you can see on the screen home car is a fabulous instructor right
4:15
he's been with I care for the last one year um car actually heads all curriculum at
4:21
interview kickstart he has helped navigate the curriculum to now focusing
4:27
on a much more patterns based approach he's the consummate nerd right and I say
4:32
that with utmost affection he is brilliant at what he does PhD from
4:38
Stanford in computer science prior to that did his computer science
4:44
engineering at IIT in India he is a gifted teacher and you know somebody who
4:51
can take very complex topics and simplify them right what he's going to
4:57
attempt to do today is provide a unifying theory right of how one thinks
5:03
through graphs which tend to be a very complex topic and also a topic for those
5:08
of you preparing for interviews that is very frequently asked right prior to interview kick-start Omkar was very
5:16
early at cosmics cosmics eventually got acquired and became Walmart labs when
5:21
homecare joined cosmics at that point of time cost fix was very well known for
5:26
having amongst the best engineers in the Bay Area comparable largely only to folks at Google right that that that was
5:35
the early bunch of folks at cosmics honker also had a little bit of a of a gaming stint right where he where he
5:41
built a game but for the last one here and I'd say this is Omkar expression he loves teaching he loves taking complex
5:49
topics simplifying them and is you know able to teach them with significant alacrity with that guys I hand you now
5:56
two home cars capable hands on car while you get started with with today's
6:01
session the way we are going to run the session guys is Omkara is going to teach you right in an R his view of the
6:08
unifying theory of of graphs right feel free to ping us with questions during
6:17
the session unkar may look at the questions from time to time if he feels otherwise once we are done right with
6:22
the are he will take detail Q&A right and with that um car over to you let's
6:29
get started on today's session I hope you guys enjoyed today's uplevel micro cost just thanks Ryan
6:37
good evening everyone so over the next hour I will be sharing with you a
6:43
glimpse of how we train our students to master graphs for technical interviews
6:49
I'm particularly pleased to be presenting this to you because there isn't much information available to the
6:55
public about the high quality and the comprehensive nature of our curriculum
7:01
much of the material that I am presenting to you has been developed within the last six to nine months we
7:07
are continuously iterating and innovating on all fronts as the demand for our program increases I do want to
7:14
emphasize that this is going to be a sneak peak much like quickly scanning through some sample pages of book on Amazon or
7:20
amazon.com so let's begin let's start by
7:26
confirming to ourselves that graph algorithms are crucial to learn for giving technical interviews at any of
7:33
the top tier companies um God is Ryan can you you know maybe yeah yeah just
7:39
come a little bit closer to the computer I think it's it's the screen is a bit dark yeah yes it's much better enough
7:44
sorry yeah so Google for example advises candidates that they should consider if
7:51
the problem that they've been given in the interview could be solved using any of the standard graph algorithms you
7:59
should know what are the different ways to represent a graph in memory their pros and cons and specifically know in
8:07
detail how to analyze an implement breadth-first search and depth-first
8:12
search facebook says the same thing in a
8:17
more condensed form you should learn graphs especially BFS and DFS Amazon
8:26
also recommends the same pretty much every top-tier company expects you to know graphs when you interview with them
8:36
since you enrolled for this micro class you probably were already convinced about the importance of graphs still if
8:42
there were any doubts what Google or Facebook or Amazon said should hopefully put them to rest so how should you
8:49
prepare well for a complex topic like graphs well regardless of whether you
8:56
are preparing on your own or planning to enroll in our program you can follow the same structure that we do
9:02
this is the I K graphs curriculum shown as a graph mentioning the major topics
9:09
and algorithms I'm going to elaborate on this slide in more detail as we go along
9:17
note for now that many of the topics are shown in green and others in blue the
9:24
green topics are the more basic topics and also more commonly asked in interviews and so much of your focus in
9:31
prep needs to be on the green topics we devote about 10 hours of instruction for
9:37
the topics in green and more than eight hours on the advanced topics so in total we have more than 18 hours of live and
9:45
recorded instruction on graphs this is way more than the amount of time spent on graphs even in an undergraduate
9:51
university course why do we do this it's
9:57
because effective preparation for a coding interview is a combination of two things both of which need time first you
10:05
need to learn how to solve questions that you have not seen before these are called problems and problem solving in
10:13
coding interviews means designing a correct and efficient algorithm that solves the given unseen question and to
10:21
do this you need to understand the fundamentals of computer science theory and in the context of graphs graph
10:26
theory and then you need to be able to draw upon it when designing algorithms using strategies like decrease and
10:34
conquer divide and conquer and transform and conquer and all this needs time especially if you didn't go through a
10:41
solid computer science program on your you know in your education secondly once
10:47
you arrive at a correct and efficient algorithm you need to be able to code it up relatively quickly in a way that your
10:53
code will not have major bugs when you are inside that room with the interviewer your mind might be in a
10:59
state of anxiety because the interview is after all a timed test and time tests always cause anxiety so if you try to
11:06
write code from scratch without having a template or pattern in your mind for how
11:11
your code will be structured chances are high that you will fumble and your code will have some major work
11:17
that is why we train our students not just in problem-solving but also in solidifying those coding patterns which
11:25
remain the same across a wide array of problems we do not move from one random
11:31
problem to another in our prep we follow a carefully laddered sequence of problems meant to solidify these coding
11:39
patterns so this makes our prep not only
11:44
as rigorous as a top computer science course but also specifically relevant
11:49
for interviews university courses and algorithms textbooks focus almost
11:55
entirely on solving problems theoretically so here's a snapshot from
12:01
the slides of cs161 which is the undergraduate algorithms course at Stanford the purpose of such courses and
12:08
of the textbooks is generally to turn you into a graduate student or a PhD
12:13
student under that professor or at least in that in that department they generally don't think about how to make
12:19
you succeed in coding interviews that's not their goal and that is why you may
12:26
find the professor himself or herself copying code from prepared notes onto the blackboard as in this example from
12:33
MIT s course on algorithms if you need written notes to write your code correctly in an interview you're not
12:40
going to clear that interview or the professor would directly put the code on
12:46
one pre-written slide and explain it as in this example from Bob Sedgwick's course at Princeton again you will need
12:55
to write code from scratch in an interview so don't get me wrong these are world-class professors who have made
13:01
fundamental contributions to algorithms research and their courses are excellent you can and should learn theory from
13:08
them but you also need to learn the theory in a way that allows you to quickly code up solutions to problems
13:14
from scratch using coding patterns in an interview you can't look up slides or
13:20
notes for reference in an interview also bear in mind that professors operate
13:25
under time constraints in election they need to cover a certain amount of material within a relatively severe time
13:31
limit which means they may not be able to devote enough time in their video to
13:37
show you how you could come up with the solution to the problem yourself trying out different options and gradually
13:43
building your way to the solution since we do that at AI K it it also means that
13:49
we need to spend more instructional time in the process we can't just tell you the final solution we have to show you
13:57
how to arrive at the solution so hopefully this clarifies why we spend more time on these topics in our courses
14:02
than even a top university course so if textbooks and university courses
14:09
provide a decent amount of problem-solving but are not focused on coding interviews per se what about leet code leet code is at the
14:17
other extreme leet code is an excellent question Bank of highly relevant interview problems but it is a question
14:24
Bank it's not a substitute for systematic and structured learning think
14:30
for instance how much would you learn in a university course by directly attempting the questions from say
14:36
previous year's exams without attending the lectures and without reading the textbooks not much similarly you won't
14:43
learn much by just attacking the problems on lead code directly they don't teach you the theory and the
14:49
problems are not organized in a carefully laddered sequence moreover the solutions to the problems
14:55
are written by different people which means you may not find a consistent coding pattern being used for different
15:02
problems within the same category many people also tend to rise right micro
15:10
optimized code going way beyond the requirements for a correct and asymptotically efficient solution and
15:16
that also tends to hide the close relationship between different related
15:22
problems the code for different problems looks more different than it should
15:27
because of these crowd-sourced solutions and micro optimizations now those will
15:32
be useful if you are doing comparative programming and if you are already fluent with this domain but we are talking about coding interviews for
15:39
experienced software engineers here not relative programming contests for college students so at the end of the
15:44
day what you want to do as an engineer is to understand how you can arrive at the solution to a given unseen problem
15:50
yourself and how to write sufficiently well performing code for the most efficient algorithm from scratch
15:56
seeing micro optimized code written by different people for random problems can can actually shatter your confidence in
16:02
yourself and can drive you to memorize those solutions making the entire
16:08
interview prep process a lost opportunity to make yourself a better engineer you would be more or less
16:14
depending on luck to hack the test hoping that you'll be given questions whose solutions you have memorized and
16:20
naturally you'll have a brain freeze the moment you are given an unseen question because you didn't prepare for the
16:27
worst-case so let's look at how we utilize those 18 or 20 hours to teach
16:33
graphs if you are planning to independently study graphs you could follow this same sequence basic graph
16:42
theory is a prerequisite for problem solving so we spent the first module
16:48
learning the basic terminology concepts and patterns of reasoning in graph
16:53
theory but not why our DRI list of definitions as is the way algorithms
16:59
books and courses introduce the subject rather we do this via the real story of
17:05
the origins of graph theory back in the 18th century in the Prussian Empire in
17:11
the city of königsberg we're in the midst of a booming economy the residents
17:16
construct seven bridges across a river that flows through the heart of the city
17:22
those seven bridges are shown here in this Google Maps image every Sunday
17:29
morning as the residents took their morning walk they were curious about the
17:34
following question can one walk across the seven bridges without crossing the
17:39
same bridge twice that is can one walk across the seven bridges crossing each
17:45
of them exactly once without repeating any bridge more than once and without missing out on any bridge
17:53
they were so intrigued by this puzzle problem that they took it up with the mayor the mayor was not a mathematician
18:00
himself and so he wrote a letter to the most famous mathematician of that time
18:05
Leonhardt Euler requesting him to solve the problem oiler did solve the problem
18:13
and published the solution to the problem in a paper and this is his
18:18
original sketch in the paper showing a specific section of the map of the city
18:23
of königsberg that has all the seven bridges diagram notice that he
18:30
represented each land area by a single letter a b c and d that was his way of
18:38
saying that it didn't matter what the shape of the land region is or what the size is all that mattered is that there
18:45
were these four land regions a b c and d and they were connected using these
18:51
seven bridges again it didn't matter what the length of each bridge was what the width of each bridge was where
18:58
exactly they were located all that mattered is that there were these four
19:03
land regions connected by these seven bridges and so we can ignore all the
19:10
other details now this diagram which oiler did not draw himself but that's
19:17
how we visually think about it today this diagram can be shown in a slightly neater way and it's what we know today
19:26
as a graph a graph is a collection of vertices corresponding to the land
19:31
regions and a collection of edges corresponding to the bridges and since
19:37
these bridges were two-way streets there were no directions on the edges and so we call this kind of graph as undirected
19:45
two vertices are said to be adjacent if they are directly connected by a bridge so where this is C and D are adjacent a
19:53
path in a graph it's just a sequence of adjacent
19:59
vertices so DB a/c is a path and a cycle
20:06
is a path that ends at the same vertex from where it starts so DB a/c D is a
20:16
cycle so if we model the map of königsberg with a graph like this the
20:25
problem that Oyler had to solve was is there a path in the graph that visits
20:31
every edge exactly once without missing out on any edge and without repeating
20:36
any edge more than once and this is the well-known Alerian path problem Oilers
20:42
answer for the specific graph of königsberg wasn't no that is you cannot
20:47
take a morning walk like that but Euler actually ends up solving the problem for a general graph how he did so is what we
20:56
cover in the first module of our foundation material it takes more than
21:01
an hour so obviously it's impossible for me to do it in a session where I have to give an overview of all of graphs so I can
21:10
just summarize a few lessons from it the first lesson boiler shows us how he
21:18
grapples with questions he has not seen before and for which he had no idea what method would work
21:24
he first tries a brute force approach which he quickly discards as being too
21:30
tedious and then he tries to attack the problem using techniques from various
21:35
subfields of mathematics from that time and he sees that this can't be solved by
21:42
any of those subfields or methods in those subfields so his starting point is
21:48
a state of feeling clueless it's not as if he instantly solves the problem and he was one of the most brilliant
21:55
mathematicians in all of human history why should we expect that we will be able to instantly come up with answers
22:01
so feeling clueless when you are given a problem you haven't seen before is a natural starting State
22:14
the second lesson specifically applies for graphs the classic sign that a given
22:21
problem can be modeled as a graph is that you are given a set of topics
22:26
sorry set of objects and you are given a set of pairwise relationships between
22:33
them pairwise relationships are a giveaway that the input to the problem
22:41
should most likely be modeled as a graph
22:46
lesson three it's extremely hard to invent a new graph algorithm from
22:52
scratch this means that in an interview when you are given a problem that seems
22:58
to be a graph problem you should immediately ask yourself how can I stand
23:03
on the shoulders of giants to solve this problem in other words what standard graph algorithm am I going
23:10
to use to solve the problem I won't be inventing my own graph algorithm I will
23:15
be using an existing standard graph algorithm so you need to know which algorithm you'll use even before you
23:23
have worked out the details of how to model the input because which graph algorithm you plan to use will dictate
23:31
the details of that modelling so having
23:38
learned the basic concepts terminology and historical origins of graph theory the next teach our students how to model
23:46
the pairwise relationships that I talked about as a graph so when we talk about
23:51
graph representations we are really talking about different ways to represent the pairwise relationships or
23:59
the edges of the graph the vertices of the graph can be maintained separately
24:05
and we don't generally worry about that in a graph problem our focuses on how to represent the edges because graph
24:12
algorithms focus on traversing the edges of the graph there are four ways of
24:17
representing the collection of edges you know edge lists adjacency lists
24:23
adjacency maps and adjacency matrices edge lists which are an unordered list
24:30
of edges or an unordered list of pairs of vertices since a pair of vertices
24:35
denotes an edge edge lists are a very common way in which the pairwise relationships are given as input in
24:43
interview problems so you'll be given a collection of objects and a collection of pairwise relationships in many cases
24:50
but edge lists are not convenient to work with from a time efficiency perspective and so the first thing we
24:57
often do in the code is to transform the edge list into an adjacency list so an adjacency list
25:06
tells you instantly who all the adjacent neighbors of a given vertex are for
25:13
example in this graph vertex zero has word vertex one vertex six and vertex
25:21
eight as its neighbors and so we list those neighbors in no particular order
25:27
in the adjacency list for vertex zero one of the problems with a list though
25:34
is that if I want to find out whether vertex eight is present in the adjacency
25:40
list for zero or not I have to sequentially scan the list to check if
25:45
it appears anywhere in it but if I organized the neighbors of zero not in a
25:52
list but in a hash table I would be able to instantly tell whether eight is a
25:58
neighbor of zero or not because all three of these neighbors would be found
26:04
in the hash table so when the neighbors are stored in hash tables instead of
26:09
lists we call the resulting structure as an adjacency map adjacency maps can
26:17
always be used in place of adjacency lists but they are especially useful when there is a chance that your input
26:23
data has duplicate relationships we don't want to add the same edge multiple
26:30
times and adjacency maps allow us to instantly check whether a relationship
26:35
already exists in the growing edge representation or not if it already
26:42
exists then we don't we don't add the duplicate edge otherwise we add it in
26:48
you can't instantly tell whether an edge exists in an adjacency list structure and so adjacency maps using adjacency
26:56
maps is one of those tips that you won't get from an algorithms book because
27:02
algorithms books tend to assume that hash tables have a theoretical worst-case complexity that is linear in
27:09
the size of the table in an interview you're allowed to assume that hash table operations will almost always
27:16
practically run in constant time insert search and delete so just like
27:22
you're allowed to assume that quicksort will practically run in n log in time can assume hash table operations will
27:27
run in constant time the probability of encountering the theoretical the
27:32
theoretical worst case is practically negligible adjacency matrices are
27:40
basically two dimensional tables which also tell you instantly whether an edge is present or absent from this
27:47
representation for example if I want to find out whether whether eight is a neighbor of zero or not in the graph I
27:54
just need to look at row 0 and column 8 and if that's a 1 it means the edge
28:01
exists otherwise if it's a zero it doesn't exist for most real-world graphs
28:08
if we were to build out such a table most of the entries in it would be a zero so this is not a space efficient
28:16
representation adjacency lists and maps effectively store only the ones in the
28:22
table that is the only store the the actual neighbors for a vertex and so they are more space efficient for
28:29
real-world graphs and so whenever you
28:34
are building a graph in an interview you will most likely be building either an adjacency list or an adjacency map both
28:42
of them are useful in quickly telling this given a vertex u what other
28:48
vertices does it have a direct relationship and if you want to find out
28:54
given two specific vertices you and we do they have a direct relationship or
29:00
not that is something in adjacency map does quickly in comparison to an
29:05
addition see list but more commonly interview questions on
29:13
graphs will require you to detect indirect or long-distance relationships
29:19
so in this diagram s is not directly
29:24
related to V but it does have an indirect relationship with me since it's connected to V through this intermediate
29:32
vertex u so one can ask given a vertex s what other vertices does it have an
29:39
indirect relationship with and given two vertices s n V do they have an indirect
29:46
relationship so to answer these we can't just look up an adjacency list or a map
29:52
since we're talking about long distance relationships we need to find a way to explore or traverse the graph starting
30:00
from some source vertex like s and we
30:06
need to find all reachable vertices from s so that leads us into module 3 where I
30:16
explain how a general graph traversal algorithm works even students of
30:22
computer science often miss the fact that all the standard graph algorithms
30:27
you see here in this row are essentially minor variations of one single graph
30:34
traversal algorithm if you understand this general graph traversal algorithm
30:39
you are on your way to understand all 6 of these algorithms so let me give you
30:45
the gist of this general graph traversal algorithm and I'm going to do that by
30:51
taking the example of small directed graphs and show you how to explore it so
30:58
it's a directed graph you can see that the edges here have directions so think of them like one-way streets so let's
31:06
say we start our exploration from vertex one since we want to figure out which vertices are reachable from there so
31:12
vertex 1 is our source vertex now you can see by visual inspection here that
31:18
this isolated vertex 6 and this vertex seven are not reachable from one because
31:24
you can't go against the direction of the arrow but all other vertices are reachable but in general we want our
31:31
algorithm to run on grass with hundreds of thousands of vertices we can't use visual inspection there to answer such
31:38
questions so we need an automated way to traverse such a graph and answer the
31:43
question which vertices are reachable from vertex one to show you how a
31:51
general graph traversal algorithm works on this directed graph I'm going to use a visualization I will imagine that each
32:00
vertex of the graph is like a fish swimming deep inside a blue ocean shown
32:06
here on the right and I'm going to think of myself as a fisherman standing on
32:12
this yellow beach sand on the left hand side holding a fishing net in my hand
32:22
now when this whole process starts I do have one fish that is already given to
32:29
me as reachable and that is the source vertex
32:35
now since I can reach that source vertex I'm going to imagine that it is already
32:41
trapped inside my fishing net at the beginning when this whole process starts so in the initialization phase itself
32:49
this vertex one is already inside my fishing net this black circle represents
32:54
my fishing net so I'm going to pull this fish out of my
33:01
net and bring it a sure what's great for
33:07
me is that this fish has hooks or direct relationships or edges connecting it to
33:14
a bunch of neighbors which so far lay undiscovered in the ocean but now I I
33:21
can catch sight of them by looking across these hooks and I trap them
33:26
inside my fishing net so notice in the graph for example that vertex one has
33:32
outgoing edges to vertex two and vertex five in the directed graph so these
33:40
vertices have now been discovered why are these hooks and are now reachable
33:48
reachable by me so now I have two more
33:54
fish inside my fishing net and I choose one of them arbitrarily for now and I
34:00
pull on the edge going to it to drag it ashore so let's say I pick what X two and I pull on this edge and I drag it
34:09
sure I place it on the sand and I have an edge now from fish one to fish two
34:16
indicating that it was the edge from vertex one to vertex two that I pulled on to drag vertex to a show now
34:28
look at the outgoing edges from vertex 2 because I can discover new fish using
34:36
them so notice in the directed graph that vertex 2 has three neighbors in the
34:41
graph one of which is vertex one but vertex one already lies on the sand next
34:47
to it so can ignore it but the other two neighbors three and four I didn't know
34:53
they existed until now I catch sight of them and I trap them inside my fishing
34:58
net I then pick the next fish that I'm
35:04
going to drag ashore and again I'm making this choice arbitrarily here so I'm dragging the fourth fish out by
35:13
pulling on this edge from 2 to 4 now
35:19
since two pulled out four I have an edge from 2 to 4 on the sand I then look at
35:26
the neighbors of for the outgoing neighbors in the graph to check if I can discover any new fish but 5 is the only
35:34
neighbor and 5 already lies trapped inside my fishing net so there are no
35:41
new fish that I discover which I can trap so I then decide to pull another
35:50
fish out of my bag let's say fish 5 I pull it out by tugging on this edge from
35:57
1 to 5 5 is dragged ashore and the edge
36:03
from 1 to 5 indicates which edge was used to pull 5 out notice that these
36:08
edges seem to be forming a tree here on the left
36:15
next look at who the outgoing neighbors of vertex v are and the only neighbor of
36:20
v is vertex 4 which already lies captured on the sand so I don't have any
36:28
new discoveries so there are no new
36:34
additions to my fishing net at this stage I have no choice but to
36:41
pull out this lone vertex that lies inside my net I pull on the edge from 2
36:48
to 3 and drag 3 asure when I look at the
36:55
outgoing neighbors of 3 I see a 4 but four already lies next to it in this
37:03
tree so there are no new discoveries no new fish that I discover in the ocean my
37:11
fishing net at this stage lies empty so
37:16
at this point I have no way to capture more fish even though there are a couple of them still undiscovered inside the
37:24
ocean but I can't reach them and notice that these are the two vertices which I could not reach from vertex one so let's
37:35
look at the result of this graph traversal the first thing that we can
37:41
note here is that on the beach sand we got a search tree the root of that
37:49
search tree was the vertex the source vertex from where we started our
37:54
exploration the second thing to note is that the vertices in this tree are
38:02
precisely the set of vertices that are reachable from s in fact every single
38:08
vertex that was reachable from s lies somewhere inside this tree the third
38:14
thing to notice is that if there was a vertex let's say C which was pulled out
38:20
of the back by vertex P that means we tugged on the edge from B to C to drag C ashore then P is the parent of C in the
38:31
search tree so for example Y is 2 a parent of three because we tugged on the
38:36
edge from 2 to 3 drag 3 a sure
38:41
I want to now be more precise about what I did by writing it down as pseudocode
38:50
so I'm writing a function for doing a general graph traversal starting from
38:58
some source vertex s and initially all
39:03
the vertices lie undiscovered inside the ocean except for the source vertex s
39:14
right so what I do is I initialize an empty bag or fishing net and then I put
39:21
this source vertex s in it my mark s as discovered
39:28
I will now repeat the following as long
39:36
as my bag or net is not empty that is as long as there is a fish still trapped
39:43
inside the net I'm doing something what am i doing you may recall that I'm
39:50
picking some fish to pull out and I made that choice arbitrarily I tug on an edge
39:57
to pull it out I took on an edge to pull it out I place it on the sand where it
40:03
becomes part of this growing tree and then I check its outgoing neighbors and
40:09
I'm gonna do that by looking at the adjacency list for that vertex or the adjacency map for that vertex which I
40:15
assume has already been built before you know we are executing this function so
40:24
if I see if I see that there is a
40:30
neighbor in the adjacency list which still lie undiscovered I discover them
40:39
and I add them to my bag or net and I keep going right I keep looping around
40:45
like that so when we look at an
40:51
arbitrary point in the execution of this algorithm I will have a bunch of
41:00
captured vertices on the left which are part of this growing tree I will have a
41:06
bunch of discovered vertices on the right some of which are tracked actually
41:15
all discovered vertices here are trapped inside my fishing net or banned and there are others which still lie
41:21
undiscovered so in the next step I will pick an arbitrary vertex from the pack
41:28
let's say I pick this vertex U and I'm
41:33
going to pull it out by tugging on some edge leading to it
41:39
so let's say I I tug on this edge from p2 you pull you out P is part of the
41:46
tree which means there is a sequence of tree edges from s to P right so this
41:53
path could have a bunch of vertices so I
42:01
pull V ashore I add u I pull you ashore
42:06
and I add you to this growing tree on the left and so now there is a sequence
42:13
of tree edges starting from s and reaching you so let me ignore the other
42:21
vertices in the tree for now and I'm just showing you that you lies somewhere inside this growing tree I then look at
42:28
the outgoing neighbors of U in the original graph some of them might
42:34
already be captured and lie on the beets and some of them might have been discovered but are still lying inside my
42:43
fishing net but what I am interested in is are there any neighbors that are
42:49
still undiscovered there may be a couple of them that were that are still undiscovered let's say V and W if there
42:58
are any undiscovered neighbors of U I'm now discovering them so I grab them and
43:07
I add them to my bag or net
43:13
keep doing this over and over and over until at some point my bag becomes empty
43:21
and I exit this while loop at that point
43:29
so notice that in the course of execution of this algorithm each vertex
43:35
except the source vertex started out undiscovered at some point the vertex
43:47
was discovered and added to the bag and then there came a point when this vertex
43:57
was pulled out of the bag and moved into the search tree on the left so the
44:04
typical life cycle of a vertex is that it's initially undiscovered and discovered but still lying inside this
44:10
this bag and then it's pulled a shore and captured inside this de screwing
44:15
search tree so how do we maintain this
44:22
collection of vertices in the bag so if
44:27
we use you know depending on what data structures we use to represent the bag
44:35
we get different graph traversal algorithms so if PI you use a first in
44:41
first out queue but store the vertices in my bag the algorithm I get is spread
44:47
first search if I am storing the vertices in a lastin first-out stack
44:52
then the algorithm would be depth-first search if I am using a priority queue to
45:01
store the discovered vertices then at the time of discovery each vertex is
45:07
going to be inserted into the bag with some priority value and the vertex that
45:14
is going to be pulled out of the bag next is going to be the one with the highest priority here if I'm using a
45:22
first in first out queue the vertex that I am pulling out is basically the first vertex that was
45:28
in depth-first search the vertex that I am pulling out is the last vertex that
45:35
was added in or the most recent vertex that was added so when we are using a
45:41
priority queue depending on how the priority is defined we are going to be
45:47
pulling out a different vertex and depending on how we define the priority we get different algorithms different
45:55
advanced algorithms like Dijkstra's algorithm or prims algorithm or best-first or a star
46:06
so all of these algorithms are variations of this general graph
46:12
traversal algorithm now since BFS and
46:21
DFS are specifically considered important for an interview right recall
46:26
the courts from Google Facebook Amazon had shown you at the beginning in our graphs curriculum we next zoom in to bed
46:36
first search and depth-first search in more detail in breadth-first search we
46:43
explore the graph in concentric waves out of a source vertex much like how our disease might spread out from a source
46:50
in a population in depth-first search we explore the graph one path at a time
46:56
going deep down a path and then retracing our steps back when we hit a
47:01
dead end and then trying some other paths so we divide our coverage of
47:07
breadth-first search and depth-first search into four quadrants great first search on undirected graphs depth first
47:14
search on undirected graphs breadth first search on directed graphs and depth first search on directed graphs
47:20
the search trees that you get for these four cases from the same code for these
47:28
four cases are different they have different properties which is why we
47:35
want to distinguish them and separate them out into these four cases and we look at undirected graphs first because
47:42
the breadth first search and depth-first search trees on undirected graphs have simpler properties I don't have time to
47:51
explain what the breadth first search tree looks like but just want to show you an example of it here this is a
47:57
breadth first search tree where all the edges in the original graph on which we
48:03
did the traversal have been shown the edges that make up the tree recall the
48:11
tree on the beats and those edges are called tree edges and the remaining edges if I take all the remaining edges
48:17
in the graph and I add them representing them as cobwebs so to speak
48:25
these remaining edges are of a second type we call them as cross edges the
48:32
depth-first search tree that is built by doing depth-first search on that
48:37
original graph also has three edges making up the backbone of that tree on the beat strand the remaining edges of
48:45
the graph appear as back edges which connect descendants to ancestors in the
48:51
tree it doesn't matter which undirected graph you explore at the end if you used
48:57
breadth-first search to traverse the graph you're going to get a breadth first search tree with tree edges and
49:03
cross edges and if you used depth-first search to traverse the graph you're going to get a depth first search tree
49:08
with tree edges and back edges which look like this once we know the
49:15
properties of a general breadth first search and depth-first search tree we
49:20
can then think of building a separate breadth-first search and depth-first search tree for each piece in the
49:28
original graph the original graph may not be connected over all it it might be
49:35
it might come in pieces which we call as connected components so when we try to explore the graph by starting from some
49:42
source vertex we can only explore the piece that the source vertex was in the
49:47
vertices in other components are not reachable so we have to launch a
49:53
separate graph traversal for each connected component if we want to explore the whole graph and one of the
49:59
popular interview questions is to find all the connected components of a
50:05
general undirected graph which is given to you as input so that is nothing but
50:11
the number of connected components is nothing but the number of times we have to launch our graph traversal algorithm
50:17
to explore the whole graph now since a general graph traversal algorithm
50:25
requires us to launch breadth-first search or depth-first search multiple times we give students but
50:32
plate or a coding pattern for a general BFS or DFS problem I don't have the code
50:38
itself here but I have the three the the names for the three parts of the coding
50:44
pattern in the first part you have to build the graph which means you have to
50:49
build the adjacency list or the adjacency map for the graph and we have
50:54
a piece of code which does that the second part of the code requires us to write breadth-first search or
51:00
depth-first search as a function y as a function because we may have to launch it multiple times to explore the whole
51:06
graph the third part of the code is what launches breadth-first search or
51:11
depth-first search as many times as needed until every single vertex in the graph has been discovered so these three
51:18
pieces of code which form our coding pattern appear over and over and over and almost every graph problem more
51:25
complex graph problems will need you to add some extra lines to that code but
51:31
the lines making up the basic coding pattern are stable this makes it easy to
51:36
write code for any BFS or DFS problem because you already know what lines make up the basic skeleton you only have to
51:43
focus on filling the remaining lines in the additional lines which are generally not that many for example if the
51:50
interview problem is about detecting cycles in an undirected graph you just
51:55
have to take the coding pattern the basic coding pattern and add a couple of lines to detect a cross edge if you are
52:01
using Britt's first search or a back edge if you are using depth-first search because crosses and back edges depending
52:09
on what creators always indicate the presence of cycles in an undirected
52:14
graph we then look at more complex problems on undirected graphs like
52:20
detecting whether a graph is bipartite or not and for many of these problems you can use either breadth-first search
52:26
or depth-first search but you have to be familiar with the breadth-first search tree and the DFS tree to know how to use
52:35
their properties to answer the given question so you to map the given question to some property in the tree
52:41
which you are going to detect in your code so in either case the chord is just a few
52:46
lines added on top of the basic coding pattern there are some problems though
52:52
for which only breadth-first search would work for example if you want to find shortest paths in terms of the
52:58
number of hops in an undirected graph you can't use depth-first search only breadth-first search gives you shortest
53:05
paths and again the cord is just a few lines added on to the basic coding
53:10
pattern there is a special category of bread first search and depth-first
53:16
search problems in which you are given a two-dimensional grid so you can decide
53:23
that every cell of this grid will become a vertex and there will be an edge
53:28
between two vertices if they if those cells are neighboring cells in the grid
53:33
so that's how you might decide to model the grid as a graph but you are expected
53:39
to do something special for this subcategory of problems in an interview the interesting thing about this
53:44
category of the subcategory of problems is that if I'm given the ID of a vertex which will be the row and column number
53:52
of that vertex I can figure out from the ID itself who the neighbors are right
53:59
for example the neighbors of this vertex with ID X comma Y can only be X plus 1
54:09
comma Y X minus 1 comma Y X comma Y minus 1 and x comma y plus 1 provided
54:17
that these exist within the legal boundaries of the grid so do I need to build an explicit adjacency list to tell
54:23
me the neighbors no I can save space and avoid building an explicit a decency list instead I can just write a get
54:30
neighbors function to whom I give a vertex ID and it will return me all the
54:35
valid neighbors of that vertex so we can represent the edges of the graph in an implicit way by having a get neighbors
54:43
function instead of building an explicit adjacency list and this approach saves a
54:48
space without sacrificing time and even marking vertices as discovered or not
54:55
can be done within the input grid instead of having a separate area
55:00
so let me illustrate this by taking the example of a popular interview problem
55:05
called the flood fill problem so in this
55:11
problem you are given a two-dimensional image which is basically a 2d array of
55:17
pixels each pixel is represented by an integer value and the value represents
55:23
the color of that pixel you're also given the coordinates of the starting pixel this this is the starting pixel in
55:31
this example and you are given the value of a new color so the color of this pixel was originally white and the value
55:38
of the new color is green and you have to implement a flood fill operation and by that what we mean is you have to look
55:46
at all the white cells that are connected to this original cell and
55:53
color them all green so this corresponds
55:58
to the common operation in you know paint software where you decide what
56:03
color you want to fill a particular region with and then you tap on a particular pixel and then when you do that that entire region gets colored
56:12
green so I don't have much time left right now so otherwise you know in the
56:20
class we often give like a few minutes of time to students to think and we have a lot of discussion before we finalize
56:26
what are we going to be want to use but in this case either breadth-first search or depth-first search would work and we
56:36
can let's say if we use breadth-first search we can actually see a parallel between the the pseudocode that i had
56:42
already written and the code that will actually get written once we implement
56:49
breadth-first search so for each line that i had in the original pseudocode there is a parallel line where I am not
56:57
looking at a general bag I am specifically looking at a first in first out queue I Marquess has discovered and
57:05
in the case of 2d grid problems you you need not have a separate array that
57:11
tracks which vertex has been discovered or not you can we change the the input cell in the grid
57:18
itself to mark it as discovered and while the queue is not empty we pull out
57:23
one of the vertices that's equivalent to popping the queue and for each neighbor of that vertex and
57:29
this is where we call the get neighbors function so normally we would look at
57:35
look up the adjacency list of this vertex you just check each neighbor in
57:41
the case of 2d grid problems we call this gate neighbor's function which returns to us a list of neighbors and if
57:47
the neighbor lies undiscovered which which is equivalent to saying if the neighbor still has the old color then we
57:55
put the neighbor in the band we add it to the queue and we mark it as discovered which means we change the
58:01
value of that cell to green and so when we run this and we exit red for search
58:07
starting from this source vertex on which we clicked so we call
58:13
breadth-first search with the coordinates of that starting vertex when we exit here this single breadth first
58:18
search call would have painted or flood filled that input grid and we just
58:24
returned that grid at the end so this is the get neighbors function just leave
58:30
that to you to think about and sometimes there are edge cases that we worry about
58:36
like if the new color is is equal to the old color our code might go into an
58:42
infinite loop so that's something you have to be careful about and we also want to think about why we use
58:49
breadth-first search what if we had to place the queue by a stack would it have worked the answer is yes it could have
58:55
worked and leave that you to think about what if the queue was replaced by a priority queue would it have worked it would still have worked but we won't use
59:02
Dijkstra or prims algorithm or those other algorithms to do a flood fill not
59:08
just because they look a little artificial in theory we can use them but adding vertices to a priority queue
59:14
removing vertices from a priority queue is more expensive than adding them to a queue or stack so we use breadth-first
59:21
search and depth-first search for that reason now having looked at a bunch of problems on undirected graphs
59:27
next move to doing read first search on directed graphs where the tree ends up being more complex in fact when we
59:34
analyze this tree in the class we often find that I mean we always find that we
59:40
can't solve any sophisticated problems on directed graphs by looking at the BFS
59:46
tree we can only solve shortest path problems and so we reserved BFS undirected graphs specifically for
59:53
shortest path problems and this is an example of one of the problems we look
59:58
at in now in in our class given a slick and ladder game which notice is again in
1:00:03
the form for grid find the minimum number of throws required to to be in the game so this is not an explicit
1:00:10
graphs problem we spend a lot of time thinking about why is this a graphs problem how do we
1:00:16
model it as a graph how do we model it in a way that we can just plug and play the chord that we already wrote for
1:00:21
breadth-first search finally we move to depth first search on directed graphs
1:00:28
which is the most sophisticated which leads to the most sophisticated kind of tree and with small tweaks in our code
1:00:37
specifically for detect for adding something called arrival and departure times we can convert depth-first search
1:00:44
into a Swiss Army knife which can literally allow us to detect all kinds of non-trivial properties about the
1:00:50
graph about the directed graph so not only can we detect cycles we can also
1:00:59
sort the vertices from left to right in a way that the edges are only going from
1:01:05
left to right if the graph hired no cycles so that's called a topological sort
1:01:10
apology refers to the spatial arrangement so if you want to arrange the vertices in a way that all edges go
1:01:16
from left to right that's called a topological sort one of the most popular applications of depth-first search this
1:01:22
by the way was the depth-first search tree fairly complex tree which we spend
1:01:29
a substantial amount of time in our life plus on this we also look at a modified
1:01:34
bread first search kind of algorithm called Kahn's algorithm which is another way to do topological sort
1:01:41
and then we look at advanced applications of depth-first search like finding bridges articulation points and
1:01:47
strongly connected components the first two problems apply to undirected graphs the third problem applies to a directed
1:01:54
graph and that's where you know we we need to do the most sophisticated
1:02:00
analysis of the depth-first search tree which gives us you know a very
1:02:07
conceptual way to answer these problems so this is what we generally do in the
1:02:15
green section right the easier part of the graphs topic I've shown dodging and
1:02:22
ghost Raju here but increasingly you know there you know there are more and more questions being asked on them so I would probably classify them today as
1:02:29
part of the life class just to give you a quick summary of what we do in our
1:02:36
advanced graphs modules we consider what would happen if the graph was not a
1:02:42
static graph but if new vertices and edges were coming in dynamically so if you want to solve problems then we can't
1:02:48
just keep resolving the same problem over and over every time a new vertex or a new edge comes in we have to do
1:02:54
something better and that leads us to actually invent a new kind of data
1:03:01
structure instead of using breadth first search or depth first search each time we invent a new kind of data structure called a union-find data structure we
1:03:10
optimize it so that it runs efficiently that's the weighted union-find structure
1:03:16
and we are we have a very neat optimization called path compression which basically makes union-find run in
1:03:23
time that is practically speaking the same as breadth-first search and depth-first search not theoretically but
1:03:29
again we're talking about an interview here where they practically run in the same time so very often you'll find that
1:03:35
there will be three ways in which you can solve many of these problems either using breadth first search depth-first search or union-find and the code for
1:03:42
union-find can some can often be smaller than the code for you know dead first
1:03:48
search and breadth-first search so we have a coding pattern for union-find as well so for example path compression is not
1:03:55
something the code for that is not something you'll find in an algorithms textbook which keep you can use readily
1:04:01
in an interview there's a very efficient version of that code which competitive programmers are normally familiar with
1:04:07
but again you that's why I said you have to combine both sides of in preparing for an interview the practical side as
1:04:14
well as skew now union-find is applied to a relatively sophisticated problem
1:04:20
the problem finding the minimum spanning tree you know weighted undirected graph so so
1:04:26
far all the graphs that we were looking at did not have weights on the edges but
1:04:31
once you start adding weights on the edges which could represent something like distances or costs for going from
1:04:38
one vertex to its neighbor then we can ask is there a way to connect all the
1:04:43
vertices together in a tree so that the cost of that tree which is basically the sum of the weights of all the edges on
1:04:49
the tree to make that cost minimum so for that there is an algorithm called
1:04:55
kruskal's algorithm which uses union-find but normally we are students to come back to these advanced graph
1:05:01
topics after they have done greedy algorithms and dynamic programming algorithms because we don't want to memorize kruskal's algorithm as a as a
1:05:08
named algorithm we want to understand how to design this algorithm it so happens that the name happens to be
1:05:14
kruskal's algorithm but we don't care about the name we want to understand how it's designed
1:05:20
there's also another greedy algorithm for solving the minimum spanning trees problem called prims algorithm and the
1:05:26
code for this again is a small tweak of the code for breadth-first search in
1:05:32
fact again algorithms textbooks won't give you the correct code because in a practical interview setting the priority
1:05:38
queue library that you'll have to use to write your code does not provide you operations like decrease key or Instant
1:05:46
Search which are required to actually implement prims algorithm efficiently so
1:05:55
how do you bypass this constraint that the standard priority queue libraries don't provide you these operations well
1:06:00
there's a way and we cover that in our classes for how how to implement this quickly with
1:06:06
out compromising on the asymptotic time complexity the same thing is done when
1:06:14
we look at another type of popular problem on weighted graphs called the single source shortest path problem and
1:06:19
again there's a greedy algorithm called Dijkstra's algorithm for it the code for it is almost identical to the code for
1:06:24
prims algorithm again and again the same issues arise here you need to be able to implement this quickly without going through this
1:06:34
bypassing this constraint so you have to implement an Dijkstra's algorithm in in a way that you don't use these two
1:06:40
operations and what if the weighted
1:06:47
graph could have negative edge weights well then Dijkstra's algorithm is not guaranteed to give you a correct answer by the way being go through the proofs
1:06:53
for why these algorithms that we are designing are correct this is important
1:06:58
whenever you have a greedy algorithm you need a proof otherwise typically greedy algorithms don't tend to work correctly
1:07:04
so but if you've done dynamic programming then with negative edge
1:07:10
weights you can actually design a dynamic programming algorithm on a graph and you find out at the end that it's
1:07:15
called bellman-ford but we don't care about that for our purposes it's a dynamic programming the same dynamic
1:07:21
programming strategy which we used on other problems in apply on graphs and get an algorithm which happens to have
1:07:28
been invented by bellman-ford and finally the the most sophisticated
1:07:37
shortest path problems generalized the single source problem to all pairs where
1:07:42
for every possible source and every possible destination you have to find the shortest path from the source to the
1:07:48
destination now you can do this by running either of these two algorithms n
1:07:53
times where n is the number of vertices in the graph but it turns out that there
1:07:59
is a slightly more efficient way to do it if you have sparse graphs for example
1:08:05
there is a slightly more efficient way to do it called Johnson's algorithm which runs also on graphs with negative
1:08:12
edge weights which Dijkstra's won't be able to do so we can make Dijkstra's
1:08:17
algorithm run on sparse graphs through a small tweak called Johnson's algorithm and if the
1:08:24
graph happens to be dense which means there are a lot of edges in the graph sparse graphs are graphs where there are
1:08:31
the number of edges is order the number of vertices dense graphs are graphs where the number of edges is order
1:08:37
square of the number of vertices with order n square so there we again use dynamic programming and we can reinvent
1:08:44
this algorithm which happens to be called as floyd-warshall and finally
1:08:50
there are some optimizations like solving the shortest path problem by not
1:08:56
proceeding purely from the source but from the source and the destination so doing a bi-directional search from both
1:09:02
sides which can improve the practical running time and then there are finally
1:09:08
algorithms that are small tweaks of Dijkstra's algorithm which allow us to
1:09:14
preferentially go in the direction of our destination so if we have some kind of information available to us while
1:09:21
doing Dijkstra as to how how far we are from our destination we can use that
1:09:27
information to implement an algorithm which which is called a star a star is
1:09:33
not typically covered in an algorithms course it's covered typically in an intro to AI course but we cover it as
1:09:39
part of graph theory and the code for it is actually a very very small tweak of Dijkstra's algorithm so belongs to the
1:09:46
same family so with that you know I would like to close this end this
1:09:53
presentation so this again is a summary of all the topics that we have in our
1:09:59
curriculum hopefully they make more sense now so I'm going to open the forum
1:10:07
for some questions questions and answers since there are let's see there are more
1:10:12
than 100 students I won't be able to answer most of the questions so we will
1:10:18
continue some discussion on uplevel so
1:10:23
operations IQ operations will send you an email with link to the discussion board where we can continue the
1:10:29
discussion in case you have questions that I can't answer a recording of this session will also be
1:10:35
provided to you on up level you will also have access to our previous micro classes from earlier weeks and again IQ
1:10:43
operations will tell you how to access the discussion board you have discussions follow-up discussions on
1:10:49
this class you can also sign up for our next micro class which is going to be two weeks from now so let me open the
1:10:58
Q&A tab here some reason and not able to
1:11:15
open the tab Steph if you are there is it possible for you to read out some of
1:11:20
the questions yeah if you go ahead and just close the presentation really
1:11:26
quickly or yeah great okay on open
1:11:33
questions is there a concept so let me go talk down there was a question that
1:11:42
required arjen's algorithm so the thing is you don't you should not worry about names like Tarr John it's really it's
1:11:51
really an application of depth-first search right and I mentioned in this presentation that you have essentially
1:11:57
three problems bridges articulation points and strongly connected components all three are advanced applications of
1:12:04
depth-first search they all require a more sophisticated use of arrival and departure times which I didn't talk
1:12:10
about but those two times are what make depth-first search very powerful and so
1:12:16
if you understand how to use arrival and departure times to answer this question that happens to be called a star jhin's
1:12:22
algorithm and again it's the same approach for all three all three of those problems how do you handle this in
1:12:28
a real interview well if you haven't gone as you know studied depth-first
1:12:33
search advanced depth-first search it's going to be very hard so these algorithms are typically studied in an
1:12:39
undergraduate course which is why I always tell students it's a good idea to know
1:12:45
at least as much as what say a sophomore at Stanford would know right so we do
1:12:51
cover this in in our program so maybe you were not part of the program
1:12:57
recently so we have we have added these in I read in a book that dynamic
1:13:04
programming problems are essentially a variant of shortest paths on dad yes actually your pre class in the free
1:13:10
class videos on dynamic programming I actually explained dynamic programming by precisely this approach by showing
1:13:18
how every dynamic programming problem is essentially a problem on a on a directed
1:13:25
acyclic graph where each sub problem that you're solving becomes a vertex can
1:13:30
be mapped to a vertex in a dag and then you have to solve each subproblem in a
1:13:35
topological sort order so so that when you are solving any given subproblem all the other subproblems it depends on
1:13:43
should have already been solved and their solutions should have already been cashed last year and some of the modules
1:13:52
were not discussed here you can you can ask operations about that let's see
1:14:00
any other questions is there a concept like good enough code for an interview
1:14:07
no so that's what I mentioned at the beginning that you should not worry about micro optimizations as long as you
1:14:13
arrive at a correct and a fish asymptotically efficient algorithm it's
1:14:19
generally good enough to be able to implement it correctly instead of worrying about micro optimizations and
1:14:24
one of the issues that you find in platforms like leet code for you for example is that they don't distinguish
1:14:30
between optimizations that are required to improve the worst-case asymptotic complexity and optimizations that are
1:14:36
just going to incremental e improve the time complexity maybe by a constant factor those are typically important in
1:14:42
competitive programming contests right those kinds of micro optimizations but they're not important when experienced
1:14:49
engineers are giving interviews at these these companies there are some exceptions though there might be some
1:14:55
problems where the interview specifically says that I want you to do this in one pass instead of two passes so only in those
1:15:01
circumstances do you worry about those micro optimizations otherwise any worse any code that runs in asymptotically you
1:15:10
know the best possible worst-case time is good enough you know online
1:15:19
interviews are compilation errors a significant red flag not necessarily right so for exact to give you an
1:15:26
example there was one student he he he knew the code he had learned how to sew
1:15:33
to detect bridges in an undirected graph that's a very popular in - you know
1:15:39
screening question at Amazon but what he ended up getting was a question on articulation points now the code for
1:15:45
articulation points is a little different from the code for detecting bridges so he wrote the code for bridges
1:15:51
that code would not pass but he was able to rationally explain you know how exactly these arrival and departure
1:15:57
times are being used to you know detect packages that are going as far high up as possible and the interviewer was able
1:16:06
to see that he's able to make that conceptual progress and he passed that
1:16:13
interview even though the the code did not compile so it really depends on
1:16:18
whether the problem is easy medium or hard articulation points are relatively
1:16:23
hard to you know they would be classified as a hard problem it's not a problem on lead code but still it's a
1:16:30
popular question in Amazon so examples
1:16:36
like this are are there can we do AI K remote in fact I K is purely remote
1:16:42
right now because of the covert situation so again you can reach out to
1:16:47
operations for details
1:16:54
shouldn't you be also marked as discovered yes when you well at the time
1:17:03
vertex you or any vertex was discovered and added to the bag we need to mark it
1:17:08
as discovered but when we are talking about 2d grid problems it turns out that
1:17:14
you can you know you can use the input grid itself to store that in information
1:17:20
in in some way that that enables you to avoid using up an extra array an extra
1:17:26
two dimensional array to mark which vertices has been discovered or not what
1:17:33
are some of the other graph algorithms well I have all the graph algorithms that would be required in an interview
1:17:39
in this presentation so these are all that you need to know in fact if you are short of time as I said just focus on
1:17:46
the green topics breadth-first search and depth-first search but if you really want to you know be totally
1:17:53
comprehensive you could look at all these algorithms what are cross and back edges yeah I couldn't get the time to
1:17:59
explain them so those are edges other
1:18:06
than the tree edges which are added to the tree we got on the end on the beets
1:18:12
and remember where I just showed the tree edges but if you take the other edges in the original graph which didn't
1:18:18
make it into the tree if you add those edges in those edges get classified as
1:18:24
cross edges or back edges or forward edges again depending on you know whether they go from ancestors to
1:18:30
descendants descendants to ancestors or between two vertices that don't have an ancestor descendant relationship the
1:18:37
last category is what you call as a cross edge can you show us the syllabus
1:18:44
page well since the recording of this will be available on uplevel you'll be
1:18:50
able to go back and look at the flow chart if you're asking about that if you are asking about the overall syllabus
1:18:56
for the program you know we can reach out to operations or attend one of our webinars
1:19:04
okay so looks like we answered all
1:19:09
questions are there any other questions
1:19:23
so thank you all for your time and yeah
1:19:29
if you have any questions feel free to sign up on up level we will have some follow-up discussions on this
1:19:35
presentation so I'll be able to answer more questions there so Steph do you
1:19:46
want to take over sure thanks everyone
1:19:52
again for joining us today like Omkar mentioned we will be sending out an
1:19:58
email to everyone that's joined it'll have some instructions on how to join
1:20:03
discussion on up level and then to be able to learn more about interview
1:20:09
kick-start please join one of our webinars you can see the sign up link on
1:20:15
our web site interview kickstart calm you'll also find that link in our post
1:20:20
email that we're going to send you all soon if you have any other questions we'll be happy to answer them on the
1:20:27
discussion once we see you on there all right so thank you um her and thanks
1:20:32
everyone for joining this evening hope everyone has a good rest of the night
1:20:54
you
1:21:19
heythe flora I saw your last minute question here about having live classes at IKEA all of our classes are actually
1:21:25
live so if you want more information about that just join one of our webinars and you can find that on our website
1:21:32
interview kicks are calm
1:21:52
you [Music]



## CLASS 2
Bellman-Ford Algorithm | Uplevel with Omkar Deshpande #FAANG
https://www.youtube.com/watch?v=GK1n_F4I2jM&list=PLTjcBkvRBqGH4DZTRho04Gc5wOOP4N4sF&index=2



0:01
[Applause]
0:08
so here is a summary of what we have so far we have defined this function f of v
0:14
comma i to be the cost of the shortest path from the given global source
0:21
to any arbitrary vertex v which uses less than or equal to i edges
0:29
and we also have a recurrence for it f of v comma i is
0:36
the minimum over all the predecessors of v predecessors of v are
0:43
the vertices which have directed edges into v
0:48
for each predecessor we calculate f of that predecessor comma i minus 1
0:55
this is the shortest path cost from the source to that predecessor using less
1:01
than or equal to i minus 1 edges and to that we add the weight of the edge
1:07
from x to v that is the final edge along that path
1:13
and if we take the minimum of the sum across all the predecessors of v we
1:19
should get the optimal value for f of v comma i
1:26
now suppose there were two predecessors
1:31
of v let's call them x
1:37
let's say x and y it is possible that in the graph there could be a vertex
1:44
which is a predecessor of both of these let's call this vertex as z
1:50
so in trying to calculate f of v comma i we would need the values of f of x comma
1:58
i minus 1 and f of y comma i minus 1
2:03
but how do we get the values of these to get the value of f of x comma i minus
2:08
1 we would need the value of f of z comma i minus 2
2:14
and likewise to get the value of f of y comma i minus 1 we would need the value of f of z comma i minus 2
2:23
and of course there could be many other vertices in this graph i'm just showing a simple example here so notice that
2:29
the worker who is responsible for calculating this value has to call someone else
2:38
under him or her to first calculate this value
2:43
and the same thing is going to be done by this peer worker this worker will also
2:50
call someone else under him or her to calculate this value
2:55
and notice that both of these are essentially the same sub problem
3:01
so the same sub problem is going to be handed over
3:07
to multiple people in this imaginary corporate hierarchy if we were to try to implement this
3:14
in a naive way so we have the scenario here of
3:21
overlapping sub problems there is going to be a lot of redundant
3:27
work done in this corporate hierarchy if we try to solve this recurrence equation in
3:34
the naive way we also know that this way of defining the function
3:42
has the property of optimal substructure
3:47
so because we have optimal substructure and overlapping sub problems
3:54
we are naturally led to using a dynamic programming based
4:00
strategy to solve this recurrence equation bottom up
4:06
avoiding repetitive work so we will start from the smallest sub
4:12
problems and build up the solutions to larger and larger sub problems until finally we have the value of
4:21
f of destination comma k plus 1 right remember the original problem had
4:26
at most case stopovers which means at most k plus 1 edges along
4:31
the shortest path from the source to the destination
4:36
so how many unique sub problems are there if we look at the first parameter v
4:45
let's assume that the vertices are in the range from 0 to n minus 1. so there are n cities in the graph that
4:52
are given to us their ids are from 0 to n minus 1.
4:57
the second parameter is the number of hops
5:04
so the number of hops could vary from 0
5:09
all the way up to k plus 1 we can't go beyond k plus 1 hops
5:17
so in terms of the combination of the two parameters there are n choices here
5:24
and there are k plus 2 choices here so this means we have n multiplied by k
5:31
plus 2 unique sub problems to solve
5:39
so let's build a table which has a cell for every unique sub problem
5:49
so the first parameter v can range from zero to n minus one
5:54
the second parameter i can vary from zero to k plus one
6:02
so if we imagine this table with
6:07
n multiplied by k plus 2 cells then each
6:14
cell in this table represents a unique sub problem which we have to solve
6:20
and finally we want the value of f of destination comma k plus 1 so how
6:28
will that overall problem get mapped to this table well let's say the
6:33
id of the destination is somewhere here we basically want
6:39
the value that is stored in this particular cell
6:45
let's say this particular cell represents this the overall problem that we want to solve and the value here
6:52
is something we want to be f of destination comma k plus 1.
6:59
so what is the right way or the right order in which to solve all
7:04
these sub problems in a way that we successfully managed to compute this value
7:12
well in general we have to make sure that when we come to any vertex
7:17
let's say or any sub problem f of v comma i
7:23
let's say this is f of v comma i
7:28
this particular cell we should have
7:34
all the sub problems that it depends on solved so that the values for them are
7:40
available which can be used to compute the value of f of v comma i
7:46
so where are the predecessors of v going to be
7:52
well the predecessors of v could be any of the vertices of the graph
7:59
we don't know except for v itself any other vertex could be the predecessor but we know
8:05
that f of v comma i is going to depend on f of
8:11
x comma f of x comma i minus 1
8:17
right that's what we had here so
8:23
we need to ask where is f of x comma i minus 1 going to be found
8:30
in this grid well it's going to be found somewhere in this column
8:40
right this column represents i minus 1 hops
8:48
so we are going to have some predecessors of v and their values
8:55
let's say this is this is one of the predecessors of v this value might be used
9:02
to determine this maybe this is another predecessor of of v so this value will
9:07
also shape will also influence the value of f of v comma i and likewise there could be other
9:13
predecessors whose values can also influence the value of f of v comma i we have to consider all of those
9:19
options and pick the one which is the best
9:24
so since the value of any cell seems to depend on a bunch of entries in the
9:30
column to its immediate left the natural way to solve these sub problems
9:36
is to fill up this table column by column from left to right that
9:43
will ensure that when we come to any cell the sub problems that it depends on
9:48
would already have been solved and the solutions to them available in the column on the immediate left
9:54
so let's start by filling in the table with the
9:59
left most column which is column 0. what values are we going to put in here
10:05
in the leftmost column so let me mark the fact that we have to allocate
10:12
a two dimensional table of size
10:17
n multiplied by k plus 2
10:22
and since we are trying to minimize the value
10:28
of f of v comma i this is a minimization problem
10:33
let's initialize the table entries by default
10:39
to infinity right as soon as we find that there is a
10:45
cheaper way to to get to a particular vertex using a certain number of hops we can always
10:50
bring down the value from infinity down to whatever that or whatever the value that
10:56
we find so by default everything will be initialized to infinity
11:02
so let's think now about filling in the leftmost column so what does the leftmost column
11:08
indicate leftmost column indicates entries of this form f of v comma 0
11:18
since we are talking about column 0 here so what does this mean if we go back to
11:23
the english definition of f of v comma i this would mean the cost of the shortest path
11:30
from the source to v
11:37
using less than or equal to zero edges
11:43
now how can there be any path that uses less than or equal to zero edges
11:50
there obviously cannot be any path from the source to v that uses less than or equal to zero edges so basically the
11:56
left most column is going to be filled with infinity values so this is in general going to be
12:02
infinity as long as the source
12:08
and v are not the same but if we ask this question for
12:15
f of source comma zero what is the cost of the shortest path from the source to the source which uses
12:21
less than or equal to zero edges well that cost is obviously zero
12:27
so if we pretend that this is the source vertex id here so this is the row for the source
12:34
this value here could be 0
12:39
but all the other values in this column will have to be initialized
12:45
to infinity so let me
12:50
mark all of these values as infinity
13:00
now let's fill in the next column let's say that's column 1.
13:08
so when i come to say when i'm looking at this particular cell
13:14
this particular cell here represents the value of f of
13:21
0 comma 1 so this is basically the cost of the
13:27
shortest path from vertex source to vertex 0
13:33
using less than or equal to 1 edge so we have a vertex 0 we have a vertex
13:41
called source we are asking what is the cost of the shortest path from the source to vertex
13:47
0 that uses less than or equal to 1 edge so if there is
13:54
an a direct edge from the source to vertex 0
13:59
f of 0 comma 1 will be the cost of that edge otherwise there is no way
14:05
to get from the source to 0 using less than or equal to 1 edge
14:11
so notice that this value f of 0 comma 1 in terms of the recurrence
14:17
would be the min over all the values all the predecessors of zero so zero might
14:22
have a bunch of other incoming vertices in the graph
14:28
we look at each of them and we look at the values of f of
14:34
whatever the ids for them are let's say the id for this vertex is x we look at the value of f of x comma 0
14:43
and then to that we add the weight of the edge from x to vertex zero
14:50
right here the zero represents the number of hops here this zero represents the id of vertex zero
14:57
so notice that this value will in general be infinity since the left most column is populated
15:04
with infinity values so in general for all predecessors except for the source
15:10
itself calculating this sum is going to result in a value that is equal to infinity
15:17
because f of x comma 0 is infinity but for the source itself
15:22
we know that f of source comma 0 if if x was the source itself
15:30
f of source comma 0 is 0 so there is a direct edge from the
15:35
source to vertex 0 the cost of that edge the weight of that edge
15:41
will become the value of this sum
15:46
and that is what we will pick to be f of 0 comma 1 if we are taking the min across all
15:53
the predecessors of vertex 0. so the value that will be stored here is
16:00
basically the value on the the weight of the edge from the source to vertex 0
16:06
if there is an edge like that otherwise it's going to be infinity and so we can fill in
16:11
column 1 like that one question to ask here is what should
16:17
we fill in for this particular cell what is the value of f of
16:23
source comma 1 right that's the value of
16:29
of of this particular cell here well f of source comma 1 is nothing but
16:35
the cost of the shortest path from the source
16:41
to the source using less than or equal to one edge
16:50
now in a trivial sense every vertex is the predecessor of itself
16:58
with a cost of 0 on the edge from that vertex to itself
17:03
this is not just true for the source but for any vertex v we can treat
17:08
v as its own predecessor with a cost of 0 on the edge from v to itself
17:15
so this means that if we want to compute f of v comma i the baseline that we can start with
17:23
is basically f of v comma i minus 1 so
17:28
f of v comma i can be no larger than f of v comma i minus 1 because if
17:34
there is a short if if there is a way to reach from the source to any vertex v
17:40
using less than or equal to i minus 1 hops that particular way is also a way to
17:46
reach vertex v from the source using less than or equal to i hops
17:52
so we can start out by assuming f of v comma i is equal to f of v comma i minus 1 for any
18:00
v and and any i and then we can see if we can improve it
18:05
further so in that sense this zero in the table
18:11
can be directly transferred or copied into this cell here
18:18
because that's the baseline for f of source comma 1 f of source comma 1 can
18:23
start out by being initialized to f of source comma 0.
18:29
so in general any cell f of v comma i
18:34
can first borrow the value in the cell to its immediate left
18:41
and then we look at the actual predecessors of v in the graph and see what the cost of the shortest
18:48
path through them is and if we get a better cost then we update it
18:55
so this means that when you are looking at a general cell lets say f of v comma i
19:00
the value here can start out by being initialized to the value in the cell to its immediate left
19:08
and in general that's true for any column when we are thinking of filling in
19:16
column i for example all of the values in column i can start out
19:21
by being equal to the values the corresponding values in the column i minus 1
19:28
and then for each vertex in column i we look at the actual predecessors in
19:34
the graph and see if the path through them if any of them leads to a better
19:41
shortest path a cheaper path to them and if so we update the value
19:47
so let's go back to writing the code here we had the initialization that was done here
19:53
and now we have to actually write the code for the for filling in the whole table the
19:59
remaining columns from column one to column k plus one
20:04
and remember the column numbers here represent the hops so we will be filling in
20:10
the table entries the shortest path values for larger and larger and larger
20:16
values for the number of hops so
20:21
let's fill in the table column by column so let's say for
20:26
the hops varying from 1 to k plus 1
20:31
right or the column number varying from 1 to k plus 1 we have to write the code here now
20:37
to fill in the entries in
20:43
[Music] the column
20:48
to the whatever the hops variable is
20:54
so how do we do that well we'll first start out by initializing the whole column
20:59
to the column on the image at left so that's the baseline so for each
21:07
vertex v in the graph from 0 to n minus 1
21:13
we will set for that table
21:18
the the value of the cell at row v and column equal to hops
21:26
will be initialized to the value in the preceding column
21:34
for the same row right this is the baseline that we are starting from
21:39
which i talked about a few minutes ago and now we have to
21:47
fill in the values by looking at the the real predecessors in the graph
21:55
now let us think about this a little carefully for a moment does it matter in what order we fill in
22:03
the entries in a particular column since the value in a particular cell
22:09
depends only on the values in the preceding column it does not matter
22:15
we write this down to make this clear it does not matter in what order
22:23
we fill in the values we fill in
22:29
the values within a particular column
22:37
so we could go top down we could go bottom up or i could just pick the vertices
22:44
arbitrarily in that column and set the values for for those vertices in any order i
22:51
want what is even more subtle is that even for a particular vertex
22:58
it does not matter whether i look at all of its predecessors in one shot
23:06
or whether i look at them one at a time with some gaps in the middle
23:13
so what i mean by that is if let's say vertex v has three predecessors
23:20
in the graph so i have some baseline value for f of v comma i which is basically f of e comma
23:26
i minus 1. now i look at the first predecessor
23:31
basically i am looking at this edge from the predecessor to v i look at the f
23:37
value here for the predecessor i add the weight of the edge from that predecessor to v and i see if the
23:43
sum is better or smaller than the existing value of f of v comma if it is then i'll
23:49
update the existing value of f of v comma i now having done that
23:54
i could now do some go go and do some other work on some other vertex and then i could come back to this
24:00
vertex later and now look at the second edge from the second predecessor to v
24:06
and i can i could again do the same calculation i could find the f value i could look up the f value here which should be available in the preceding
24:12
column i add the weight of the edge into v and i see whether the sum
24:18
now is a value that is lower than f of v comma i or not if it is i update f of e comma i
24:24
and then i go somewhere else and i do some other work and then i come back later and i look at the third
24:31
predecessor i look at the f value i add the weight of the edge to b
24:38
to that f value and then i see if f of v comma i can be lowered by that and if so i update f of
24:44
v comma i so it's not necessary that i had to look at all three of these predecessors together
24:52
now why is this important this is significant because
24:57
if i don't have to look at my predecessors in one shot that means i don't need to build an adjacency list
25:03
for the graph in my code if i had to look at all my predecessors
25:10
in one shot then i would need an adjacency list structure to tell me who my predecessors are
25:16
in fact this adjacency list structure would record my incoming neighbors not my outgoing neighbors which is what i
25:22
generally record when i build the adjacency list for a graph but i would need an explicit adjacency
25:28
list structure to tell me who my incoming neighbors are so that kind of structure has to be
25:34
built at the very beginning of the code but i don't need that i can look at the edges
25:41
in their raw order in which they were handed to me in the edge list in the
25:47
input edge list i could look at each edge one at a time
25:53
so let us say i am looking at the edge from u to v when i look at this edge i know there is
25:59
a weight associated with the cost associated with that edge i could at that point set the value of f
26:06
of v comma i to be equal
26:12
to either its i mean either it does not change its value or it could be lowered
26:17
potentially to a value that is f of u comma i minus 1
26:22
plus this w right if f of u comma i minus 1 plus this w
26:29
equals a number that is smaller than f of v comma i then i can update f of e comma i to be
26:35
that so i can say f of v comma i is the the the min of
26:42
let me erase this here f of v comma i
26:47
is going to be the min of f of v comma i and f of u comma i minus 1 plus w
26:56
so let's go back to the code and write this in so i'm going to look at the edges of
27:02
the graph in any order i want so i could literally look at them in the order in which
27:09
they were given to me in the edge list i'm just going to assume that the edge list is called
27:16
edges here so for each edge for each raw edge in the
27:23
edge list i could set or update rather the value of
27:30
table of v comma hops right remember hops is the
27:35
the column number that i'm in i could update this to be the min of
27:42
whatever it is right now and
27:49
the alternate value which is the value table of
27:56
u comma hops minus 1 right this is f of u comma i minus 1
28:03
plus the weight of the edge from u to v
28:11
so if i do this update for each edge in my graph
28:17
i would have updated all the f values in my current column number which is
28:24
given by the hops variable so having done this
28:29
i could then go back and increment my hops value
28:34
which means i start filling in the values in the next column number to my immediate right
28:42
so this is what we keep doing until we have filled in the whole table
28:47
and at that point the value that we were we wanted for the original problem which was f of
28:54
destination comma k plus 1 should be sitting in the rightmost column
28:59
so we just return that value which should be sitting in the table entry corresponding to that
29:05
particular vertex id that's the row number and the column number which is k plus 1.
29:13
right remember k is a number that is given to us as part of the input
29:19
the source is also given the destination is also given and the ids are given of the vertices
29:24
from 0 to n minus 1 apart from the raw list of edges so what is the time complexity of this
29:31
algorithm well let's look at the main loop first
29:36
we know that this for loop this outer loop where we go column by column this is going to run k plus 1 times
29:44
and each time it runs we are going to be spending order n time setting the baseline values in that
29:50
column and then we are going to iterate over each edge
29:57
in the graph so that is going to be order m time to do that since there are
30:03
m edges assumed to be in the graph for each edge we spend a constant amount
30:08
of time so we spend order m plus n time each time
30:13
this outer loop runs and since m is
30:19
typically bigger than n i could express the total complexity
30:25
here as order of m plus n times k plus 1
30:31
which is roughly order m times k since m is more than n i can
30:37
write this as order m times k now we also had some initialization
30:43
prior to that we allocated a 2d table of size n times k plus 2
30:49
and we set these initial values in the first column so that that would
30:56
have added just order n time initializing the table itself would have
31:02
taken time order n times k right so order n times k plus order m
31:10
times k is basically order m times k since in general
31:16
m is going to be larger than n otherwise we could write it as order m plus n times k if we wanted to but let's
31:22
say it's order m times k so that's the overall time complexity
31:28
what is the space complexity well the size of the table that we are using is
31:35
n times k plus 2. so the space the total space that we
31:42
will be using is order n times k plus 2 which is order n times k
31:49
now you might notice that when filling in the table the values in any particular column
31:55
depended only on the values in the preceding column we didn't need to look at columns
32:01
to the left of the preceding column this means that we can optimize the space further
32:07
by working with only two columns at a time and this is a trick that i have done
32:12
before if you remember the pre-class videos on dynamic programming we've done that a few times
32:18
before the way we work with only two columns is by initializing only two columns so when
32:25
allocating a 2d table we will allocate the table to be of size n times
32:31
2 instead of n times k plus 2 let's have only 2 columns
32:39
we initialize column 0 to infinity except for you know
32:46
row the row corresponding to the source so this doesn't change but
32:51
when we look at the the main body of the code
32:57
we want to now work with just two columns instead of
33:02
all the columns so we will still have the column number going from 1 to k plus 1 this is the hypothetical column number
33:09
if all the entire table had existed but since we actually have only two columns with let's say indices zero and one
33:17
when we are looking up the entry or or setting the entry in any row or
33:23
column number we will have to take the column number and do a mod 2.
33:28
so this hops has to be replaced by hops mod 2 this hops minus 1 has to be
33:34
replaced by hops minus 1 mod 2. and in the same way in this for loop as
33:40
well this hops has to be replaced by hops mod 2. this hops has to be replaced by hops mod
33:46
2 and this hops minus 1 has to be replaced by ops minus 1 mod 2
33:53
and the final value that we will be returning will be stored at table destination table of destination
34:01
comma k plus 1 mod 2. so wherever we are referencing
34:07
or using column variables we have to make sure that we take that particular column variable and
34:14
do a mod 2 and that will make our column number map to one of those two columns which we
34:21
have allocated so we will keep switching between those two columns as the execution of our
34:27
algorithm proceeds so this is going to bring down our space complexity
34:33
from order n times k to order n since we need only two
34:39
columns the time complexity won't change however
34:44
it's still going to be order m times k
34:50
this dynamic programming based algorithm that we designed to solve the
34:56
given problem this is the single source shortest path problem although we were
35:01
we also had a constraint here on the number of hops we could use along the shortest path but this algorithm is
35:06
called bellman ford algorithm and again it's not a coincidence that
35:12
the name of bellman is associated with this algorithm since he was the one
35:18
who coined the name dynamic programming and he was working on building lots of dynamic
35:23
programming algorithms so the bellman ford algorithm
35:28
like dijkstra's algorithm calculates the shortest path distances from a given
35:34
source to all the other vertices in the graph
35:40
but while dijkstra's algorithm cannot look at the constraints
35:47
of the form that we were given that we can't use this more than a certain number of hops bellman ford algorithm
35:53
can handle that type of constraint now if we had let
35:59
bellman ford execute for more and more and more number of
36:05
columns how high could we go so how high could we make the number of
36:11
hops we know that the number of hops given to us were restricted to k plus 1 in this
36:16
problem but we also know that this value can go
36:23
at most to n minus 1. right once we cross n minus 1 hops
36:30
the table entries are not going to change because any shortest path in the graph from a
36:36
given source to a given destination can use at most n minus 1 edges we saw earlier that if the edge weights are all
36:42
positive there can't be any cycles in the graph
36:48
so because there can't be any cycles we can't go beyond n minus 1 hops even if we did
36:55
we would not be doing any useful work because the values that we would get in those column numbers beyond n minus 1
37:02
would be no different from the values in the preceding columns and so
37:09
the maximum that we can allow bellman ford to execute
37:14
is up to n minus 1 hops or n minus 1 iterations of this
37:19
algorithm after which we will have the absolute shortest path distance values written
37:25
down in the rightmost column so we could use bellman ford algorithm to compute the shortest path distance
37:32
values from a given source to all destinations regardless of the number of hops we just have to make sure that it
37:38
executes for at least n minus 1 times
37:43
once it executes n minus 1 times the shortest path values would be written in the rightmost column now between bellman
37:50
ford and dijkstra which one would you prefer if you only had to compute the shortest path distance values well if
37:56
you remember from an earlier video dijkstra's algorithm on a sparse graph
38:02
took time order n log n how long will bellman fort take if we
38:08
were to keep executing it until the absolute shortest path distance values
38:15
were written down in the column numbers well if we were to continue bellman ford
38:20
for order n iterations in each iteration
38:26
we took time order m plus n so we actually had this complexity of order m times k
38:33
what we are saying here is that k is going to be order n in the worst case and so bellman ford
38:40
will end up being order m times k or order m times n
38:46
this is the complexity of bellman fold in the worst case
38:51
and in a sparse graph this would be order n square because m would be order n so clearly order m times n
38:59
is worse than the complexity we had for dijkstra even in on a dense graph the complexity of dijkstra was order m log n
39:07
so order m n is clearly worse than order m log n and on a sparse graph clearly order n square
39:12
is worse than order n log n so bellman ford is in general more expensive than running dijkstra's
39:19
algorithm which is why we would prefer dijkstra's algorithm if we only had to compute the shortest path distance
39:24
values independent of the number of hops but because we had that constraint here that we can't use more than a certain number
39:31
of edges along the shortest path we saw that bellman ford came in handy whereas
39:37
dijkstra would not be looking at the number of hops along the way [Music]
40:05
you

##CLASS 3 
Problem Identification | UI & DOM for #FAANG+ Interviews







Transcript


Search in video
0:00
[Music]
0:10
the first part of this lecture is going
0:12
to be problem identification
0:15
and that basically entails what makes a
0:18
ui dom problem
0:20
and almost always there's going to be
0:22
some kind of design even if it's just
0:24
drawn
0:25
and there's going to be a component
0:28
or a component or components and
0:30
generally there's going to be some
0:32
interaction between these components the
0:34
ui and the dom
0:36
so how much css html and javascript do
0:40
these front-end problems entail
0:43
well that totally depends and
0:46
as an interviewee you should or you
0:49
should always be asking the interviewer
0:52
what they expect from this problem
0:54
almost always there's going to be a
0:56
javascript aspect to these problems
0:58
especially with the rise of take-home
1:02
interviews with everything that's
1:04
happening in our world
1:06
javascript is always going to be a part
1:08
of these problems
1:09
and like i said earlier we're going to
1:11
prefer semantic markup because that is
1:14
going to be one of the differences
1:16
between a lower and a higher level
1:17
position in an interview
1:19
and now we get into the meat of this
1:22
which is
1:23
the magic formula
1:24
and it's not magic but it is a formula
1:28
and it is going to be what helps us be
1:30
able to set up all these types of
1:32
problems and be able to attack them
1:35
without having to worry about how we're
1:37
going to set up these
1:38
problems and
1:40
this is it'll be new to a lot of you and
1:42
that's totally okay and we're basically
1:45
just here to learn so first why would we
1:48
give ourselves a temple
1:50
and that's because it's easy to repeat
1:53
it removes the boilerplate
1:56
it reduces errors
1:58
and it helps us focus on the meaningful
2:00
parts of the problem and that's where we
2:02
always want to be spending the majority
2:04
of our time when solving front end
2:06
issues we don't want to worry about
2:08
setting up the problem getting the
2:10
information that we want we want to
2:12
worry more about how does interacting
2:15
with this information change the page
2:17
and how do we render that correctly for
2:20
our end user and that's what we mean by
2:23
focusing on the meaningful part of the
2:25
problem
2:27
so
2:27
what is in this template
2:30
first there's gonna be an initialization
2:33
how do we create components
2:36
there's going to be a state what is the
2:38
state of this component
2:40
and how is it changing over time
2:43
and events how do we respond to the
2:46
events and the components how is it
2:48
changing the state and how we're going
2:50
to render this on the page
2:52
so the first thing that we're going to
2:53
have to learn is defining components
2:56
defining the different things that we're
2:58
going to want to have on the page and
3:00
there are a couple way to define
3:01
components and here are two of them the
3:04
first way is to use a class and all of
3:07
us know what classes are
3:10
and basically you can use a button class
3:12
drop down to define your component the
3:14
other way to do this is to use
3:17
a data component
3:19
and we're going to basically be using
3:20
data components throughout this lecture
3:23
because it is semantically more clear to
3:25
what we're using
3:27
and we don't really want to use classes
3:29
where we don't need to use classes
3:31
purely because of worrying about
3:33
overriding css in the future
3:36
so we're going to focus on using the
3:38
data component way of defining our
3:40
components but there are multiple ways
3:42
of doing it and you should use whatever
3:44
is most comfortable
3:47
front end development
3:48
is not strict in the way that we must do
3:51
it there are many different ways to
3:54
get the same results in front-end
3:56
development and no answer is really more
3:59
right or more wrong than another answer
4:03
and that's why having a template makes
4:05
it easier to solve these problems
4:07
because it gets you out of the headache
4:08
of having to figure out
4:10
which way and what do i want to do in
4:14
this type of solution rather it allows
4:16
us to focus on a way of solution and to
4:20
attack the problem
4:21
in that form
4:23
so after we create our components
4:26
we are going to need a way of accessing
4:28
these components and this is the first
4:30
time we're going to see cory selector
4:33
all
4:34
query selector all is going to be a way
4:36
for us to select different components in
4:39
our html
4:41
and be able to
4:44
and be able to see what's inside our
4:45
data component
4:47
so
4:50
document.queryselectoral is going to
4:51
allow us to access this component as a
4:54
node
4:54
and it's going to allow us to access all
4:57
the different attributes that are inside
4:59
this node and that is one of the reasons
5:02
why we really like to use query selector
5:04
all in order to instantiate
5:06
our
5:08
our entire front end so now we get into
5:11
the meat which is what it how are we
5:14
actually going to instantiate this and
5:16
here we're going to show three different
5:18
ways of instantiation
5:19
but we are going to focus on class
5:22
substantiation up here and throughout
5:25
this lecture i am going to focus on
5:27
class instantiation
5:29
but for
5:30
this first
5:32
breakdown i am going to show the three
5:34
different ways of instantiation just so
5:36
that we get a little bit of
5:39
understanding of different ways that you
5:40
can solve this problem
5:42
so in class based instantiation it would
5:44
look something like this where we have a
5:46
constant component and that's going to
5:49
equal a new drop down node which is
5:51
going to go and reference into a class
5:54
then there's function based
5:56
instantiation which looks very normal
5:58
where we just have a drop down function
6:01
that pulls in the node
6:03
and then there's a factory variant which
6:05
is utilizes a dropdown.create
6:08
and that is basically a dropdown class
6:11
with a create node
6:14
of course
6:15
so
6:16
this question pertains to this slide
6:18
right here and the question is can you
6:20
comment on using data component equals
6:22
drop down verse roll equals list box or
6:25
the equivalent aria attributes
6:28
you can absolutely use any of those
6:30
the
6:31
like i said front end development is
6:33
about using what's comfortable to you
6:35
and
6:37
if you're very comfortable using roles
6:39
or using aria equivalents then those are
6:42
totally viable and allow you to access
6:44
the same nodes and utilize the same type
6:47
of data sets that i'm going to be using
6:50
using data component is nice for people
6:53
who aren't already used to using roles
6:56
or using aria attributes
6:58
and
6:59
that's because it's already utilized by
7:02
es6
7:03
and
7:04
everything inside of the html including
7:07
like data set is going to
7:10
make a little bit more sense and our
7:13
typing actually is going to be very
7:14
consistent so we're going to use things
7:16
like data dash component data dash
7:19
templates data dash or data set which is
7:22
why we're going to continue using data
7:24
dash component however using things like
7:26
roles and aria equivalents are totally
7:29
fine
7:30
thank you for that question
7:31
all right so first we'll look at so we
7:34
just looked at three different types of
7:35
instantiations so this is how you can
7:37
instantiate
7:39
this is how you can actually get your
7:42
query selector from your
7:44
from your dom
7:46
and this is how we're actually going to
7:47
plan our html
7:51
[Music]
8:06
you

##RECENT CLASSES # 1
https://www.youtube.com/watch?v=nAeaOZwIyyc




Transcript


Search in video
0:00
What is llmops? Basically, it's machine learning operations with regards to
0:06
large language models. I want to serve thousand but could be millions and hundreds of millions of requests of
0:12
users. I want to make sure the system is 24/7 available and there's no downtime
0:18
and just spending money on infrastructure and on tokens and all
0:23
that unnecessarily. I want to make sure I'm doing this in a costoptimized fashion. Right? We're going to be
0:29
looking at a production grade API. We're going to be building an automated deployment pipeline. Also, we want to
0:36
make sure that the infrastructure we use to deploy to production is scalable and
0:41
we want to make sure we have tools that allow us to monitor. Little introduction about myself. My name is Alias Serun.
0:48
I'm currently based in LA. I work at Amazon. I'm a software engineer. I took on different roles like as a software
0:55
engineer working with different technologies as a backend developer IAS developer as well as currently more of
1:02
an MLE MLOps engineer in the Amazon fashion org working in the search
1:07
ranking team. What is LLMOPS? Basically it's machine learning operations with
1:14
regards to large language models. This is lops which is this set of practices
1:19
that allow us to productionize machine learning models in this case. The whole cycle from taking an experiment and
1:27
going to production. An experiment where the goal of that practice was just to solve a business problem using machine
1:34
learning and we're going with that to production where we now we want to serve millions of users in real time. We want
1:42
to make sure the system is always available, no downtime and all that. So what allows us to do this is those set
1:49
of practices that come with tools and of course some of them are inherited from DevOps that allow us to do this in
1:56
production. If you are liking this video then you love what I have to say next. You can spend months learning agentic [music] AI
2:03
or cut that time into half with a structured program by interview kickstart where you actually build and
2:09
ship to AI agents into production with the mentor from fang guiding you through
2:15
every step. It's called agentic AI career boost program. And if you think it's just another generic course,
2:22
spoiler alert, it's not. It's customizable depending on your domain. engineers, you have a Python based AI
2:28
engineering path. If you're a PM or a manager, you take no code, low code use case path. And both paths could be
2:36
paired with Fang interview prep for AIdriven roles. But the best way to learn more about this program is through
2:42
the free webinar. Here you can get all the details about the program, ask your questions, and see if it's the right fit
2:48
for you. And if it is, you'll be joining a company of 25,000 professionals who have already leveled up through
2:54
interview kickstart. many now at Fang, Anthropic, Open AI already crossing 300K
3:00
plus and here is what some of them say.
3:06
[snorts] I'll put the link to the free webinar in the description as well as pin it in the
3:12
comments so you don't miss it. See you all there. Okay. So, we have two kind of worlds in
3:18
this case where we have the research and development which has its own practices, its own tools and production. Some to
3:25
mention here in development uh when we're trying to solve the business problem using machine learning we would
3:31
be using tools like Jupyter notebooks. Jupyter notebooks they're kind of an a
3:37
tool and IDE that allows you to run code and to share that code and to install
3:43
everything you want in term of like dependencies and access certain hardware
3:48
as well. Like if you want to train a model and you need GPUs, you can connect that Jupyter notebook to a GPU. A famous
3:55
example of a tool like that is Google Collab notebooks that allows you to do this. And for anyone starting in ML, I
4:02
highly recommend it. It's part of Google Drive. You can create notebooks and you
4:08
can access actually strong hardware for free within limitations. When we're also when we're working on the experiment, we
4:15
don't care about scale, right? because all I care about right now is solving that business problem using machine
4:22
learning approach. So as long as my experiment works, it's solving that
4:27
problem, that part is is achieved, right? And all sort of testing that I'm doing at that point is just manual
4:33
testing. I have my test data set. I have a set of scenarios that I want to test with. I do some manual validation. All
4:41
that is done manually. Now in contrast to production where I want to serve
4:46
thousand but could be millions and hundreds of millions of requests of users. I want to make sure the system is
4:53
247 available and there's no downtime. And also I want to make sure that I'm
4:58
not just doing this and just spending money on infrastructure and on tokens
5:04
and all that unnecessarily. I want to make sure I'm doing this in a costoptimized fashion. Right.
5:11
[clears throat] And lastly, I want to make sure that just any piece of software that I deploy, I want to make sure that this software is operating at
5:20
an optimal capacity and that there's nothing wrong going on behind my back.
5:25
So, how do I do this with monitoring? All right. So, what we're going to be building today or looking at the demo,
5:31
we're going to be looking at a production grade API. So we want to serve we want to deploy a machine
5:38
learning model an LM in real time for real-time inference to generate predictions in real time. An example of
5:44
that would be, you know, you're talking to Chad GPT and you're sending requests in real time and uh you're getting the
5:50
response in real time, right? Also for that, we're going to be building an automated deployment pipeline where
5:56
whenever I push code, I want this code to build to be tested to deploy to
6:04
production all in one pipeline, all in one run. I don't want to be interfering
6:09
with that process. Also, we want to make sure that the infrastructure we use to deploy to production is scalable. It's
6:17
something that will allow us to serve a large amount of users. So, we want it to autoscale and we want to make sure we
6:24
have tools that allow us to monitor our software. Okay, we'll we'll we'll dive deeper into the different tools and all
6:30
that. All right, so let's talk about some concepts here and tools. So, one of
6:36
the main tools that we're going to be relying on where we're talking about MLOps or LLM ops is Docker. Docker is is
6:43
an amazing tool that allows you to package your application with all its
6:48
dependencies in one unit. Now, why is this important? Well, I'm sure you've
6:53
run into the issue where you're working on a project, you pass this project to a colleague, they try to run it on their
6:59
machine, and they run into the issue where it's not running. Why is it not running? they're missing some dependencies. The version of this module
7:07
on their laptop is not the same one that you have and or the underlying one like
7:13
downstream in the dependency graph is not the same one. So you're running into this issue where it's working for you,
7:19
it's not working for them. So what Docker allows you to do is you package
7:24
the application alongside the dependencies and everything that it needs to work so that when you pass it
7:31
along to someone to work on or to run it, it will run regardless of their
7:36
machine or operating system. So want to take a look at that. Okay. Now, Docker,
7:41
as I said, is this amazing tool. It will allow you to run your application with all its dependencies. And when you run
7:49
it, you create basically a running container. When you build your application with all its dependencies,
7:56
we're talking about an image. So the image is that blueprint that you package everything with. That's like the recipe.
8:03
And then when you run it, you have a running container. Think about it like in object-oriented programming where you
8:08
have a class definition and then when you instantiate it, you have an object. So when you have the image, you
8:14
instantiate it or you run it, you have a container. Now in order to create the image, I need to specify my recipe. I
8:22
need to write it down somewhere. I need to say I want in this image to include this code, these dependencies, this
8:29
version of Python and so on. So where do I do this? We do this in something
8:35
called docker file. So you define your docker file and it has this recipe.
8:40
Okay. Now when I run my container and think about the container here that it contains basically a running server that
8:48
will serve external traffic and will contain my model. So an external request
8:55
comes in the container will go to the model for inference to generate a
9:00
prediction and will return a response. Now if I have a single container I'm limited to the amount of traffic I can
9:07
serve. I'm limited to the number of threads and processes that I can spin
9:12
up, right? So, in order to scale up and to be able to serve a large amount of
9:18
traffic, we need Kubernetes. Kubernetes is an orchestration framework for Docker
9:24
containers. And it's basically the maestro of this orchestra that would be a series of containers running and
9:32
interacting and serving traffic in a very organized manner. So, Kubernetes
9:38
will allow you to orchestrate all these running containers, but not just orchestrate the running ones. It'll
9:44
handle so much more in term of scaling. It'll handle redundancy. So, you have
9:49
multiple containers running the same code in in case you have a lot of traffic. It'll handle failed containers.
9:56
So, if you have a container that fails, it'll spin up another one. And it's not really a container that it spins up. It
10:02
spins up a pot that has a container. That's the smallest unit in Kubernetes
10:08
and it handles autoscaling, different types of autoscaling. It handles load balancing. So imagine you're just
10:14
creating this entire fleet that is is running and is self-maintained where it
10:22
handles failures and all that and that's what Kubernetes give you gives you. Another concept here in LLM ops is
10:29
CI/CD. So whenever we're going from experiment to production, we want to make sure that any changes that you make
10:36
to any of the code that's involved in the process is fully automated in term
10:42
of testing, building, catching bugs, code quality and all that, but also that
10:48
it propagates through the different stages that serve your different clients. And by clients, I mean internal
10:54
clients and external clients. So let's go through CI and CD. So CI is
11:00
continuous integration where I have the automatic code building and testing upon
11:07
every commit. Okay. And once I build my code, I run unit tests, I catch bugs, I
11:12
make sure code quality is optimal. Now what comes after? We have continuous
11:18
deployment where once that is done, once the integration is done, now I'm moving this code through different stages. For
11:25
example, I move it to the beta stage. What's happening in beta? I'm running I'm running the container and I'm also
11:31
running integration tests against that running service. So beside my unit test
11:37
that I ran during the build I'm running some integration tests to make sure the
11:42
entire end to end flow is is working fine. And then the next stage would be for example a gamma stage. Gamma stage
11:48
would be where I'm running my canaries load testing the the service and then the last one would be production. Now in
11:55
the continuous integration or as part of that integration, we can do much more than what's listed here. We can if we're
12:02
dealing with a Docker image, for example, we can build that image. We can push that image to a registry, we can
12:09
run any sort of security testing on that on that image as well. So we can do a lot more all as part of this CI/CD flow.
12:17
And we create basically in this in this scenario, we create a CI/CD pipeline. We're going to be talking about this
12:23
more later, but this is just to tell you that when we mentioned Kubernetes as
12:29
this amazing orchestrator for my Docker containers, there's there's a small
12:34
requirement here is that I host this Kubernetes cluster. So when we create
12:40
Kubernetes to manage a series of containers, we create a cluster and this
12:45
cluster needs to be hosted somewhere. So I to provide the hardware and host it. But we have other solutions where you
12:51
have managed Kubernetes and this is an example here through Amazon EKS elastic
12:57
Kubernetes services and basically it it hosts that cluster for you and all you
13:03
have to manage is the is the nodes in that cluster and uh everything else in term of setup infrastructure
13:09
observability is is actually handled for you which is which is amazing. But this is similar to pretty much any service
13:16
that's provided by a cloud provider. they abstract all the infra setup and all that annoying setup and you just
13:23
work focus more on your application. So this is kind of like a a luxury if you
13:28
can use that. Now with Kubernetes, we can run into this mess of managing
13:36
configuration files because for every change and every small component of Kubernetes that you want to update, you
13:43
have to create a YAML file concerning that that component that you want to update. So it can get messy especially
13:50
if you have a complex cluster. But luckily we have a solution for this that we will also use today which is Helm.
13:57
Helm is basically a Kubernetes package manager and it it handles all these YAML
14:03
files for you in a very nice way and it does cross reference between them and it allows for upgrade easy upgrade roll
14:10
back and initial install as well. So it's a pretty good tool to use. Now for
14:15
monitoring this is something you want to do to any software that you deploy and you put between the hands of your
14:22
customers, right? It's not necessarily specific to machine learning or machine learning applications. So this is
14:27
something you definitely want to do. Examples of tools to do this, we have Prometheus to emit and collect metrics
14:34
and you have graphana for visualizing these metrics and dashboards and all that. But you're not limited to these.
14:41
If you're also doing work ins for example cloudatch, you can do all that through cloudatch as well. Okay. So when
14:48
we're talking LLM ops in today's section, we're just focusing on the deployment. But MOPS is not just the
14:54
deployment. It's the entire process. It's productionizing the entire process from collecting data, processing the
15:01
data, feature engineering, and deploying the model somehow. In this case, if
15:07
we're deploying in real time, then we're deploying it as a service and it's
15:12
serving traffic in real time. But also you can have a model deployed for batch inference which is running as as part of
15:19
an offline pipeline. So depending on the use case but here we're focusing on this section where we have an already trained
15:27
machine learning model and we just want to serve it put it in the hands of our customers. All right just see a couple
15:34
questions here. Will the same ops tools like MLflow work for LLM ops as well?
15:40
Yes, definitely ML ML flow is is agnostic of what you want to do. So it's
15:45
it allows you for experiment tracking for training and tuning and model
15:51
versioning all all these components that you used for in MLflow you can use for LLMs as well. Are these enterprise grade
15:57
recommendations? Yes, definitely. All right. So focusing more on the demo and the tools that we're going to be using
16:02
for the demo and I'll that will help answer uh one of the questions here as well. So for serving the model as I said
16:10
we want to build this backend service that exposes the model through APIs that
16:16
the customers can call and we will be using a tool called Bento ML. Now, you
16:22
probably heard like because we're doing it in Python, you probably heard of Flask of fast API, but we're choosing to
16:29
do Bento ML. And Bento ML is actually more optimized for serving machine learning models where it has built-in
16:36
model versioning. It has automatic API generation where you don't have to
16:42
explicitly specify the APIs, the status codes, and all that. It's optimized for
16:47
production where you need to consider cases like batching and caching. If you
16:53
are experienced with or if you have some experience with deep learning models, you know, you can run you can specify
16:59
the batch size for example. And what that allows you to do is basically let's say you're trying to predict what's
17:07
what's in that image. So instead of processing one image at a time, you can create a batch of 200 images and you
17:14
you're basically running inference on a single batch that contains 200 images. Now when you're serving this in real
17:21
time for behind a a server or inside the server, you need to implement the logic
17:26
that basically takes in the 200 requests, aggregates them and passes
17:32
them to the model. Now if you were doing this with something other than bento bento ML then you will be doing that.
17:39
However with bentoml it's just a single line of code that handles all that for you. You just tell it the batch size and
17:46
it handles the batching for you. For example, you see how it's optimized for machine learning applications rather
17:52
than just building a server for a generic use case. Also it handles
17:57
containerizations. Remember we spoke about Docker and how I want to package my application inside a docker image and
18:05
run it as a container. Now with Bento ML using Bento ML command line tool I would
18:10
be able to containerize directly. I don't need to handle Docker commands. So it's all done with Bento ML. Okay. And
18:18
it integrates easily with monitoring tools as well. All right. Another thing is that it allows you to declare
18:24
resources that you want to allocate to your service like number of CPUs, memory and all that. This is something that you
18:31
might not be able to do with other frameworks. All right. And also it includes some APIs that are kind of
18:38
baked in already like a health check API where you basically have the
18:43
server/healthz that gives you access to the health of the service. Now having said those, it
18:50
doesn't mean that it's abstracting everything for you. So you don't have granular control. The beauty of it is
18:56
that if you want something easy deployable, thank you Ron. If you have
19:01
if you want something easy deployable with minimum configuration, you can definitely leverage those out of the box
19:07
features and it allows you to spin up a service that's fully running and it's
19:13
handling everything for you. Uh but also it allows you to have this granular control if you want. You can define your
19:18
own health check API or more custom API. So the sky is the limit with this. Yes,
19:24
it includes GPUs. So we have a question. Does the service resources management?
19:29
Yes, it does. For CI/CD, you can use Jenkins as well. Yeah. Something to mention here is that you're not limited
19:35
to whatever I'm going to mention here, but whatever I'm I'm recommending here is industry level tools. It's not just
19:43
for like a quick prototype or anything. So you can use these and you can use definitely others. There's plenty of
19:49
other tools that are that are there and I encourage everyone to experiment and see what works for you for your use
19:55
case. Some tools offer a lot but like the setup is too complex. It's an overkill. So you might want to go a
20:01
different route. Okay. Another thing that's important when we're going from experimentation to production. Well,
20:08
what is it in experimentation? I'm working in notebooks. I'm saving that notebook. I'm giving it to another
20:14
scientist or MLE and they're continuing to work on it or they're rerunning my
20:19
experiment and all that. However, when we're moving to production, we are dealing with we're not dealing with
20:25
notebooks anymore. We're dealing with Python code. I need to implement everything in Python. I'm dealing with
20:31
because I'm using Bentoml. I have Bentoml configuration file which is bentoml YAML. I'm defining my
20:37
dependencies as a list of dependencies in a requirement text file and so many
20:43
more configurations. So I want to make sure all this configuration is versioned
20:48
is is auditable if there's any change and the way to do this most of you have used it I'm sure or have heard of it is
20:56
git. Uh with git I'm able to version all this code and much more and keep track
21:02
of all the changes and roll back to specific changes and take action based
21:07
on different commits based on different changes that I'm pushing. Right? Another thing it's with git you're able to
21:14
create branches which allows you to maintain the main code on a main branch whereas you have a feature branch where
21:21
you're experimenting with a new feature before you merge it back onto a main. So
21:26
when you go production going with git is is a must at this point. All right. And
21:32
what we spoke about we're going to be using docker here to package our application and run it as a container
21:39
with the benefit of dependency isolation and being able to run on any cloud
21:46
provider if we were going uh with cloud providers or even locally on any machine. And again, because we're using
21:52
Kubernetes and we have in this for this demo the luxury to use EKS from AWS,
21:57
we're not going to be hosting our Kubernetes cluster. We're going to be using WS EKS that will host this cluster
22:04
for us. And it'll because Kubernetes will handle the the autoscaling for us,
22:10
self-healing, load balancing, allowing also any changes to have zero downtime
22:16
deployments. All right. Now, we had a question on CI/CD and using Jenkins. As I said, you can. It's not the only one.
22:23
In this demo, we're actually using GitHub actions for automating this entire pipeline. And with GitHub
22:30
actions, what happens is when I push code, I'm able to trigger build and
22:36
building of the image, deploying of the image or pushing the image to container registry and then updating Kubernetes
22:44
cluster with the latest code so that it reflects on the client side so that
22:50
customers can see the latest changes. Okay, so all that is going to be orchestrated with GitHub actions and
22:56
we're going to go through all these steps together. All right, just looking at the questions here in Q&A before we
23:03
move forward, I'll I'll answer you towards the end. So we have a question
23:08
on Bento ML and SageMaker deployment. When you say SageMaker deployment, I'm assuming you're talking about SageMaker
23:15
endpoints to deploy models for real-time inference. You can go either route if you have also the luxury to go with
23:22
SageMaker endpoints. I've used it in the past. It does the job. But also if you want to have a more granular control
23:29
also if you don't have access to all these AWS services you can do ML and
23:34
define your own APIs as well. All right. So continuing on this what are we going to be using for our demo? Well because
23:40
we're using Kubernetes and we have all these YAML files that are used to update
23:45
our cluster. We don't want to deal with all these YAML files. And here's an example of some of them. We have
23:50
deployment YAML which usually contains the pod configuration. You want to mention there what replicas you have. I
23:57
want for example two replicas of my pod and I want a minimum of two pods and a
24:03
maximum of 10. So basically if I'm scaling up I'll go all the way up to 10 replicas. And by replicas here we mean
24:10
we have pods running the same code so that if we have more traffic we're able
24:16
to serve that traffic. Service yaml is the configuration for the networking.
24:21
What type of load balancing I'm I'm going to be using and we have different types of load balancing that could be
24:27
configured for Kubernetes. HPA horizontal scaling basically what type
24:33
of autoscaling you want to define and uh the basically all the configurations
24:38
that go around that like when do you want to autoscale maybe when I hit 70% of my CPU and so on all right now with
24:46
Helm you can define you all you have to define is the values YAML and the
24:53
initial charts so in Helm you have a charts YAML which just general
25:00
information but the the core of your configuration is going to be in the values YAML and the rest is autocreated
25:07
for you. All these deployment service HPA and many more configurations are autocreated for you by Helm which is
25:14
amazing because you don't have to create those. All you have to do is define your values for what needs to go in those
25:20
configurations and the rest is handled for you. All right. And once you have your values and your charts YAML files,
25:28
all you need to do is helm upgrade installed the name of your service and where your charts reside in your repo
25:36
and that will create everything for you and does all the referencing between them and it saves you a lot a lot of
25:42
time. All right. Now monitoring observability. What what are we going to
25:47
be using for this? We're going to be using Prometheus and Graphana. And we should be able to see information about
25:54
our cluster, CPU usage, memory usage, and much more. It depends on the metrics
25:59
that you get to define for your for your running application. In our case, we're going to see CPU, metrics, memory, and
26:07
also the requests coming to our application. All right. So just to
26:12
summarize and revisit the the big picture, we have a local development
26:17
happening locally and then once we create a commit, a git commit and push
26:23
it is going to trigger GitHub actions which will build the docker image push
26:28
it to ECR. ECR and I haven't mentioned that before. ECR is elastic container
26:34
registry. It's a container registry that is provided by Amazon. It's a private
26:40
registry that you need authentication to access it. And we're basically storing our Docker images over there. It's not
26:47
the only one. You can use other registries, but that's the one we're going with today. Okay. And once we
26:54
build and store our image in ECR, we're going to update our Kubernetes cluster
26:59
with the recent changes which will reflect those changes in through our
27:05
Kubernetes service that our user interacts with. All right. And because we have Prometheus and Grafana
27:11
configured, we should be able to see some of the metrics about our cluster and the interaction that the user is
27:17
having with it. Now for the because we're limited on time, I I had to do
27:23
some setup beforehand. So I deployed the EKS cluster already with two nodes. I
27:29
set up Bentoml service and GitHub actions, the Helms charts, and
27:34
everything is set up. Now you have access to all that in my GitHub repo that is already I believe shared with
27:41
you. If not it will be shared with you after the session and I have all the instruction that you need to to
27:47
replicate what I'm doing here. But we're going to go through all that at least in this session. All right. So let's do a
27:54
quick highle overview and we will then go deeper into code setup and all that.
28:02
All right. So this is the GitHub repo and whenever we're pushing something to this repo, what happens is we actually
28:10
trigger GitHub actions. Now, when you see GitHub actions, it's a set of steps
28:17
that are executing sequentially. And we can we're going to go through the code for those, but it's a series of steps
28:23
that execute sequentially that will eventually that will eventually push the
28:28
image to ECR, elastic container registry ins. And we can see in this private repo
28:35
that the image is pushed. And once it's pushed as also part of the GitHub
28:40
actions that are running, it deploys to EKS. So it updates the Kubernetes
28:47
cluster with the latest image. And here we have my cluster running. And you can
28:53
see different information about the cluster here. Let's see. You can access some you can monitor the cluster as well
29:00
through here. cluster health control in monitoring. Okay, so all that is provided by AWS. However, you can also
29:06
still implement your own external monitoring system right through Prometheus and Graphana as we said.
29:14
Let's see. For some reason, the the metrics are now populating in the dashboard here. It's showing no data. It
29:20
was running up until I restarted the cluster today. But what we would see
29:25
here is basically the CPU utilization, memory and the the requests coming to
29:31
the cluster and and much more. And you can define more metrics like for example, you want to monitor the the
29:36
length of the prompts, how many tokens and all that. So in our case, we're using an LLM that is hosted and is
29:44
serving traffic in real time and we want the users to interact with the LLM similar to how people talk to chat GPT
29:50
and write prompts and get responses, right? So you want to monitor tokens and
29:56
other metrics, you can do that here as well and be able to visualize it. Maybe to fix this, I just need to restart the
30:02
cluster or something, but we're going to move past that just for the interest of time. Now just for the quick demo, I
30:09
have this small front end built as well and all the instruction to to run it is
30:14
in the repo read me as well. You can do that. So similar to chatting with
30:20
similar to chatting with like your cloud tell me about yourself generate response
30:27
and it says I'm a fan of Rick and Morty and that's actually a response from the model. So we can see that we're going to
30:32
see the model that we're using. I think it's just a mediumsiz LLM from Microsoft and yeah it it has a sense of humor and
30:40
this is not a funny response but I got a lot of funny responses from it before and for all Rick and Morty fans like
30:46
myself that's that's good to see that the model is a fan as well. Perfect. So
30:51
so this is at a high level what we have we have this fully deployed service that I'm able to interact with and many
30:58
people can interact with because it's scalable and we're not going to run into issues. It can scale automatically
31:04
because we're using Kubernetes and it's deployed on EKS. And if I want to make any changes, I can make changes to my
31:10
code, push it. The GitHub actions will take care of building and deploying these changes to the to the service.
31:18
Now, let's do a deep dive into the different components that we have.
31:23
Perfect. Do I need to make it larger or is this is this good? Is this good enough? That's fine. Perfect. So, we
31:30
have different components. Yes, please. Sure. So, in this repo, you'll see we have different components. The first
31:35
component is we have to build our bentoml service where I'm using bentoml.
31:42
Bento ML to create my service and define my APIs. Now, first thing you want to do
31:49
is to load the model. So, I'm using a pre-trained large language model and I'm loading it and serving it in real time.
31:56
The model we're using is Microsoft dial GPT medium and this model is coming from
32:04
hugging face. I don't know if you're familiar with hugging face but let's hugging face but on hugging face you
32:11
have access to all sort of models and data sets and the community as well to
32:17
to get some answers and all that and they have a lot of courses as well. So feel free to to look into it and use the
32:24
the free tools that you have here. You can even host some code on spaces that's
32:29
provided here. So the model that we're using is Microsoft Dialog GPT medium.
32:35
And if you visit that page on hugging face, you can actually see more information about this model and how it
32:41
was trained. Maybe depends if the model is like open source or no. You can see
32:47
maybe the the scientific paper behind it and all that. And some models depend.
32:54
Let's see. Oh, I can't use it here. But on some models, you can actually they give you like an interface that allows
33:00
you to prompt it and to try it out to see for yourself. And yeah, it's pretty cool. And you can, this is not the only
33:06
model we that you have you can use. You can choose any model from hugging face. If you go under models, you see text to
33:13
speech, text generation, all sort. Okay. Now to use it in in Python all you have
33:20
to do is import the transformers library from hugging face and pass your model
33:27
name to it. Now also hugging face gives you the tokenizer for that model which
33:32
is important to transform the input that you get from the user into input that
33:38
the model can understand. Okay. Now first step as I said all we want to do
33:43
is load this model. Now that the model is loaded, when we say loaded, it's loaded in memory ready to serve traffic.
33:50
Now we define our generate API. And the generate API is this method that will
33:55
take a prompt which is a string from the user similar to what I was passing here.
34:01
And we'll encode this input. And by encoding here we mean because the model
34:07
is not going to understand text directly. The model will first or the
34:12
the application will take text convert it into token ids that actually point to
34:19
the vocabulary in the model's vocabulary and that would be the input for the
34:25
model and then all you need to do is call model generate. You set different
34:31
configurations for that specific model like temperature top P and other configs
34:37
and then you get your output. Now when you get your output, in order for us
34:42
humans to read it, we need to decode it. Convert it from token ids which which is
34:48
basically like an array of numbers to actual words. Okay. And once we have the
34:54
full text, we can display it to the user. All right. So this is my service. All I have is a method to load the model
35:02
and a method to generate a response from a prompt. Okay. Now you're not limited
35:08
only to those methods or these APIs. For example, here I have implemented a
35:13
health check method. Even though bentoml provides you with a built-in health, it's usually at health Z, but in my
35:21
case, I wanted maybe to do some more. I wanted to customize more and I wanted to actually dump more information in the
35:28
health check. So you can do that as well. Okay. Now this would be the core
35:33
logic for my inference for inference which is generating predictions. Okay, the next thing we have is All right, so
35:41
I have my code. Now I want to remember we said we want to use Docker and we want to containerize and and all that
35:48
but we're not going to do it directly. We're not going to write our recipe file which is a Docker file. We're going to
35:54
do it through Bento. Remember Bento ML provides you with the ability to
35:59
containerize your application and serve it. And that's what we're going to be using. We're going to be using a bentoml
36:06
sorry a bento file yl which contains some instructions on how to package this
36:12
docker image from bentoml. So we're telling it include the source file kubernetes directory which contains
36:19
basically my yaml files for helm and we're going to talk about that in a bit and my requirements txt. So what's in
36:27
requirements txt? It's a list of my dependencies, my Python dependencies
36:32
that I need to run this application. Okay, I have bentoml, I have transformers, torch, pytorch, tokenizer
36:40
and others. Okay, so I also have the base version of Python or the base image
36:47
of Python that I want to be using for this application. So this is what I include in my Bento file yaml and this
36:54
is required for bentoml to containerize my image. Now the next thing is values
37:00
and chart. So remember we're we're not just containerizing this image. We want to deploy it to be orchestrated for
37:07
Kubernetes, right? So to be orchestrated by Kubernetes and to do that, we need to
37:14
to create different YAML configurations for different components of our Kubernetes cluster. However, we decided
37:21
to go the lazy route, smarter route, which is using Helm. Helm will allow us
37:26
to manage these files and create some by itself. All we have to do is create the charts chart YAML file that just
37:34
contains general information and the important one is values YAML. And in
37:39
this file we have the number of replicas for redundancy and we also have the
37:44
image that we're reading from. We also have the service which is the load
37:50
balancing technique that we're going with. resource allocation maximum minimum per per node and also
37:57
autoscaling configuration. So I want to enable autoscaling minimum replicas two
38:02
maximum four when I hit 70% CPU utilization. Okay, I'll get to your
38:08
questions after this. And now I have my core code set up. I have my Bento file
38:16
YAML which allows me to containerize. I have my charts and chart and values YAML
38:22
for Helm to manage all the components needed for Kubernetes. Now I need to
38:28
automate that process where whenever I push code it goes all the way and deploys etc. Now to do this you need to
38:36
create a deploy YAML file which lives under.github/workflows
38:42
and this contains basically your pipeline definition for your CI/CD
38:47
pipeline. Let me just collapse these so we can go through them. So the first step we have here, so some of them are
38:53
not like specific to this project. They're just general general cleaning or
38:58
maintenance steps. So the first one I'm freeing some disk space. Set up Python. Okay, so just removing some files to
39:06
save some disk space. Setting up Python to 3.9. Installing dependencies which
39:12
are my requirement.txt dependencies. this list of dependencies
39:17
here. So basically this is what's happening in an automated fashion to be able to containerize this image and
39:24
deploy it to ECR. So installing my dependencies and now because I'm using
39:30
ECR which is the container registry for pushing my Docker image. It's a private
39:36
registry. It's a private Docker registry and in order to access it I need credentials. I need to log in. So to do
39:43
this you need to also specify a login step in your pipeline. And if you notice
39:49
here something very interesting that I need to log in. So I need my AWS access
39:55
key and secret key but I should not be in the region but I should not be hard
40:01
coding them in my code. Otherwise if you go to my GitHub repo right now you would be able to see those credentials but
40:08
you're not able to see that. Why is that? Well, because GitHub actions actually allows you to store these
40:15
configurations in GitHub actions safely, secure behind your GitHub authentication
40:22
and two-factor authentication and you just reference them here as such. Okay, so you keep your secrets in a safe place
40:28
and you reference them here and they're the responsibility of GitHub at that point to keep them safe for you. Okay,
40:35
so this step all we did is we configured WS credentials and then we log in to ECR
40:42
to make sure that we have access to that repo that we want to push an image to.
40:47
The following step is to build Bentoml container and push it to ECR and for
40:54
that we're using a bentoml command. As you can see here there's the hovering things. So you see here we're using
41:00
bentoml build. Now, if you're not using bentoml and you just want to containerize the application, you would
41:07
specify your docker file and then you would do docker build instead. Okay, but
41:13
the benefit here is that bentoml allows you to do that on its own by doing bento
41:19
build and we're doing we're tagging and doing docker push. So
41:24
these are standard docker commands to push the the arange. All right. So next
41:30
thing we do is just also some maintenance clean up docker space otherwise we run into some issues with
41:36
GitHub actions and the next thing we make sure we install a couple of tools here which is cubectl and helm. Helm we
41:44
spoke about it is to manage my yl files and configurations for kubernetes. But
41:49
what is cubectl? Cubectl is also a tool used to push commands and push updates
41:57
to to the Kubernetes cluster. And in this case, I needed to be installed.
42:02
Why? Because this is not running on my machine. When I when I showed you all these steps here, these are not running
42:08
locally. These are actually running on some GitHub server that is executing
42:14
these commands on my behalf. So I want to make sure that that GitHub server has
42:20
cubectl and helm. So I have to install them. So we have to add a step to install them as you see here. Next step
42:26
is again just it's not really needed but it's it's good to have it's verifying that you have access to that EKS cluster
42:34
that you intend to access and update before you go ahead and update. And the last step is to deploy to EKS the latest
42:41
image or the latest configurations as well. Okay. And these steps that you see here are the exact same steps that if I
42:49
go to my GitHub actions are getting executed here. And the beauty here is that when you make a change and push it
42:57
to to GitHub, you would be able to see this executing in real time. So let's
43:02
actually do that. I'm going to make just a small change. So let's just add a comment here. For example, load model
43:09
ant tokenizer from hugging face. Perfect. Let's expand this to see what we're doing. So get status. I have this
43:17
file that has changed. So I do get this file get commit and update
43:25
documentation. Okay. So I created the commit and all I
43:30
need to do now is to push it to trigger that CI/CD pipeline. All right. So now
43:36
it's pushed. If we go to actions, we see the latest commit showing here. It's
43:41
showing that it's deploying and it's actually going step by step showing you the output of every single step that's
43:48
happening here. Okay. So, we're not going to wait for it to execute. It's just going to take 10 15 minutes. But
43:54
other than that, this is the the the process to do this. And once that is created and updated, we're going to see
44:01
a new image being pushed to my ECR repo. A new line basically being added here
44:07
containing that image. And then we're going to see an update in our cluster.
44:13
And if we made some code change that actually changes the behavior, we would be able to see that change reflecting.
44:19
Okay, we're not going to worry about this part for now. But so yeah, we're pretty much done with the demo. It's
44:25
just like some recommendations and best practices here. So for El MOPS, what are some good things to do? What are the
44:32
best practices? Well, you want to make sure that you version your models, your and your model deployments. So that you
44:39
have a traceable or lineage of what is the code that I used with which model
44:45
version with which docker image that led to which deployment that is that users
44:52
are currently using. Right? And why do you need this? Because you want to make sure that when users are interacting
44:58
with a specific deployment, you can actually know what model contributed to
45:03
that, which code, which inference code, etc. Right? because if there's any issue, you want to connect all that.
45:10
Another thing that you want to be able to do is AB testing before full roll out. So for example, let's say we
45:17
decided to use a new version of this model instead of the current one and we
45:22
want to roll it out. The last thing I want to do is to just change that model and push a new deployment. I want to do
45:29
some AB testing where I allocate a small amount of traffic for the new model. I
45:34
collect the metrics and the rest of the traffic to the old model. Right? Another thing I want to do is monitoring the
45:41
quality of the infrastructure as well as the service that's that's running. So
45:47
through monitoring tools and the integrations that we saw today. Uh another thing is this is more of an
45:54
optimization where if you're dealing with prompts for example for an LLM like
45:59
in our case sometimes you might have common queries. So of course you're
46:04
going to be monitoring the incoming queries from the users and maybe as an optimization you can if there's no
46:11
personalization involved in your model or in your service where like you're including other information in the
46:17
response that is making the response more personalized. If it's just general like what we have maybe you want to
46:23
common query so you're not sending all the queries to the model right so you want to cache the response and uh return
46:30
that and you want to introduce batching as well and that's that's what I said earlier with bentoml you're able to
46:37
batch uh requests to the model rather than having one request hit the model
46:42
and another request hit the model now you can batch and with Bento ML you should be able to do that security
46:49
pretty common sense You don't want to hardcode your secrets like I showed you here. Remember when we said we want to
46:55
log to ECR to access yeah configure credentials. You want to use these
47:01
placeholders and let me show you actually where it references in GitHub. We go to deploy. If you go to settings
47:08
in your in your repo, if you go to settings and you can see secrets and variables and [clears throat] you go to
47:14
actions, you can actually see all these secrets that I configured in my repo and
47:20
these are the same ones that are being referenced here. So this way I don't have to hardcode any secrets in my code.
47:27
Also because in this case we're implementing an API you want to make sure you have you know all these rate
47:34
limiting for the API so that it you don't get people abusing the the service
47:40
and maybe you want to introduce authentication also depending on the service uh that you're giving your
47:46
customers and also you want to introd
47:53
to trace any issues and be able to look into them later. All right. Common mistakes that happen with working with
48:00
LLMs or real-time deployments like what we saw today could be not setting
48:05
resource limits. So you're not working with tools like Bento ML and Kubernetes
48:10
allow you to actually limit the resources used by your service so that
48:17
you can manage it in a better way. So you want to make sure you use that. Another issue, another mistake is not
48:23
setting health checks or using the health check APIs. Another thing is you
48:29
know being happy that the service is currently working and not having monitoring for prod service. When this
48:36
happens at the beginning you might be like my my service is working. I don't need to set up all this monitoring stuff. But then whenever an issue
48:43
happens you you're just blind. You can't see where the issue happened and what caused it and you'll regret it. Another
48:50
thing is manual deployments. Something I didn't mention earlier is that all the steps that are happening here, all these
48:58
steps, you can run them manually. I can and I included all this these instructions for manual testing locally,
49:05
manual deployment and everything locally. I included them in the readme file in the repo. You can actually do
49:11
that. So if you don't want to go through the CI/CD, however, with CI/CD, you make
49:17
sure that you don't make mistakes. It's fully automated and you avoid any issues due to human error. All right. And
49:23
because we're using Kubernetes and the whole point of Kubernetes is to orchestrate a scalable service for you,
49:30
you want to use replicas. You want to make sure you have redundancy so that if one container, one pod fails due to some
49:37
network issue, due to some whatever issue, some unhandled exceptions in your code or whatever might be the the the
49:44
root cause. you want to make sure that you have other replicas running to serve clients, right? So these are common
49:51
mistakes that are usually made and yeah, so this is what we looked at. This is
49:57
what we built and once the code is shared with you and the slides, feel free to go through it again and try to
50:04
recreate it. And yeah, I'm looking forward to hearing some feedback from you as well on that. Thank you so much.
50:14
[music]

##RECENT CLASSES 2 - AGENTIC SYSTEM
##IMPORTANT

How to Build Multi-Agent AI Systems | Architectures, Communication & Real-World Pitfalls

https://www.youtube.com/watch?v=7pfzq6IaiKo

0:00
There's one comment that I really like which is saying multi- aents are like microservices [music] on architecture perspective that is true
0:07
single agent is monolithic. Yes, rather than having a single huge piece of work
0:12
whether it's an agent or software it's harder to manage it's harder to maintain we will break it down into smaller
0:18
pieces. If you add more logic to that [music] agent, it's either going to make the pump very complex or the number of
0:25
tools very large and unless URL has like the [music] capacity to scale with this,
0:30
your quality will drop. Do parallel agent running at the same time, you'll
0:36
get faster responses, but we are paying something to get that. Also, if you can actually run multiple agents, you can
0:43
[music] basically reduce hallucinations basically by getting consensus from different agents. All right, cool. So,
0:49
let's get started with multi- aent. Before that, I want to go quickly about before an intro about myself. I work as
0:56
an ML engineer at Google. I spend most of my time working on genai applied
1:01
problems. This means that I do not work on training the foundation model. Instead, I work on using existing
1:10
foundation models and using them to solve business or engineering problems.
1:15
I spend most of my time on the applied side and I try to talk about applied aspects rather than the theoretical
1:21
parts. Before that I spent some time in academia doing regular academia things
1:26
which are teaching and research on different ML topics. So that's a brief
1:31
intro about myself. As I mentioned earlier, I will talk about multi- aent systems and basically so far you have
1:39
learned about the foundations like things like retrieval agent generation and what it what it does it mean and
1:44
what are the main components and then you learned a little bit about how to build a single agent and you saw some
1:51
coding demos probably to to build that and you learned also about MCP. You can
1:56
spend months learning agentic AI or cut that time into half with a structured
2:02
program by interview kickstart where you actually build and ship two AI agents
2:07
into production with a mentor from fang guiding you through every step. It's
2:12
called agentic AI career boost program. And if you think it's just an other generic course, spoiler alert, it's not.
2:19
It's customizable depending on your domain. engineers, you have a Python based AI engineering path. If you're a
2:26
PM or a manager, you take no code, low code use case path. And both paths could
2:32
be paired with Fang interview prep for AIdriven roles. But the best way to learn more about this program is through
2:38
the free webinar. Here you can get all the details about the program, ask your questions, and see if it's the right fit
2:44
for you. And if it is, you'll be joining a company of 25,000 professionals who have already leveled up through
2:50
interview kickstart. Many now at Fang, Anthropic, Open AI already crossing 300K
2:56
plus. And here is what some of them say.
3:02
[snorts] I'll put the link to the free webinar in the description as well as pin it in the
3:08
comments. You don't miss it. See you all there. Now we will focus on like the scenario where we have multiple agents
3:21
that basically mixes different capabilities and helps you achieve more sophisticated goals. So we have like a
3:28
list of subtopics. We will start first by talking about why in the first place we need to have multi- aents and then
3:35
once we have like enough reasons to have multiple agents, we'll talk about different architectures for how these
3:41
agents can communicate with each other. Uh we will dig deeper a little bit into the communication modes and then once we
3:47
have multiple moving parts, this can cause different security and safety risks. So we'll talk a little bit about
3:54
that and then briefly we will talk about operations. Operations. So basically things like evaluation,
4:01
scaling, different frameworks that we can use and then we will see a coding demo and then we will wrap up the
4:06
discussion with about some antiattern designs that we should avoid. So the
4:11
last few topics we'll talk about these last four ones very briefly. We're not going to spend too much time with them. I'm going to spend most of the time with
4:17
these topics and the coding demo. So before we actually get started with multi- aents, we need to know why we
4:25
even think about multi- aents. So if you can if people can share with me on the chat some thoughts on why in the first
4:32
place we will need multi- aent system instead of a single agent. So rather than having a single agent with a single
4:39
set of tools why would we need to have multiple of these any thoughts on the
4:45
chat if you can share your thoughts that would be great. Specialized agents for different tasks. Yes scalability.
4:51
Scalability is an interesting thing. We'll talk a little bit about scalability. Each agent will be simpler. Yes. Each agent will do a simple or more
4:58
specific thing more complex problems. Yes, this is a great point. There's one comment that I really like which is
5:04
saying multi- aents are like microservices. On architecture perspective, that is true and I will
5:10
keep this comment because it will make explaining the architecture for me way easier. So yeah, thanks for for that
5:15
comment. Complex tasks, balance checks, we will talk about that as well. Single
5:21
agent is monolithic. Yes. So this is very similar to the idea of having architecture like microservices right
5:27
rather than having a single huge piece of work whether it's an agent or software it's harder to manage it's
5:34
harder to maintain we will break it down into smaller pieces a single agent can handle one task at a time quick response
5:41
quick responses depending on how we are implementing them increase the number of applications complex workflows each
5:47
usual task okay awesome so I think people almost all people agree that task
5:53
complexity is one of the reasons why we need to to have multi- agent. So basically the whole idea is single agent
6:00
can perform relatively simpler tasks. If you add more logic to that agent, it's
6:07
either going to make the prompt very complex or the number of tools very large and unless URL has like the
6:14
capacity to scale with this, you will you'll run out of like quality your quality will drop. So basically, rather
6:20
than asking your agent to do everything, you're going to break down the problem into smaller sub problems small for
6:26
multiple agents to work together. If you're still using old Fang prep guides, you're already losing ground. The rules
6:32
have changed, and this free AI playbook shows you what's really being tested in modern fang interviews. Insider
6:39
strategies, hidden evaluation matrix, a 90-day mastery plan, the exact playbook
6:44
successful candidates are using right now. Everyone's grabbing it. Don't be the one left behind. Link in
6:49
description. If you actually run your agents, these different agents in parallel, you can
6:55
actually achieve more within the same time budget. If we actually run agents in parallel, it seems that we're going
7:02
to get the same response relatively quicker. But at what expense? If we run agents in parallel, we will get faster
7:09
responses. But what are we paying for this instead? If we do parallel agent running at the same time, we'll get
7:16
faster responses, but we are paying something to get that a default thing. More resources, right? Or more TPUs or
7:23
more GPUs and more money. So, we're paying money for that. So, we should be making all of these decisions
7:28
consciously. We should be aware of the pros and cons. And speaking of like the skills that you need to have, what I
7:35
have seen is basically people working on ML related jobs, there are either people on the quality side, people who train
7:41
the foundation models and they do they know how to train them and how to deploy them and people actually use them in the
7:47
applications and these are one of the decisions that people who use these models in applications have to make like
7:52
do I want to trade money for latency or latency for money. This is where it's
7:58
more important even in multi- aent system. We'll talk about different architectures later. So basically one of the things that you you should think
8:04
about is task complexity whether you will run these in parallel or not and what is the implications and the latency
8:09
and the cost. Also if you can actually run multiple agents you can basically
8:15
reduce how these similar to what some folks have said basically by getting consensus from different agents. If you
8:22
get like if you run multiple agents and they give you the same response then it's very less likely that this is
8:27
helenation. uh if there's a disagreement between them then this is basically a a
8:32
signal that there's there should be a signal that there a higher chance to to see howated results. Not all problems
8:40
lend themselves into a multi- aent. Some problems actually you have single agent
8:45
should be enough. So basically if your task is simple or if your task is sequential in nature or it doesn't
8:51
require extra processing you should use single agent. So it's not always the answer is not always multi- aents
8:58
depending on the problem itself depending on the cost analysis or return on investment versus complexity you
9:06
should make that decision and it's on the ML engineer who is building the application to make that assessment as
9:11
we mentioned earlier the cost the number of how much TPUs or how much GPUs do you
9:17
use is one deciding factor as we will see you actually require more LM calls
9:22
for a multi- aent system which is kind of intuitive. So you should keep this in mind also whether you you would expect
9:29
performance gain or not. Basically if your problem is simple you you will not see a performance gain by moving into a
9:35
multi- aent system. So basically these are one of the decisions on the on the person who is designing the system
9:42
should take into account. So some of the problems for for example to give you some examples on like cases where multi-
9:48
aent system can make things even worse is if you are overengineering your
9:53
simple problem you have a trivia problem you made an overengineered it and decided to use a multi- aent system then
9:59
actually this is a an antiattern that you should try to avoid. Sometimes your agents will either keep exchanging
10:06
unnecessary messages which is basically burning money for you or they can keep running in like infinite number of
10:12
retries or they can even complicate things. I think there was some recent research by Apple I think they were just
10:18
showing that if you show some of the models simple math problems they tend to
10:24
over complicate things. So again, if you have multi- Asian system that is overfitting or overengineering a
10:30
problem, you are more likely to run into this plan drift. So I'm just including this slide to tell you that multi- Asian
10:37
system is cool. It solves complex problems, but you should not be using it for every single application that you
10:42
have. So now we have an idea about when to use multi- aent system which is basically for complex problems. If you
10:49
understand the return return on investment versus cost or versus complexity. So let's say that we decided
10:56
to use multi- aent system. We did this homework and we decided to use multi- aent system. The question is how can we
11:03
architect our multi- aent system? How do we structure our multi- aent system? Right? So we know that a single agent or
11:11
a single agent system will mostly consist of an LM that has access to
11:16
multiple tools, right? one or more tools and plus that you will have probably a memory because your LM is stateless so
11:24
you will add a memory to to your but what about multi- aent tested what are the different ways that we can architect
11:31
these agents so that they can communicate with each other what are people thoughts let's say that we have
11:37
three agents or 10 agents whatever the number of agents how can we have what architectures can we use to to to
11:44
organize the communication between these agents let's see what people think. Shared memory. Okay. Parent agent plus
11:50
child agent. Yes. Like some form of hierarchal architecture. Yes. MCP protocol is actually not for agents. MCP
11:58
protocol is more for agents that communicate with tools. So MCP actually will fit some somewhere here. More or
12:05
less. There are different protocols for agents similar to MCP. They're called agent to agent. They are developed by
12:11
Google's ADK. So yes, A2A is developed by Google ADK. But what are the
12:16
different architectures regardless of the implementation? A coordinator agent or supervisor agent. Yes. A message
12:22
passing. Okay. Orchestrator. Okay. So, so basically people mentioned different
12:27
architectures. One of them is the idea of having supervisor agent, right? This is basically the agent that let's say
12:34
that our problem has three agents, right? Research agent, writer agent and
12:39
spelling checker agent. whatever number of agents every agent will perform their own task and they will basically
12:45
transfer the control back to the supervisor agent that will look at the next step. So basically this supervisor
12:52
agent will be your entry point right this will be the entry point and then based on the question it will decide
12:58
that I need first to call the research agent and then the research agent will use a bunch of tools does its job and
13:05
then it transfers back the control to the supervisor agent and will call another agent and so on and so forth.
13:11
Right? So basically this supervisor agent how would it make the decision on
13:16
which agent to call? What do you folks think? How would this supervisor agent make that make that call? It will
13:23
basically use another L1. It will provide to it the question, the context
13:30
and you will start to see actually that the supervisor agent to make the
13:36
decision here for for example after I called the research agent if it needs to
13:42
decide which agent to call next it will need to call an LM right it's an agent that has access to an LM and the prompt
13:48
it will send to the LM most probably needs to contain the history what agents I called before and what did they say
13:56
and what then give me the next step. What is the problem with this? If we actually keep adding the history to all
14:03
the calls that the supervisor agent makes. There's an obvious problem that you can think of it. Let's see. Bias is
14:11
one of the problems. It's resource in intensive context length. Yes. Yes. Thank you. So basically, yes, you will
14:17
run out of context and the calls will be more expensive. So basically it's with
14:22
every single time you want to call an agent for the user question you will have to recite the whole history rerun
14:29
the whole history through the LM so that it remembers what happened in the past. This actually increases the cost and the
14:35
latency also because you're just increasing the prompt that you sent to the O. So this is obviously one of the
14:41
limitations of the supervisor agent. Also if you decided to add a fourth agent here that this will make the
14:47
problem even worse. So what is the other alternative to the supervisor agent? How
14:52
can we make this a little bit better? Can we actually make the hierarchy a
14:57
little bit different? Rather than having a supervisor agent that is responsible for everything, we can have like some
15:04
form of hierarchy. So an example of this could be if you have an agent that does so many things, it actually writes code
15:11
and it makes design and this design has UI design and UX design. Rather than
15:17
having the supervisor agent being aware of all the agents, we're going to abstract all the agents that are related
15:23
to the programming. We're going to abstract them with a programming team supervisor or another sub agent and this
15:30
sub agent will take care of the communication between these agents and then basically it reduces the things
15:36
that it needs to to worry about and then the sup supervisor agent will use these layers of structure. So it's basically
15:42
reducing the responsibility for each of these agents by creating a layer of
15:47
abstraction. Of course, this increases the number of agents because rather than having the actual agents only, we're
15:54
adding some form of middle management between the supervisor agent and the actual agents. But on the flip side, it
16:01
reduces the number of agents that each one of these supervisors have to worry about. So it's just like as someone
16:08
mentioned earlier, you can think of this as designing microservices and you have like large number of design options and
16:15
there's no one single design design pattern or a specific design that solves
16:21
all the problems. Does that make sense to people? Any questions so far? Okay, awesome. Let's see. Then this is still
16:28
problematic because we add unnecessary agents in between, right? We have so we
16:34
we have like we started with one supervisor agent we were not happy with it and then we said like let's add bunch of other supervisor agents. What are the
16:41
other alternatives that completely gets rid of supervisor agents? What people thoughts? What about we actually get rid
16:49
of all these supervisors and then let every agent talk to each other to any
16:55
other agent. So each agent will be aware of all the other agents. they can just send a message to that agent or it
17:02
doesn't have to be like this. It could be more of popup or like publisher and subscriber where basically this is where
17:09
like things get like really interested from systems perspective. I personally prefer this pattern. I work on a mini
17:15
project that uses this pattern where basically me as one agent I publish a
17:20
message. I say I am done with this piece of work. anyone who is interested in
17:25
this type of output for example research output can subscribe to my broadcasting
17:31
channel or something like that and then other agents can subscribe to the message and then basically they can
17:36
process it or drop it completely uh and then they do the same. So it's basically publisher subscriber design better and
17:43
again this is really from this point it's very like it's in in in the intersection between machine learning
17:48
and design and people who have this both skills are actually doing very well in this position because they understand
17:55
system design better and they also understand the implication of equality. So [clears throat] in this scenario,
18:01
we're not adding any intermediate agents that are just there to coordinate the connection between the different agents.
18:08
Every every and each agent can communicate with all other agents or subn number of agents. So you can
18:13
basically have a full network or you can have like some form of or you can have like some form of a sub network between
18:20
the different agents. Both options are available where basically all agents could talk to each other or you have
18:26
like some custom network agent. This is great. It doesn't add unnecessary
18:31
supervisor agents. But there are problems with this. What are the problems with this? There are different problems. Debugging any popups debugging
18:38
any publisher subscriber pattern or design will be extremely difficult
18:43
because it's not sequential, right? Like you don't know who received the message, who decided to to subscribe to it. In
18:50
some cases, other agents need to agents have to be aware of all other agents which also can be quite messy. uh and
18:57
also the infrastructure that will support this design pattern has to be really scalable. It has to give you lots
19:04
of visibility into what's going on. So this is in my opinion is a great design
19:10
that hits multiple or basically provides you with multiple compromise between different two extremes or two ends two
19:17
two ends of the spectrum. But it requires having very solid infrastructure to support this publisher
19:23
subscriber. Requires a good monitoring and logging system to tell you exactly
19:28
what happened and debugging this will not be as easy and as any sequential uh like something like this where you
19:35
exactly know where the call went because it's a single point that makes a decision or even that one hierarchal one
19:41
but you can easily trace it. So that's basically the trade-off that you you'll have to think about. Okay, let's see if
19:46
there's questions on the Q&A. I think it is based state machine is okay. So, it's not fully a state machine. You can there
19:53
are some variants of this like lang chain which you can it's very close to state machine. We will talk about this
19:59
but yeah that's that's very close to state machine. Uh one of the implementation is lang lang graph sorry
20:04
not lang chain lang graph which is very close to state machine. One question that says what if you use a protocol
20:10
like precision time protocol to provide time based context for agent call for network agent calls. You can use
20:16
different protocols to for for agents to talk to each other. We're going to talk actually about some of the specs for the
20:23
protocols. One of the things is that the whole geni field is relatively young
20:29
right hasn't been there for 3 years. Protocols are evol evolving. Everyone is proposing a different protocol. I think
20:36
give it few years and then people will settle down on the specific protocols to use. We haven't talked actually about
20:42
how these agents will talk to each other. What goes in here and we haven't talked but regardless of the architecture basically what are the
20:49
protocols or how do these agents talk to each other who takes the decision for which architecture it is the ML
20:57
engineers that work on the building the applications. So I have seen few different patterns where in relatively
21:04
smaller teams or smaller organizations or smaller companies that each product team makes their own decisions right
21:11
like we want to build a multi- aent application let's reuse one graph or
21:16
crew AI or something like that in larger organization or larger companies they build their own infrastructure inhouse
21:23
and you have like some form of infrastructure team and then product teams connect to these existing
21:29
capabilities It's something similar to saying who builds the database management system, right? Some companies
21:35
like Meta and Microsoft and Amazon and Google, they build their own database
21:40
management systems. Smaller companies use or reuse something that exists. So that's the same for generic. This is the
21:47
pattern that I started to see where infrastructure for building agent applications is becoming extremely
21:53
important because if you don't have solid infrastructure, you can't chip good applications. Okay, since people
21:59
actually started talking about the communication between the different agents, one of the most common patterns
22:05
that I see for these communication between these different agents regardless of the architecture is
22:10
through what people call handoff where basically it's a very common pattern for agents to interact with each other where
22:17
basically one agent hands off the control to the other one. This one this this other agent can be the supervisor
22:23
in the hierarchal architecture or the supervisor architecture or it could be another agent in the network architecture right you have to it's a
22:30
general or very flexible protocol you need to specify the destination and the
22:36
payload the information that you send to the other agent. So to give you an example, it could be something like this
22:42
is a random graph implementation. We basically on an specific agent, you
22:48
specify what is the next agent, the next node that you will go to it. If you specify a supervisor agent, then you are
22:55
actually in a supervisor architecture. If you specify another completely different agent, then you're basically
23:01
in more of like a network agent where all people can talk to each other. And then here you basically share the the
23:08
context that you want to hand off to the other agent. So basically in in this application before the research agent
23:16
handle the the the control to the spell checker agent it adds some state updates
23:22
on or context that should be passed to the other agent. Okay. Any questions? Let's see. Does handoff is a producer
23:30
consume better? The handoff can can happen in producer consumer or in supervisor. It just like it doesn't
23:37
really matter. It's more about how these different agents will talk to each other regardless of talking to a supervisor or
23:44
another neighbor agent. Let's see. Hand off in handoff. Does the agent need to
23:49
rely on what previous agent passed or need to know with original ink? Really good question. Uh I have dedicated
23:55
slides for this but the short answer is it's almost always the case where you need to know the original question or
24:02
and at least what your previous agent what the previous agent has produced.
24:08
These are the at least the two pieces of information that you need to the original question and what is the latest
24:13
agent's output so that at least the agent knows what the problem is trying to solve. How is the performance
24:19
measured in multi- aent architecture? We'll talk about this towards the end but mostly we're interested in the
24:25
quality of the output the cost how many tokens have been produced or consumed
24:31
and the latency right these are the main three things any use case on when do we use network agent I used network agent
24:38
in almost all the multi- aent applications I used year or year and a half ago actually a little over a year
24:43
and a half ago people were user supervisor agent mostly because the quality of the LMS at that point in time
24:51
was was not great. So you couldn't have like agents that are dedicated to specific domains. You we couldn't have
24:59
like that can handle user questions and also making make the routing. So we needed to have a supervised agent that
25:05
is not confused by fulfilling user requests and at the same time making making routing decisions. But now LM are
25:12
much much much more better and then we can basically afford to have a network agent for infrastructure. You think
25:17
system design is important 100%. You can't make a good system. You can't make
25:23
infrastructure without system design. Is network agent same as L graph? Lang graph implements network agent. Will we
25:29
talk about observability like loging agent? We are going to talk about observability very very briefly. Okay.
25:35
Awesome. So let's continue for the content and we will take like other breaks and we'll talk about different
25:40
questions. Okay. So we talk about handoff very very abstractly. Let's actually dig a little bit deeper into
25:47
what are the information that are passed between different agents because actually there was one of the questions about like do we pass everything do we
25:53
pass only the question do we pass only the output and this is basically the third topic about the communication and
26:00
the state management between different agents. So, so what do you folks think
26:06
should be the information that is passed between different agents regardless of supervisor architecture or network
26:13
architecture or regardless of the architecture? Two agents, one of the agents want to hand off that they
26:18
control to the other agent. What should be the information that they hand off? State and messages. What else? Let's see
26:24
what people say. Pass relevant information only. The question and the summary of info provided so far. context
26:32
original prompt and the output of previous agent context window question answer and some text query plus context.
26:40
Okay. So I think people are saying that there are two things that should be
26:46
passed between the different agents, right? Generally the these will be abstracted as messages and the first
26:51
question we will talk about is what should be in these messages, right? What exactly should be in these messages? And
26:57
people are saying mostly two things. either we pass the last agent if it's
27:03
basically agent two agent one calling agent two we only need to worry about the response from the previous agent
27:10
right so basically I don't care how the previous agent reached this conclusion I
27:15
only care about the final output that this agent has reached right another
27:21
proposal would be actually I will need to know what are the final output that
27:26
the previous agent has produced but I also want to know what are the thought process of that agent and what are the
27:31
tools that this agent has called. Does the difference make sense? Basically between agents either the agent only pro
27:38
pass in addition to the original question only passes the output from the caller or it passes the output and also
27:45
the thought and the tools that have been called and all of that will be done through message passing. So basically
27:50
the idea is we pass messages between different agents and we wanted to talk about first what are the content of
27:57
these messages and there are two things either you pass the final output of the agent or you pass the details of how the
28:04
agent reach it to that conclusion or reached that output. So what are the pros and cons of each one? There is like
28:11
pros and cons. Again it's was almost for almost all design decisions it's going
28:16
to be pros and cons. It's not going to be a single solution that works in all cases. There's like very very rare
28:23
situations where like there's a clear answer. There's always pros and cons. So for the first one, what are the
28:29
advantages of doing this? It's very clear that passing the whole like
28:34
history or passing all the intermediate steps will increase the memory and we increase the token the tokens that are
28:41
used and it will eat up from the context window. But there are also benefits that we will get. what are what would be the
28:47
benefits. So it's basically giving the destination or the final this next agent
28:52
more insights about what happened in the past and it might help actually the next agent make better decisions. It's
28:58
providing the next agents with more information. So this is going to more
29:03
likely increase the quality but basically complexity will increase. it will eat up from the context limit that
29:10
we have will also increase the cost and this is again the the the cost that we
29:15
have to pay or the decisions that we have to make in and in in in the whole session I will keep repeating the word
29:21
taking decisions because this is what design designing or this is part of designing applications not only writing
29:28
the prompts but also about how to manage the context between different agents or even within the same agent. So basically
29:34
on the flip side, if you only share the final results, you are less likely to
29:40
run out of context, the cost will be less and it actually might work better for systems with many agents. So the
29:46
more agents that you have, the more likely that you will need to only share the final results or actually agents are
29:52
are more complex and they are more likely to have like long intermediate output to be shared. Does that make
29:57
sense? Does the trade-off make sense for people? How about if the responsibilities of both agents are different? Actually the responsibilities
30:03
of both agents should be different but you can think of examples where you say
30:08
something like pull the relevant information about geni and write a report about it.
30:17
Pulling information from the internet is the responsibility of an agent for example that has a specific skill set
30:24
and writing a report about it is the responsibility of another agent that requires a different skill set. However,
30:30
they work together to achieve the same task. So if the tasks are different there, then the two agents not do not
30:37
need to work together. What we're talking about is two agents working together to solve the same question, but
30:42
they are bringing different things to the table. They have different skill sets. Of course, calling multiple agents
30:48
will increase cost, will increase latency. And people try to run as many
30:54
steps as parallel as they can. But sometimes you have like some dependencies between these steps. So you
30:59
can't actually run everything in parallel. But yes, it will increase cost it increase latency. When does the accuration to look for results stop in
31:06
multiple agents? Uh if there accuracy rate for an output. So usually it stops
31:11
or basically people usually allocate either a time budget basically I will
31:17
allow this to go for x seconds or x minutes or whatever or number of calls and after that they make a decision to
31:24
abort the call and declare failure basically. Okay. So this is basically the first point in the in the
31:30
communication and state management. Also one of the things that you should always do is adding the which agent is making
31:37
the call just to disembigate. this is agent one or research agent or writing agent or coding agent so that you
31:44
actually identify the source of each message. Okay. So going back to the idea
31:49
of passing messages between agents through the handoff and basically we talked the first thing about what would
31:56
be the content of these messages. uh the second thing that I wanted to briefly talk about it which is a little bit
32:02
lower level details about how handoffs actually were presented on the implementation wise we are not going to
32:10
show like or run code or anything like that but I just wanted to give you a little bit of more concrete idea about
32:16
how the handoff is represented on the on the implementation side usually the
32:21
handoff is done through calling a specific handoff tool so basically it's represented as one of the tools that an
32:28
agent can call and then basically that tool call will be added as an AI message
32:33
to the history. Right? We know that for any agent or any application that uses an LM you have a history of the
32:39
conversations that happened. We add as an implementation detail we add an AI message that is basically calling a
32:46
specific tool. Modeling the handoff as one of the tools is really nice because
32:51
you can rely on your LM choosing that I need to hand off this to another agent
32:57
that is responsible for writing code or drawing graphs or making web search or
33:03
something like that. You don't have to add a specific mechanism to that if you just model this as a tool. So basically
33:09
implementation wise online graph again it will look like this where you basically as we mentioned earlier this
33:15
is one of the tools that you will have which is basically transferring the or handing off the control to another
33:23
agent. So from implementation perspective you can look at this or you can think of this as one of the tools
33:29
and your LM will be responsible for choosing that tool. Does that make sense? Let's take another question from
33:36
the Q&A. Sharing only the final results wouldn't only work better for network of agent and if there's also an agent who
33:42
is consolidating the information. Okay, so this is answering question. See if we have other questions here in network. Do
33:48
agents have to run this to use the same LM? Really good question. No, they don't have to. And
33:55
actually most cases it's better better to use different LMS if the responsibilities are completely
34:01
different. It really varies by application but no they don't have to use the same LM and that's why we are
34:06
actually separating them into multiple agents. Really good question. All right see can chain can chaining agents be
34:13
dynamic based on the context. Yes and this is basically what I was trying to say around if you add the the handoff as
34:22
a tool your agent or your LM will make the call or make the decision whether it
34:27
needs to hand off to agent A or agent B. That's basically the idea that you want to make it dynamic not very static lang
34:34
graph hits a it strikes a balance between two things but you want to rely on making that decision so that the
34:41
quality or you don't basically write don't hard code the decision okay is it possible to have two agents performing
34:46
the same task agent one always gives answer but expensive agent two most of the time gives answer but sometimes no
34:53
agent will call agent two okay so this idea is very similar to what people refer to as speculative decoding it's
35:00
done through you can do this within the same agent it's there's a famous technique called the speculative
35:06
decoding I would suggest reading about it because it's very similar to the idea that you were mentioning right how does
35:11
chain of thought processing impact the choices of available agents chain of thought is mostly within the agent it's
35:17
an implementation detail within the agent you can expose the thoughts to other agents if it helps them but it
35:23
will not really be impacting the direct protocol between the different agents okay so Let's go to the third question
35:30
that we had. How do we manage the state for sub agents? So what I'm trying to
35:36
get into into this point is that um we mentioned earlier something like this here, right? Where we said we can either
35:42
pass the chain of thought or the different tool calls that we made and
35:48
the different internal reasoning steps that we made, right? Or we can only share the last one. It's very typical to
35:55
do the share the last one. But what if agent two [clears throat] that is making
36:02
the call and it decided to only share the final output if the control goes back to that agent. Right? So basically
36:07
what I'm trying to say here is that we have agent one that called agent two
36:13
right and then the control went back to agent one again. we are passing only the
36:19
last message but agent one when the control went back to the to agent one it
36:24
needed actually the previous processing steps that it didn't pass it didn't share with all the other agents so I
36:31
want to have the f fix flexibility to pass the last the last piece of information to other agents but if it
36:37
ever comes back to me the control comes back to me I want to have the full history people achieve this by one of
36:44
two goals either you keep everything in the history But other agents ignore the
36:50
intermediate information. They see it but they ignore it and then basically when it goes back to the the first agent
36:57
it can actually read that information or you can keep that information internally on each. So basically what I'm trying to
37:03
say here is that you can have a trade-off. You can have like solution in between here where you make that
37:09
intermediate information available only to that agent across different calls but but other agents do not have to worry
37:16
about it so that it doesn't explode the context. Does that make sense to people in questions? Let's see how are the
37:22
results validated between agents. So the when you say validated do you mean like some form of quality evaluation or some
37:30
form of ensuring that there are no errors? Errors are handled by basically
37:35
having a proper safe error handling or basically if one of the agents fails it
37:40
should fail gracefully. It should send an error message to the other agents and it depends on the protocol that we implement. If you're concerned about the
37:47
evaluation then you should have like some end to-end evaluation for the system. Why not leverage an summarization to solve context size
37:53
problem? Yes, this actually sometimes happen. This is a really good point. Good point. If your context grows then
37:58
at some point you may want to stop and summarize that. That's also another option that I didn't include in the
38:04
sites. Let's see. So, there's one master log. They all update. Yes, you can think
38:09
of it as a main log that they all update, but they pull the relevant information that they only interested
38:14
in. I wouldn't say like it's a central one. It just keeps propagating and being passed between different agents. Let's
38:20
see if intermediate use the history information will not impact on the their
38:26
functioning. Sorry, Muhammad, I'm not following your question. If you can if you can post a follow-up that would be
38:32
easier. Is this handoff called agent to agent or agent to agent is different? Agent to agent to agent is one of the
38:38
protocol that implements the communication between that one of the protocol that specifies how this handoff
38:44
should happen. Let's see. Is there a reason why MCP isn't discussed when it comes to standardizing the communication
38:49
protocol between agents? MCP is designed specifically for LM or backends or
38:55
agents calling tools. It's designed for a different use case. There are different protocols like agent to agent
39:01
and others have mentioned and it's basically designed for agent to agent communication which is different than
39:06
which is different than calling tools. Okay. So basically this is the of course
39:12
there are tons of details around how these protocols are implemented. I don't think we can cover this within an hour.
39:18
So tons of questions how is that but due to time we can't uh we can't dive in all
39:24
the details. The other thing that I wanted to talk about is how adding
39:29
multiple agents can increase the safety uh risk. So what safety risks are
39:36
introduced by having multiple agents? What do people think? What are the safety risks that can happen if we have
39:42
multiple agents? What do you folks think for multi- aent system? What are specific safety issues that can happen?
39:48
Leakage. Yes. In in any architecture, any architecture of multi- aent system, it actually doesn't doesn't matter.
39:54
Managing governance. Yes. Security of Yes. Leakage of personal leakage of personal information. Yes. This is a
40:00
great point that is people refer to it as prompt injection. You can read a little bit about that. Authentication.
40:06
Yes. Misuse between different agents. So basically you can think of a scenario where you have multiple agents right
40:14
some of these agents are allowed to read information. Like for example, I have an agent that I authenticated it to read my
40:21
emails, but I haven't authenticated another agent to use these emails to
40:27
update a database. What if the reading agent communicate with the writing agent and my information has been leaked? You
40:34
can read actually about a an attack that happened for an MCP server, official MCP
40:40
server for GitHub where basically private repo information has been le has
40:46
been leaked into a public repo information. So basically the the setup of the attack where basically you have a
40:53
private rebel and you have a public rebel. So this everyone can see it. This
40:59
only you can see, right? And then you have an MCP server that is connected to them. The MCP is allowed to read that
41:06
information, but it was not allowed. It was not supposed to write here. So it's allowed to read, it's allowed to write
41:13
to the public ripple, but not information from the private. So
41:18
basically this is one of the scenarios where adding multiple or many moving parts can actually result in flow of
41:26
information that is not was not intended. So basically this is related to the permissions and making sure that
41:33
the the flow of the information is intended. There's no bypassing of authentication or there's no bypassing
41:41
of different rules that you have. So basically you can solve this with many
41:46
many ways again from like designing your agent communication should enforce many
41:51
things should actually enforce some kind of policies right you should allow like basically say do not tag the information
41:59
that are read from private sources or authenticated sources with the right annotation so
42:05
that the policy is enforced on them and again when you think about this this should be on the platform that supports
42:12
the agent to agent communication. It shouldn't be each agent arbitrarily enforcing these rules should be on the
42:18
platform itself. And this is again goes back to why having a platform that supports proper agent to agent
42:24
communication is important. Another thing that you can have is define explicit rules or explicit constraints
42:31
on things that shouldn't be done. For example, any sensitive information shouldn't be tend to write operations
42:37
before user authentication. You can enforce having human in the loop before any write or something like that. You
42:42
can also from the safety side you can have like some content filters since these are LMS they can produce content
42:49
in the OB chance the content can be unsafe could be like abusive content or something like that. So you can actually
42:56
add some filters that block these content mistakes happen sometimes people mess up and then you should have some
43:02
automatic escalation where humans can always step in and stop these agents. There are many details but basically the
43:09
the idea I'm hoping that it gives you like an idea why having a rigorous and
43:15
um solid platform that implements this agent to agent communication is important because it enforces many
43:21
things and the application or the people who write the agent or the application developer in this case do not have to
43:27
worry about these horizontal compliances or horizontal enforcement. Does that make sense to people? And since some of
43:35
the folks actually mentioned here one of the good points that Muhammad mentioned,
43:40
how can we perform auditing which was really good question actually the reality of working on Asian systems is
43:47
usually answering these questions which is basically does the agent have access to this information? Should the agent
43:52
actually have access to that information? What about other agents that the first agent communicate with it
43:58
and they try or by mistake we actually leak some of the information? How do we even know that these things happened?
44:04
One of the things that you should be having is proper auditing. Every single time you access sensitive information,
44:10
you should leave traces that with that source for that purpose. And this is the
44:16
operation ID. You should be logging everything. And when I say everything, I
44:21
don't mean the data, but at least logging the access that you have, logging which resources you have read.
44:27
You definitely should have monitoring with proper alerting. If something goes wrong, you should have proper alerting
44:33
that basically triggers on call and they can investigate. You should always have like big red button that allows you to
44:39
have emergency stops because again believe me things sometimes go completely in under unwanted directions.
44:46
Let's see if we have other questions. What team handles the guard rails and policies? Really good question. So
44:53
bigger companies usually have teams dedicated for safety evaluation,
44:59
guardrail and policies that they work horizontally across product different product teams. They also work very
45:06
closely with the platform teams to enforce as much as possible of these guard rails on the platform. So when
45:15
basically towards the beginning of 2023 things were were being done completely
45:20
ad hoc. Every product team who wanted to implement geni features they will go ahead and implement their own
45:26
infrastructure. Over time especially bigger companies realize that many teams are repeating the work. They're trying
45:32
to implement their Asian to Asian communication protocol and at the same time they're trying to implement their
45:39
safety or guard rails. So they decided to have like a common infrastructure that offer all these capabilities and
45:46
they you can think of the product team as application developers at this point that they do not have to think about how
45:53
to implement these basic capabilities. Instead they will just think about how to use them. So that there's all there
45:59
has been a push for a little over a year now to push down all of these common
46:04
problems to shared layer whether it's an infrastructure or libraries or whatever
46:11
so that not every single product team has to implement this but this is a really good question should a proxy
46:17
enforce policies so all inter agent communications pass through it this is
46:23
the some people believe that this is a great solution but I have some thoughts on on this. If you enforce something on
46:30
every agent to agent communication, first you don't know what are the
46:35
different needs for different agents. Second, it will slow down all the communications between different agents.
46:41
So, you only enforce things that violate specific rules, for example, explicit
46:47
content or fraud or violent content. But if there's something that is application
46:54
specific, you should you should allow applications to configure it. So if it's something that violates an explicit and
47:00
clear signal, a clear policy, then yes, you should enforce it on the platform. If it's something that is application specific, then you should basically let
47:07
the applications control it. Okay, I can talk all day long about guard rails and
47:14
safety and security, but unfortunately we don't have time to do all of that. So I will punt the discussion for now. But
47:19
I will be happy to talk towards the end about all things related to safety and security and go gates. Okay. So since we
47:26
talked about like measuring how many times things went wrong and being able to alert based on that there's always
47:33
essay that says like you can't improve what you can't measure. So basically as
47:38
I mentioned earlier we should have some observability so that we can make evaluation and you should basically
47:44
leave traces of different things that happen. What are the traces that we should be leaving terms of logging? What
47:51
are the traces or logging that we should be leaving or the logging that we should be implementing for multi- aent system?
47:57
What exactly should we be logging? Start time and yes start and end of hang
48:03
handoff agent message being sent. What else? Agent info times and messages. So
48:10
basically yes if any handoffs happen you should always have like an ID for a trace. You should have specify the
48:17
different types of logging and you should have like causality tree which agent called which other agents. You
48:23
should also loging things that are application specific like how many tasks did I make to or how many steps did I
48:30
take to solve this question or to achieve or finish that task. Success rate latency how many times I actually
48:37
kept running in a loop and then I had to abort. This is actually kind of failures. All of these like operational
48:43
stuff will help you at least understand if your system has a problem or not. And you can always keep adding fine grained
48:49
logging. Again, there are like lots of things around logging and performance evaluation. We will not dig deeper into
48:55
this, but I just giving I'm giving you some pointers and some hints about like what you can read a little bit more about in terms of evaluation. As some
49:03
folks asked actual actually earlier about like how can we evaluate this, right? You can have like some scenario
49:09
tests. You can have like some golden outputs. Uh I expect the final output to look like this. And I'll run these
49:16
questions or these queries through the system and then we'll compare the final outputs to the golden outputs so that
49:22
you can basically get an idea about the the quality of the system as an end to end. In terms of the safety and guard
49:28
rails, you can have actually adversarial tests where you intentionally try to get your systems to do things that it
49:34
shouldn't be doing. things like trying to leak users data or try to get the model to talk about topics that it
49:40
shouldn't be talking about it like violent content or hateful speech or something like that and if your model
49:46
falls for it then this is this means that you are not ready to launch the application. If your model declines or
49:52
pushes back on this then you're ready to to release your application. adversarial testing or safety testing is something
49:58
that you have to do and you basically have to have your scenario test to have like as higher coverage as you can about
50:04
your application. Uh you can also have like some infrastructure CI/CD like for in continuous testing. Okay. So that's
50:12
for the evaluation. Circling back about the the idea about the the scaling since
50:18
we started talking about like the when we started at the beginning some folks mentioned like running agents in
50:24
parallel and reducing the cost. We we all know that running multi- aent system
50:30
will increase the number of of LM calls which almost always translates to
50:36
increase in TPU cost and latency. There are different techniques that people do to allow for higher scaling and increase
50:44
reliability for your system. One of them, the obvious one is parallel execution, but it comes at the cost of
50:50
like paying more money and we are aware of that. We should be accounting for that. One of the things that we already
50:57
touched on is if one agent fails, it shouldn't fail your whole system, right?
51:02
You basically you should be like what people call like crash resilient or having like a graceful failures where
51:08
one agent failing doesn't have to shouldn't take the whole system down which means you basically need to have
51:14
like process isolation. You should have proper system design but runs each agent in isolated as much as possible isolated
51:22
processes so that one of them failing doesn't take the whole thing down with it. You should definitely have load
51:28
shedding as much as you can. You should have like paralyze your sorry replicate your back end so that if one node fails
51:35
not everything fails. If every if actually failures happen you should as much as you can checkpoint your your
51:43
processing right. So basically if your task required calling seven agents and
51:49
after the third agent there's a failure that happened. If you resolve that failure you shouldn't start all the way
51:54
from the beginning. You should checkpoint that periodically after maybe each agent should checkpoint the outputs
52:00
and then pick up the computation from there. So you should have like basically some resilience and that resilient
52:06
actually that resilience reduces your cost because you do not have to start from the beginning which means you don't
52:12
have to repeat the work which means you will pay less money. One of the things actually that helps with paying le this
52:18
money which is one of the mentioned earlier which basically you can mix different models. So you can actually
52:25
use cheaper and smaller models for less critical tasks and save some money or
52:30
save some of the your capacity to use bigger models for more critical tasks. So you can actually mix and match
52:36
different models with different capacity with different cost or with different sizes so that you can reduce your cost.
52:43
You should do some form of caching like if you have seen this question or a very similar question to it, you should
52:50
actually bypass running the again and you should just basically basically cache your requests. One of the things
52:55
that actually happen but this happens on a lower level which is batching requests to the LM. This improves the utilization
53:02
of the hardware. I wouldn't say it reduces the cost or directly reducing the cost but it reduces the increases
53:08
the utilization of your hardware. One of the very important things and people
53:14
overlook it is budget enforcers. Budget enforcement. So basically if you're
53:19
releasing an application for many many users and you don't want some users to
53:25
either intentionally or unintentionally abuse the resources. Some people will may ask questions that require multiple
53:32
reasoning steps and they will end up straining the resources that you have on your system. should actually enforce
53:38
some budget as we mentioned earlier either latency budget or number of alert calls budget on each question so that
53:45
you do not strain your system. So these are few things or few pointers around
53:50
how we can basically scale or add resilience to our application. Let's see
53:55
if people have questions. Is there a concept of business B2B in the context
54:01
of A2A? Basically engaging A2A interaction between enterprises. As far
54:07
as agent to agent is concerned, it doesn't matter if the agent lives on the
54:12
same business or not. Uh but there's a notion of authentication. Is this agent
54:18
allowed to call with that agent or not? Or there should be should we have an additional authentication if we call
54:24
another agent or not? Not sure if this is what you were asking about or not. When do you stop continuous integration
54:31
in order to standardize for scaling? I'm not sure how stopping the continuous
54:38
integration will standardize the scaling. Sorry if you can clarify this. Maybe I misunderstood your question.
54:44
Does agent have any co coordinator especially they are sharing common
54:49
resources? That depends on the architecture that we talked about earlier, right? If you have like a supervisor architecture, then yes, they
54:55
all go to the same supervisor. If not then everyone can talk to everyone. What are the top commercially available tools
55:01
for agents monitoring and observability? Langmith is the easiest one. I think some folks mentioned using data dog. I
55:08
haven't used it but it seems to be popular and it seems to be integrated
55:14
with agents as well. These are the two that I have heard about. Let's see. Okay. I don't see other question. If we
55:19
catch information, how the cash refresh when the LM update? This is basically
55:25
depends on the eviction policy, right? You can configure your cash to evict all the information stored if depending on
55:31
the different condition. One of them could be the L1. All right. So let's talk about some of the frameworks and
55:37
then we can see a coding example and then we depending on the time we can talk about other topics or not. So let's
55:44
talk about different topics topic sorry different frameworks that you can use to build multi- aent systems. What are the
55:52
frameworks that people are familiar with? Are folks familiar with specific frameworks that you can use for building
55:57
agents? Blraph. Yes. Strand. I haven't used it but I have heard about it. Init
56:03
Google ADK. Yes. What else? What is the difference between init for example on
56:10
one hand and langraph and Google ADK and link chain on the other hand? There's like a clear difference between things
56:16
like init one hand and link chain on the other hand. Yes, init is a new code. You
56:23
can have like new code uh frameworks things like there are many applications not only init
56:30
and and make which is which is more suitable for people who are not interested in writing code or not
56:36
technical by their day-to-day job is not basically writing code u and this gives you less control over what can be done
56:43
or how it can be done on the other hand things like it's mostly UI driven where
56:49
you connect or define through UI what are the things that you wanted to be done. On the other hand, you you have
56:55
things like code based frameworks like clang chain, langraph, crew AI. One
57:01
example is lang graph. I'm not going to talk about lang graph in detail because we have a coding demo in crewi that I
57:06
wanted to show you. But basically lang graph is nice because it gives you some
57:12
control about the state machine or the state graph that you want your agent to
57:18
go through. So you define main steps and you let the LM execute each of these
57:24
steps but you force your agent to go through specific steps. One example is there has to be authentication first and
57:30
then there has to be Google search next and then it has to be database or
57:36
something like that. So you have some control over how the application will be performed. The other example is Crew AI
57:42
which we're going to use for today. Basically, Crew AI is a framework that allows you to define multiple agents in
57:50
more of orchestrator. You can actually use different uh architectures, but basically you can what you have to do is
57:57
define multiple agents. You define how these agents will talk to each other. We will see a coding example and then they
58:02
work together to achieve a common goal. So you basically define agents. Each
58:07
agent will be like specialized in a specific task or a specific goal or a specific uh domain and then you specify
58:14
create a task. You say like I want this question to be solved and then basically you configure your agents and they will
58:21
work together to achieve a final goal. We will see an example. The example that we will see is basically creating a an
58:27
an agent that research specific topic that you ask it and then it will write a
58:33
some information about that topic and then it will publish this on the social networks. So let me show you if we have
58:41
slides about this. Let's see it's over here. Yeah. So basically our demo will
58:47
basically take a user input. there will be an orchestration layer and then this orchestration layer will call multiple
58:54
agents and they will give us a final output. The specific example that we will have is that is basically we'll
58:59
have like a question and we'll have multiple agents that will help us achieve that goal. The first agent we
59:05
call it research agent or researcher agent which will learn about that topic. We'll use an example for about question
59:13
about economics. Our may or may not know about best question. It can use its internal information or internal
59:19
knowledge to get information about best or use Google search tool to get more
59:25
information. Once it has that information, it will pass the flow to another agent that will basically write
59:33
a more comprehensive answer to that question. So basically we call it the writer agent. The third agent is a an
59:41
agent that's basically writing about this topic for social media uh um posts.
59:47
So basically each one of these agents will be specialized in one specific task
59:53
and they will work together to basically achieve the final goal. So the first agent is the researcher agent which will
1:00:00
basically pull some information um to better understand this domain and then the second one is writing or analyzing
1:00:07
the information and the third one is publishing to social network about that specific topic. Does the problem setup
1:00:13
make sense to folks? See there are many questions. It's hard for me to go through all these.
1:00:20
Let's scroll all the way to the bottom. Give me a moment. I'm not sure what what is happening. Let me scroll all the way
1:00:27
down. I think people have used both chat and Q&A to interact with you. So there
1:00:33
become like super cluttered. Yeah. Yeah. And for some reason I keep scrolling. By the way, uh I'll share this crew AI
1:00:39
demo link with folks. I hope it's okay. The same one that has been that's on the deck, right? I will. Is it okay if I share this on
1:00:46
the chat? Because I made few changes. So I think it's okay. Good if I update that. Okay. I'll send
1:00:52
to folks the latest one. So let me switch. Yeah. Okay. Let me switch tabs.
1:00:58
And then this is basically the demo that will do exactly what we mentioned. Are folks familiar with Google Collab? If
1:01:06
you're not familiar with it, it's the easiest way for running Python code. Uh
1:01:11
you'll run it technically in the browser. You don't have to install anything. The code will be structured in
1:01:17
cells and you can run each cell in isolation or not really in isolation, but you can run each cell alone and you
1:01:23
can inspect the output. So it's a really easy way to have some Python proof of
1:01:28
concepts. Uh and you basically you will have to install all the dependencies
1:01:34
explicitly within your notebook until you close the notebook the runtime gets disconnected and you will have to run
1:01:41
the installation again similar to what I am doing here even though I run this before the session. So this is basically
1:01:47
it's very excuse me it's very similar to Jupyter notebook. Yes they are very
1:01:52
similar to each other. So it will take a moment to download the the install the
1:01:57
dependencies. Let me take questions in the meanwhile. One question on the Q&A that says is there any auto
1:02:03
discoverability mechanism for agents? Yeah, this is a really good question. How can we actually allow agents to know
1:02:10
about each other? Do we have to explicitly let them know? I think this will be done on the platform level. One
1:02:16
of the ways that I have seen people doing it is basically you have a registry like a single registry and
1:02:21
every time you create an agent you say like register agent and then there's a central place that each agent goes to
1:02:28
pull the information of the recent agent that exists. That way you don't have to
1:02:34
discover other agents yourself. It's packaged and supported by the platform which goes back again to the importance
1:02:39
of having a single platform. Okay. Awesome. So the installation is done. Let me this cell is basically importing
1:02:46
all the necessary libraries. It's just like all the libraries or similar to Python import blocks. You just basically
1:02:53
import everything that you need. This cell is basically allowing me to enter
1:02:59
my open AI API key because we're going to use from open AI. This is just a way
1:03:06
for me to copy paste my API key without it being visible to everyone. and I will store and save that in a variable called
1:03:12
open AI API key. I will create LM object. Here you can see that I'm
1:03:17
creating this using chat opendi which I am importing from language chain. So
1:03:22
this is basically relying on language chain to use my API key to connect to
1:03:28
openi. Now I have an ln object right that I can use to connect or to open and
1:03:34
then send requests to them. As we mentioned earlier that we are going to have multiple let's see sorry there are
1:03:41
multiple things okay so there are as we mentioned earlier we're going to have three agents one of them is write doing
1:03:47
research one of them is connecting to writing the summary and the analysis and the third one is connecting to social
1:03:54
network and posting content. So basically we will have different functions that basically one function
1:04:01
that creates the post for me just the text post for me that will be posted in
1:04:06
the session network. You can see that we're writing some simple prompt that says write a prompt sorry write a post
1:04:12
on this specific platform. It could be LinkedIn or Twitter and talk about this specific topic. So it's just a function
1:04:19
that accepts some parameters and populate this prompt and eventually calls the LM. And the other function is
1:04:25
basically making a request to the post request basically to post a specific
1:04:31
content to different platform. So it's just a simple Python implementation that connects to different to connect to
1:04:38
different social networks and create a post. What really matters is how we're going to package this as a tool. This is
1:04:45
this is just a Python function. What really matters is how we can package everything as a tool. You can see here
1:04:50
that within the make social copy tool, the tool that will be used by an agent
1:04:56
to call or to create a post. You can see here that we're creating a name and a description for the tool because these
1:05:03
will be used by the to know whether it needs to call this tool or not. So this is really important to write this in a
1:05:09
descriptive language because your LM will use this. What's going on? Because your LM will use this to decide whether
1:05:16
it needs to use this tool or not. So we are packaging this as a tool. You can see that there's something called base
1:05:22
tool and the base tool is imported from crew AI. So it allows me to create tools as Python function. I need to create the
1:05:29
name and the description. And what happens? It's not technically not a Python function. It's a Python class and
1:05:34
I have to to define a run function. The same for the tool that posts the content
1:05:41
on the social network. I have to create it as a class. create the name and the description because this will be used to
1:05:48
solit tool and eventually I define the run function. So basically I create an
1:05:53
instance of that class the make social copy and the post available hook the
1:05:58
name is important here and the description is important and basically here you have your custom Python
1:06:05
implementation. So this is basically the first thing that I will create the two tools for one of them is for creating
1:06:11
posts on social network and the other one is to actually publish this post. So
1:06:17
let me now define my agents. Now I'm ready. I have all the building blocks I have to define. Then my next step is
1:06:24
defining my agents. So here I will import agent from crew AI. This is all
1:06:29
what I have to do here. Again I am defining the rule and the goal of each
1:06:34
agent. Similar to tools, the name and the goal of the agent will be used by
1:06:39
other agents whether other agents directly or through the orchestrator or the supervisor to select which agent
1:06:46
will be used the rule and then the goal as text or strings are extremely
1:06:52
important and then basically we define the LM that this agent will use the here
1:06:58
we didn't define any tools for the researcher agent this means that it will use its internal information to do the
1:07:04
research will not for example use Google search tool. If we needed it to do that then we
1:07:10
should have add created a Google search tool and added it to it. Similarly the writer tool it writes or drafts posts
1:07:18
with a specific architecture or a clear structure but it's not using any tools.
1:07:23
The only agent that uses tools is the social agent. The social agent again we
1:07:28
give it a rule and goal and backstory just describing what it does. And you can see clearly here that we are giving
1:07:35
it tools. So basically these are three agents. One that researches the problem.
1:07:40
It's more of like using itself information. It's not using any tools. Uh the other one is the writer which
1:07:46
writes a description about the problem itself. And finally the social one that actually uses tools. Now to connect
1:07:54
them, we'll see how we can connect them. But basically the before we connect them, we can actually create different
1:08:01
tasks. So we can create a task that says research the topic state of Canada's economic growth in 2020 blah blah blah.
1:08:08
You just basically ask it to research a specific topic and then you create another task where you basically
1:08:14
explicitly tell it write a structured content about this topic and the final
1:08:19
one create a social media posts. You create an agent and then for each agent
1:08:24
you create a corresponding task and then you assemble everything together in a
1:08:29
crew. So you have a crew which is defined by your agents and tasks. Each agent each task maps an agent and then
1:08:37
you can basically start your crew by saying crew kickoff and then basically you ask questions to that crew. So let
1:08:43
me run this. So here we define the task because a crew consists of agents and
1:08:48
tasks. First thing is we define the agent and then we define the tasks and
1:08:54
the last thing is we are assembling the crew. We are basically telling it what are the agents and what are the tasks
1:08:59
and then basically it takes questions and you can see that it will rely on the
1:09:05
first question that is basically asking it to research the state of Canada's economic situation or growth or whatever
1:09:12
and let's actually go through the logs together because the logs are actually very interesting you can see that it's
1:09:17
we enabled the verbosive answer so we can see everything that happens the first thing is basically saying that I
1:09:23
will start with the researcher agent and the task is research the topic state of CA's economic growth and blah blah blah.
1:09:30
So we have like a task ID. Let's actually say yes here. Let's go through
1:09:35
the logs from the beginning one more time. So the first thing is the researcher agent, right? We can see the
1:09:42
name of the agent, the ID which is 1 A17. And then basically we can see there's like some information about
1:09:49
Canada's economic situation or economic growth and then you can see that task A17 blah blah blah is completed and then
1:09:57
it handed off the flow or the control to the writer agent and you can see the task is saying write a structure 250 to
1:10:05
300 word. So this is basically the name of the task that we created here. So it knows what it should be doing and the
1:10:11
control has been handed off to it from the researcher agent because the researcher agent made the decision that the right next thing is the writer agent
1:10:19
which basically again we have the trace of the first task that has been completed and this is the next next task
1:10:24
that is in progress and similarly it improved the writing based on the
1:10:30
information from the research step and then this is also completed and it handed off to the social media
1:10:35
strategist which also knows the task. from the description that we gave it here it it identify the task and then
1:10:43
the final thing that it you can see actually that this agent has a thought that I need to create. Does anyone know
1:10:50
actually why this agent has a thought in the tool output but the other ones do not. One question saying how data is
1:10:57
passed from one agent to the other. Yes, this is handled by this is handling handled by crew platform. Yes. So the
1:11:03
the only reason that this agent had thoughts and used tools is that if you
1:11:09
go back to the description or definition of these agents, this is the only agent that has tools. Other agents did not
1:11:16
have tools. So they didn't need to think about which tools to use in the first place. This agent doesn't have tools.
1:11:22
That agent doesn't have tools. That is the only agent that has tools. So basically that's why it has some
1:11:28
thoughts and different tools to use. So it used the make social copy tool. You
1:11:34
can see that the inputs for the tools and the output from the tool is the actual the actual post and since we are
1:11:40
actually running this in like just like debug mode. We didn't actually post anything. So it's only past just wrote
1:11:47
the name of the tool. So, it did this for LinkedIn. It did this for Twitter,
1:11:52
but it didn't actually call any web hook because we're just doing this in in in just debug mode or like dummy mode
1:11:59
without actually posting anything on on LinkedIn or Twitter. Finally, you can always export or log the traces. You can
1:12:06
see the the traces in a more summarized format. And then finally, it just tells
1:12:12
you that the workflow is completed. So this is really a simple notebook or a
1:12:17
simple example that shows an end to end workflow that works like in in in like
1:12:23
connecting three agents together to achieve a simple task. Any questions about this? Let me go to the journey.
1:12:31
See if we have questions. There are many questions I know but let me try to answer some of them in LinkedIn. There's
1:12:37
a post right in your account here. I didn't write in my account because let me actually show you what happened. Uh
1:12:43
let's see in some of the descriptions I said like you shouldn't do you shouldn't actually try to go and post on LinkedIn.
1:12:51
You should just make it as dummy post. Here let me show you here. Publishing
1:12:56
not publishing automatically just showing what the post would would look like because in some of the instructions
1:13:01
I told it like it's if the URL is not specified to an account you shouldn't post on LinkedIn. For testing purposes
1:13:08
do not publish automatically. Just show what the post will look like. So in the task I explicitly told it told the LM to
1:13:14
not try to post it. Just showing the content will be enough. Let's take more questions. If the code asks to publish
1:13:22
the post but if your instruction in the text say do not post it, it automatically understand not to post it. Yes. Yes. It follow the direct
1:13:28
instructions. LMS are good are getting even better at instruction following. Oh this tool where you are writing and
1:13:36
running this program. What is this tool? So the tool is a Python function. You define the tool and you define what the
1:13:42
tool can do here. This is what the tool can do and this is the call back that will be executed once this tool is
1:13:48
called. How about let's see sorry why is this giving is I don't know why this
1:13:54
keeps scrolling. Why is this giving an agent a background important? Uh this is
1:13:59
just part of how agent being defined in in crew AI is done. You just give it
1:14:05
like basically the skills or the capabilities of each agent. Let's see. I
1:14:12
don't know why the Q&A keeps scrolling without me touching anything. How to create front end to visibly see in the
1:14:19
form of UI possibly interactive. It's similar to building any front end, right? Like you just have to add the
1:14:25
right end points in your back end that exposes the information that you want. And to expose the information from
1:14:31
Koreain, you need to know which APIs to call so that you can propagate back to the front end. Let's see what should be
1:14:39
the value of ro for each agent. This is basically depending on what the agent is doing like this is a researcher agent.
1:14:45
This is like custom string that you write similar to the name of the agent. So it should be descriptive of what
1:14:52
agents are capable of doing. Let's see. Okay, let me actually wrap up the
1:14:58
discussion and then we can go to the Q&A because for some reason it keeps us scrolling and there's like around 170
1:15:04
questions. So, we'll keep scrolling back and forth. So, let me quickly rub up the remaining slides and then we can have
1:15:10
like maybe 10 minutes or so. I tell you one thing, sorry to interrupt. What I'll do is I'll try to clean up the Q&A section. If you're
1:15:16
going to take five minutes, I'll clean it up. Yeah, that will be really helpful because for some reason keeps calling
1:15:22
without me doing anything. Okay, so we hopefully this demo clarify a few things
1:15:27
on like or made things a little bit more concrete. I have like literally a couple of slides on the things that I have seen
1:15:34
as like to watch out for before you actually launch your application or before you you you publish or release
1:15:42
your your end to-end agent. One of them is observation loops where basically your agents keep running in inference
1:15:48
loops. Uh this can happen for many reasons. either you have like Asians are trying to do the same things or low
1:15:54
quality LM and basically the other thing that you should have an explicit stop
1:16:00
criteria or like kill switch like you allocate a latency budget or you allocate number of calls that you say
1:16:06
like after that we're going to stop the execution and also plan drift where you
1:16:12
some people have observed that OM are more likely to to to over complicate the
1:16:17
problems watch out for these make sure that your evaluation covers First cases like this final checklist is have clear
1:16:25
create clear roles and separation of responsibility between different agents. Optimize your context very carefully
1:16:31
because it can you can different agents can eat up from the the same context and you should have like proper design and
1:16:38
proper limit for that. Focus on safety guard rails. It's extremely important. It distinguishes between a proof of
1:16:43
concept and final application. allocate budget for each request otherwise people
1:16:49
will abuse your your your application. We talked about the stopping condition. We talked about the structured
1:16:55
communication. So invest in a good infrastructure to abstract many of the details for you. Uh monitoring and
1:17:02
evaluation is extremely important. You you cannot improve what you can't measure. You need proper monitoring and
1:17:08
you need some evaluation offline and online. And always think about scaling,
1:17:14
right? Like the more users you get, the more traffic you get. You have to have like more of an incremental scaling.


#AGENT VIDEO WITH 1.6 THOUSAND HITS
Before you build an AI Agent, MASTER THIS | Agentic AI 101
https://www.youtube.com/watch?v=IhxVqUznyek

Why AI Agents Are More Than Just Interns
0:00
Imagine you've got an intern helping you
0:02
with content. You give them step-by-step
0:04
instructions. They turn out a draft and
0:06
then wait for your next command. That's
0:08
how most AI works today. Now, imagine a
0:10
producer instead. They brainstorm the
0:12
topics, script the video, schedule the
0:14
posts, and even track the analytics
0:16
while you sleep. That's a not just
0:20
answering prompts, but acting, planning,
0:23
and executing like a teammate. According
0:25
to a recent Morgan Stanley report, this
0:28
technology alone could generate an
0:29
annual net benefit of nearly $920
0:32
billion for SNP500 companies by
0:36
automating tasks in financial services,
0:38
healthcare, and more. Stick around as I
Morgan Stanley’s $920B Agentic AI Prediction
0:41
break down Agentic AI, what it is, how
0:43
it runs, and the frameworks that make it
0:45
tick. If you're still using old Fang
0:47
prep guides, you're already losing
0:49
ground. The rules have changed, and this
0:51
free AI playbook shows you what's really
0:53
being tested in modern fang interviews,
0:55
insider strategies, hidden evaluation
0:58
matrix, a 90-day mastery plan, the exact
1:01
playbook successful candidates are using
1:03
right now. Everyone's grabbing it. Don't
1:04
be the one left behind. Link in
1:06
description. Before we begin, a quick
1:08
introduction about myself. I am Dr.
Meet Your Host: Dr. Surid’s Background
1:10
Surid. I am currently a director of
1:13
physics- based machine learning at a
1:14
climate tech startup here based here in
1:16
Boston. I finished my PhD in mechanical
1:19
engineering in 2018. Since then, I've
1:22
been working at a variety of e-commerce,
1:24
robotics, as well as big tech companies.
1:26
Just before working at a startup, I was
1:28
at Tik Tok leading their supply chain
1:30
and logistics and e-commerce teams. At
1:33
Tik Tok, I led multiple engineering
1:36
teams which focused on evaluation and
1:38
development and fine-tuning and
1:40
development of state-of-the-art large
1:42
language models. Before Dematic, as I
1:44
mentioned, I was at a robotics company
1:45
called Dematic. And before Dematic, I
1:48
was at Staples as a lead researcher in
1:50
their transportation optimization
1:52
division. I have over 10 years of
1:55
experience in delivering measurable
1:58
results by deploying machine learning
2:00
systems to solve complex business
2:02
problems. And I have been actively
2:04
participating in continuously expanding
2:06
AI education and democratizing AI based
2:10
services and AI knowledge by
2:12
collaborating with local startups as
2:14
well as attending industry conferences.
2:16
I am a reviewer for multiple leading
2:20
academic journals which focus on
2:23
computational mathematical modeling in
2:25
the energy sciences domain as well. So
2:27
today we'll be talking about agentic AI
What Is Agentic AI? Vision & Fundamentals
2:30
and how you can use agentic AI to solve
2:32
a particular business problem. So what
2:35
is the vision associated with agentic
2:38
AI? Typically agentic AI focuses on
2:41
transforming rigid processes that can be
2:44
automated using intelligent AI workflows
2:47
and we'll see what that exactly means.
2:49
What we'll particularly explore in
2:51
today's session is a low code framework
2:53
called AAI for agentic AI applications.
2:57
We'll specifically see how you can use
3:00
this crew AI framework to research a
3:03
particular topic of your interest and
3:05
generate a research summary associated
3:07
with this topic. And then we'll
3:09
essentially see how you can build a live
3:11
system or a live autonomous agent that
3:14
can think, that can research, and that
3:16
can execute actions based off of your
3:19
tools, based off of your specific use
3:22
case and your nuances that you are
3:25
wanting that tool to abide by. So we'll
3:28
see all of that in action and we'll see
3:30
how agentic AI essentially separates
3:33
separates itself from non-agentic AI
3:35
which is just quering a large language
Why Not Just Use LLMs? The Limitations
3:37
model directly. So why should we be
3:40
using agentic AI? Let's think about it.
3:42
What would happen if we don't use
3:44
agentic AI and just rely on traditional
3:46
MS? So typically traditional large
3:49
language models focus on single
3:51
interaction with minimal context. What
3:53
that what that specifically means is the
3:56
following. Whenever you interact with
3:57
chat GPT, you are essentially relying on
4:00
knowledge that is that the large
4:03
language model has been trained with. It
4:06
doesn't have any context that is
4:08
specific to your use case. So whenever
4:11
you give a particular query to the large
4:13
language model, the responses associated
4:15
with large language model are therefore
4:18
only those responses which it can
4:21
produce based off of the knowledge that
4:23
it has learned. that essentially leads
4:25
to to to lack of generalization ability
4:29
and also in certain cases hallucinations
4:32
as as you may have experienced. Another
4:35
issue with using only LLMs is that it
4:37
has a limited persistence. By limited
4:40
persistence what I mean is that
4:42
typically LLM don't retain all of their
4:44
memory if you keep on interacting with
4:46
them beyond a certain point. they they
4:49
lose they they forget your interactions
4:52
with it and you cannot give a purely
4:55
large language based application doesn't
4:57
have a tool access. Imagine you need a
5:00
very specific task that needs to be done
5:02
for your organization. Agentic AI allows
5:05
you to build custom modular functions
5:08
that you can essentially pass those
5:11
functions to these agentic AI frameworks
5:14
as a tool and ask that agentic AI
5:17
framework to use this custom function
5:19
that you have built for for answering a
5:21
particular question that tremendously
5:24
decreases hallucinations also. So that's
5:26
just traditional LLMs. So agentic
How Agentic AI Combines LLMs, Tools & Memory
5:29
systems essentially enable autonomous
5:31
planning and essentially because they
5:34
have access to these tools and they have
5:37
access to a large language model
5:40
typically through an API they are
5:42
essentially very powerful at combining
5:44
the best of both worlds essentially and
5:46
because they are also because you can
5:49
pass these custom functions there's
5:52
there's scope for agentic systems to
5:54
also have a certain memory. So what I'm
5:57
trying to say here essentially is that
5:59
if if you combine the the ingenuity of
6:02
just pure LLM and give them tools and
6:05
memory that can lead to tremendous value
6:08
creation that would be like the true
6:11
automation of a very complex workflow
6:13
that you might have. This would also
6:15
enable continuous execution and mimic
6:18
almost humanlike problem solving. So
From Static Chat to Goal-Driven Agents
6:21
typically agentic AI represents a very
6:24
fundamental shift from just using a
6:26
large language model for direct
6:28
inference to to going in going in this
6:31
paradigm shift direction of seeking an
6:34
active goal-driven systems that can
6:36
operate independently. For example, if
6:38
you are just using LLM to generate a
6:41
response based off of certain query that
6:44
is in a way a static interaction. But
6:47
imagine if you are using this question
6:50
answering session with an LLM to do a
6:54
particular task. Maybe it's sending an
6:56
email. Maybe it's scheduling an event on
6:57
your calendar. Maybe it's writing a blog
6:59
article. If you are combining the
7:01
response of the LLM to do all of these
7:04
tasks and you automate this on a
7:07
periodic basis, that is the that is the
7:10
true value of agentic AI and that's how
7:13
it's it separates itself from just LLM
7:16
interaction with with LLMs. So I quickly
7:19
wanted to point out some of the the
Code-Based Frameworks: LangGraph & LangChain
7:22
codebased frameworks that people have
7:24
used for agentic AI. The a big advantage
7:27
associated with code-based frameworks is
7:29
that all of these code-based frameworks
7:31
provide maximum flexibility and control
7:33
for people who need to develop very
7:36
specialized very custom agents with with
7:39
highly specific capabilities. Land chain
7:42
and LAN graph are some of the most
7:44
popular codebased agentic AI frameworks.
7:48
They typically require strong
7:49
programming knowledge. They require a
7:51
fairly deep understanding of not
7:54
necessarily how LMS are built but at
7:56
least a deep understanding of how LMS
7:59
function and what are their pitfalls. It
8:01
requires a formal understanding of what
8:03
prompt engineering is and it also
8:05
requires experience with software
8:08
architecture and writing just good
8:10
object-oriented code and how to
8:12
orchestrate all of these things to
8:14
deploy a production ready application.
8:16
So codebased frameworks are excellent if
8:19
you have a very specialized task and if
8:21
you require high amounts of flexibility.
8:23
Some of the examples as I mentioned on
8:25
the previous slide also of all code
8:27
based frameworks is langraph. So
8:29
langraph essentially provides a very
8:32
minute control over your applications.
8:34
Langraph requires you to essentially
8:38
build your own graph with nodes and
8:40
edges. What I mean by building a graph
8:44
which has its own nodes and edges is
8:46
that given a particular task if the task
8:49
has a action associated with it based on
8:53
a certain result it takes action A
8:55
versus action B you would have to embed
8:57
all of this information in in the form
9:00
of a grass. So building agents using
9:03
Langraph involves defining all of these
9:06
states on what different actions your
9:10
task requires. creating all of these
9:12
nodes associated for this action and
9:14
they're essentially connecting all of
9:15
these actions with some other actions
9:18
which are dependent on this previous
9:19
action and all of these things require a
9:22
like complex algorithmic logic and and a
9:26
relatively verbose coding and coding
9:29
structure. So so this requires cycling
9:31
through some of the reasoning steps
No-Code Frameworks: Zapier, n8n & More
9:33
using multiple different tools and also
9:36
incorporating human feedback. Some of
9:38
this is also required in low codebased
9:41
framework like QAI that we'll see today.
9:43
But the extent to which abstracts away
9:46
all of this graph construction tool
9:48
calls access to multiple different APIs
9:51
is is much more in in low codebased
9:54
frameworks. This is much more abstract
9:56
than something like langraph. So we I I
9:59
mentioned about all code frameworks
10:01
which was langraph. On the other side of
10:03
the spectrum are no code frameworks
10:06
which some of the examples are Zapier,
10:08
Netn.
10:10
What they essentially do is they enable
10:12
you to have an option of dragging and
10:16
dropping certain components into your
10:19
agentic workflow diagram. For example,
10:21
something like Netn will have a a
10:24
schedule trigger which essentially does
10:26
something every time a particular at a
10:28
particular time in the day. Let's say it
10:30
goes and runs a program based on the
10:33
output of that program. It essentially
10:36
passes through an if else logic and does
10:38
something based off of the if else
10:40
logic. Similarly, Zapier is another one
10:43
of these no code frameworks where you
10:46
can essentially drag and drop these
10:49
multiple components onto your onto your
10:51
agentic AI framework. And what one of
10:54
the key benefits of no core frameworks
10:57
is that they provide pre-built
10:59
integrations and they connect to
11:01
hundreds of APIs like Google and
11:03
LinkedIn and Spotify and you can connect
11:05
multiple databases that you may have to
11:08
all of these API and as I as I mentioned
11:11
because you have these this drag and
11:13
drop ability there's no need for you to
11:16
write verbose code that you saw like pre
11:19
saw previously in in langraph for
11:21
example and so This empowers
11:23
non-developers to without without much
11:26
coding experience to essentially create
11:28
complex multi-step business processes.
11:31
But in in this particular application in
11:33
no code frameworks you do lose a little
11:35
bit of flexibility that you may have in
11:38
in all code based frameworks like
11:40
langraph. So a a intermediate solution
11:44
without really relying on completely
11:46
codebased framework or completely no
Low-Code Middle Ground: CrewAI Explained
11:48
code framework is something what they
11:51
call low code frameworks. Low code
11:53
frameworks afford you the flexibility a
11:56
little bit of flexibility associate
11:57
associated with all code frameworks but
11:59
at the same time they abstract away some
12:01
of the boilerplate verbose code that you
12:04
may have to write in in all code based
12:07
frameworks frameworks. So today we are
12:10
going to look at an example of low
12:12
codebased framework. Specifically we'll
12:14
be looking at crew AI. We'll create an
12:16
agentic framework to solve a research
12:20
task and create a blog which can be
12:23
posted to social media channel. And
12:25
we'll do all of this using using Crew
12:28
AI. So QAI essentially is a very
12:31
powerful agentic AI framework where
12:33
multiple different AI agents work
12:36
together to accomplish a series of task
12:39
and eventually to accomplish one complex
12:41
task. And in this presentation we'll
12:44
explore some of those concepts. We'll
12:46
see how to set up AI and we'll see how
12:49
you can generate fairly robust
12:52
applications using using crew AI. So
12:54
essentially the problem as I mentioned
12:56
previously of using LLMs directly or LLM
13:00
API directly is that is that you don't
13:03
get access to tools or access to your
13:07
nuanced requirements using just a
13:10
question answering ability of LLM. So
13:13
LMS typically execute and run on a fixed
13:17
logic. They are not they are not
13:19
fine-tuned to your application.
13:21
Typically, LLM also cannot adapt to
13:23
changing conditions or to new data that
13:26
might come in. You will have to manually
13:29
update. If new data comes in, you will
13:31
have to make the LLM aware either
13:33
through a prompt or through something
13:34
else that this new data needs to be
13:37
taken into consideration. And as a
13:39
result of all of these things, it has
13:41
limited output capability because it
13:43
doesn't generate insights. you will have
13:45
to manually intervene and and update all
13:48
of its information for you to truly be
13:50
able to generate insights based on the
13:52
latest information. So, so those are
13:54
some of the limitations with using LML
Why Agentic AI Beats Legacy LLMs
13:58
legacy style. Modern requirements
14:01
essentially need you to move away from
14:03
fixed logic and take this dynamic logic
14:06
based on new requirements that come in
14:09
every day. Modern requirements need you
14:12
to adapt to new changes or new data. As
14:15
I mentioned previously, you would
14:17
ideally want to keep human inter
14:20
intervention at a minimum and you would
14:22
ideally want to generate output that
14:24
reflects latest data without any human
14:27
inter intervention and be able to
14:30
provide business value and that is
14:31
something that is quite directly
14:33
addressed with low code agentic AI
14:37
frameworks or any agentic AI frameworks
14:39
for that matter. So typically with
14:41
agentic AI frameworks you are able to
14:43
able to generate dynamic content content
14:45
based on the latest information because
14:48
agentic AI essentially combines LLMs
14:50
with the use of tools. Tools are a
14:53
critical concept in agentic AI and tools
14:55
can be anything. It could be like Google
14:57
API for internet search weather app for
15:00
checking the weather the live weather
15:02
that is or your own custom tool that you
15:05
have developed. And because of this
15:07
agentic AI solutions enabled intelligent
15:09
decision-m the need for human
15:11
intervention is a little bit less
15:13
because you have provided LLMs with
15:16
specific tools that the LLM can use and
15:18
as a result you are able to generate
15:20
rich human readable outputs and provide
15:23
actual business value. Some of the real
Real-World Use Cases (Finance, Healthcare, Manufacturing)
15:26
world use cases of agentic AI are as
15:28
follows. As I mentioned previously,
15:30
Agentic AI combines LLM and gives them
15:32
tools such that they can access live
15:35
information with those tools and convert
15:37
the results of those tools into human
15:39
readable insights. And as you can
15:41
imagine because of that there can be a
15:44
variety of use cases. Some of the
15:45
popular ones I mentioned here are in
15:47
financial services you can use agentic
15:49
AI for automated reporting, risk
15:52
analysis for financial summaries, market
15:55
analysis, trend reporting and so on.
15:57
Similarly in healthcare you can imagine
15:59
agentic AI tools can generate patient
16:02
summaries do research check for
16:04
compliance based on the documentation
16:07
and also generate operational efficiency
16:09
reports based on some other propriety
16:12
proprietary data that you may give these
16:15
agents access to for manufacturing
16:18
systems. Similarly, you can write
16:20
production summaries, rec do supply
16:22
chain analysis and recommendation.
16:24
Again, again, similar to financial
16:25
services, do performance optimization
16:27
for your specific manufacturing task,
16:31
provided you give it the specific tools
16:33
associated with manufacturing. If you
16:35
like this video, you will love what I
16:37
have to say next. The 14week Agentic AI
16:40
program at Interview Kickstart helps
Building Career Skills with Agentic AI (Interview Kickstart Program)
16:42
engineers build real career ready
16:44
skills. You'll work on two live projects
16:47
with fang instructors and leave with
16:49
portfolio work that actually makes a
16:51
difference in hiring. Over 20,000 tech
16:54
professionals have already trained with
16:56
interview kickstart with many moving
16:58
into roles paying $300,000 plus. Want to
17:02
learn more? Join the free upcoming
17:04
webinar, ask questions, meet the
17:06
instructors, and see if it's a fit. Not
17:09
ready yet? No problem. Start with the AI
17:12
agent building videos here on the
17:14
YouTube channel. We recommend this one.
17:16
Either way, the key is to just get
17:18
started.